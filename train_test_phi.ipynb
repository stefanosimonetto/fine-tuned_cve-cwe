{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the best F1-score on the first 40 epochs and extract the best model\n",
    "\n",
    "Embedded: \n",
    "\"\"\"\n",
    "### CVE description:\n",
    "CVE description\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train examples:  84025\n",
      "Number of classes :  64\n",
      "Test examples:  15508\n",
      "Number of classes :  64\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import pickle\n",
    "with open('train_without_test2.pickle', 'rb') as f:\n",
    "    train = pickle.load(f)\n",
    "\n",
    "print(\"Train examples: \", len(train))\n",
    "cve_counter = Counter(entry['cwe']for entry in train)\n",
    "print(\"Number of classes : \", len(cve_counter))\n",
    "\n",
    "with open('test_phi-2_decisivo_base_0.pickle', 'rb') as f:\n",
    "    test = pickle.load(f)\n",
    "\n",
    "print(\"Test examples: \", len(test))\n",
    "cve_counter = Counter(entry['cwe']for entry in test)\n",
    "print(\"Number of classes : \", len(cve_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "263/263 [==============================] - 2s 7ms/step\n",
      "Epoch 1 - F1 Score: 0.5076\n",
      "Saved best model\n",
      "[0.5076094421020623]\n",
      "2364/2364 [==============================] - 32s 13ms/step - loss: 2.0636 - accuracy: 0.4481 - val_loss: 1.6701 - val_accuracy: 0.5286\n",
      "Epoch 2/40\n",
      "263/263 [==============================] - 2s 6ms/step\n",
      "Epoch 2 - F1 Score: 0.5516\n",
      "Saved best model\n",
      "[0.5076094421020623, 0.5515852637959237]\n",
      "2364/2364 [==============================] - 42s 18ms/step - loss: 1.5398 - accuracy: 0.5677 - val_loss: 1.5162 - val_accuracy: 0.5730\n",
      "Epoch 3/40\n",
      "263/263 [==============================] - 1s 5ms/step\n",
      "Epoch 3 - F1 Score: 0.5826\n",
      "Saved best model\n",
      "[0.5076094421020623, 0.5515852637959237, 0.5825799300662835]\n",
      "2364/2364 [==============================] - 39s 17ms/step - loss: 1.4043 - accuracy: 0.6002 - val_loss: 1.4319 - val_accuracy: 0.5960\n",
      "Epoch 4/40\n",
      "263/263 [==============================] - 1s 5ms/step\n",
      "Epoch 4 - F1 Score: 0.5830\n",
      "Saved best model\n",
      "[0.5076094421020623, 0.5515852637959237, 0.5825799300662835, 0.5829835365071483]\n",
      "2364/2364 [==============================] - 31s 13ms/step - loss: 1.3270 - accuracy: 0.6199 - val_loss: 1.4257 - val_accuracy: 0.5986\n",
      "Epoch 5/40\n",
      "263/263 [==============================] - 1s 4ms/step\n",
      "Epoch 5 - F1 Score: 0.6028\n",
      "Saved best model\n",
      "[0.5076094421020623, 0.5515852637959237, 0.5825799300662835, 0.5829835365071483, 0.6027995154543089]\n",
      "2364/2364 [==============================] - 31s 13ms/step - loss: 1.2790 - accuracy: 0.6326 - val_loss: 1.3860 - val_accuracy: 0.6162\n",
      "Epoch 6/40\n",
      "263/263 [==============================] - 2s 6ms/step\n",
      "Epoch 6 - F1 Score: 0.6095\n",
      "Saved best model\n",
      "[0.5076094421020623, 0.5515852637959237, 0.5825799300662835, 0.5829835365071483, 0.6027995154543089, 0.6095178822911034]\n",
      "2364/2364 [==============================] - 32s 13ms/step - loss: 1.2303 - accuracy: 0.6448 - val_loss: 1.3923 - val_accuracy: 0.6125\n",
      "Epoch 7/40\n",
      "263/263 [==============================] - 1s 5ms/step\n",
      "2364/2364 [==============================] - 32s 14ms/step - loss: 1.1952 - accuracy: 0.6529 - val_loss: 1.3618 - val_accuracy: 0.6167\n",
      "Epoch 8/40\n",
      "263/263 [==============================] - 1s 5ms/step\n",
      "2364/2364 [==============================] - 36s 15ms/step - loss: 1.1702 - accuracy: 0.6588 - val_loss: 1.3857 - val_accuracy: 0.6186\n",
      "Epoch 9/40\n",
      "263/263 [==============================] - 2s 8ms/step\n",
      "Epoch 9 - F1 Score: 0.6138\n",
      "Saved best model\n",
      "[0.5076094421020623, 0.5515852637959237, 0.5825799300662835, 0.5829835365071483, 0.6027995154543089, 0.6095178822911034, 0.6074539773611636, 0.609275536647887, 0.6138094152963834]\n",
      "2364/2364 [==============================] - 31s 13ms/step - loss: 1.1442 - accuracy: 0.6663 - val_loss: 1.3628 - val_accuracy: 0.6233\n",
      "Epoch 10/40\n",
      "263/263 [==============================] - 2s 6ms/step\n",
      "Epoch 10 - F1 Score: 0.6290\n",
      "Saved best model\n",
      "[0.5076094421020623, 0.5515852637959237, 0.5825799300662835, 0.5829835365071483, 0.6027995154543089, 0.6095178822911034, 0.6074539773611636, 0.609275536647887, 0.6138094152963834, 0.628999149307842]\n",
      "2364/2364 [==============================] - 41s 17ms/step - loss: 1.1193 - accuracy: 0.6710 - val_loss: 1.2979 - val_accuracy: 0.6375\n",
      "Epoch 11/40\n",
      "263/263 [==============================] - 2s 8ms/step\n",
      "2364/2364 [==============================] - 42s 18ms/step - loss: 1.0975 - accuracy: 0.6774 - val_loss: 1.3652 - val_accuracy: 0.6261\n",
      "Epoch 12/40\n",
      "263/263 [==============================] - 2s 6ms/step\n",
      "2364/2364 [==============================] - 47s 20ms/step - loss: 1.0793 - accuracy: 0.6817 - val_loss: 1.3103 - val_accuracy: 0.6392\n",
      "Epoch 13/40\n",
      "263/263 [==============================] - 1s 5ms/step\n",
      "2364/2364 [==============================] - 33s 14ms/step - loss: 1.0614 - accuracy: 0.6847 - val_loss: 1.3936 - val_accuracy: 0.6180\n",
      "Epoch 14/40\n",
      "263/263 [==============================] - 1s 4ms/step\n",
      "2364/2364 [==============================] - 28s 12ms/step - loss: 1.0515 - accuracy: 0.6882 - val_loss: 1.3853 - val_accuracy: 0.6257\n",
      "Epoch 15/40\n",
      "263/263 [==============================] - 1s 4ms/step\n",
      "2364/2364 [==============================] - 29s 12ms/step - loss: 1.0342 - accuracy: 0.6918 - val_loss: 1.4169 - val_accuracy: 0.6132\n",
      "Epoch 16/40\n",
      "263/263 [==============================] - 1s 4ms/step\n",
      "2364/2364 [==============================] - 30s 13ms/step - loss: 1.0198 - accuracy: 0.6951 - val_loss: 1.3315 - val_accuracy: 0.6430\n",
      "Epoch 17/40\n",
      "263/263 [==============================] - 1s 5ms/step\n",
      "2364/2364 [==============================] - 30s 13ms/step - loss: 1.0048 - accuracy: 0.7013 - val_loss: 1.3917 - val_accuracy: 0.6228\n",
      "Epoch 18/40\n",
      "263/263 [==============================] - 1s 4ms/step\n",
      "2364/2364 [==============================] - 30s 13ms/step - loss: 0.9971 - accuracy: 0.7006 - val_loss: 1.3702 - val_accuracy: 0.6291\n",
      "Epoch 19/40\n",
      "263/263 [==============================] - 1s 4ms/step\n",
      "Epoch 19 - F1 Score: 0.6380\n",
      "Saved best model\n",
      "[0.5076094421020623, 0.5515852637959237, 0.5825799300662835, 0.5829835365071483, 0.6027995154543089, 0.6095178822911034, 0.6074539773611636, 0.609275536647887, 0.6138094152963834, 0.628999149307842, 0.6088830770027176, 0.6288629124963244, 0.6113909453652233, 0.617168406837279, 0.598523716299416, 0.6281432484204976, 0.6168845228559321, 0.6233082866537714, 0.6379692853622965]\n",
      "2364/2364 [==============================] - 28s 12ms/step - loss: 0.9851 - accuracy: 0.7053 - val_loss: 1.3473 - val_accuracy: 0.6437\n",
      "Epoch 20/40\n",
      "263/263 [==============================] - 1s 5ms/step\n",
      "2364/2364 [==============================] - 32s 14ms/step - loss: 0.9737 - accuracy: 0.7093 - val_loss: 1.3928 - val_accuracy: 0.6267\n",
      "Epoch 21/40\n",
      "263/263 [==============================] - 1s 5ms/step\n",
      "2364/2364 [==============================] - 31s 13ms/step - loss: 0.9616 - accuracy: 0.7111 - val_loss: 1.4488 - val_accuracy: 0.6270\n",
      "Epoch 22/40\n",
      "263/263 [==============================] - 2s 6ms/step\n",
      "Epoch 22 - F1 Score: 0.6394\n",
      "Saved best model\n",
      "[0.5076094421020623, 0.5515852637959237, 0.5825799300662835, 0.5829835365071483, 0.6027995154543089, 0.6095178822911034, 0.6074539773611636, 0.609275536647887, 0.6138094152963834, 0.628999149307842, 0.6088830770027176, 0.6288629124963244, 0.6113909453652233, 0.617168406837279, 0.598523716299416, 0.6281432484204976, 0.6168845228559321, 0.6233082866537714, 0.6379692853622965, 0.6182429907852816, 0.618375878231317, 0.6393545770704494]\n",
      "2364/2364 [==============================] - 31s 13ms/step - loss: 0.9561 - accuracy: 0.7113 - val_loss: 1.3851 - val_accuracy: 0.6448\n",
      "Epoch 23/40\n",
      "263/263 [==============================] - 1s 4ms/step\n",
      "2364/2364 [==============================] - 38s 16ms/step - loss: 0.9422 - accuracy: 0.7164 - val_loss: 1.4100 - val_accuracy: 0.6399\n",
      "Epoch 24/40\n",
      "263/263 [==============================] - 1s 4ms/step\n",
      "2364/2364 [==============================] - 28s 12ms/step - loss: 0.9323 - accuracy: 0.7189 - val_loss: 1.3761 - val_accuracy: 0.6441\n",
      "Epoch 25/40\n",
      "263/263 [==============================] - 1s 5ms/step\n",
      "2364/2364 [==============================] - 29s 12ms/step - loss: 0.9299 - accuracy: 0.7206 - val_loss: 1.4507 - val_accuracy: 0.6272\n",
      "Epoch 26/40\n",
      "263/263 [==============================] - 1s 4ms/step\n",
      "2364/2364 [==============================] - 28s 12ms/step - loss: 0.9156 - accuracy: 0.7227 - val_loss: 1.4267 - val_accuracy: 0.6330\n",
      "Epoch 27/40\n",
      "263/263 [==============================] - 1s 5ms/step\n",
      "2364/2364 [==============================] - 29s 12ms/step - loss: 0.9146 - accuracy: 0.7229 - val_loss: 1.4268 - val_accuracy: 0.6330\n",
      "Epoch 28/40\n",
      "263/263 [==============================] - 1s 4ms/step\n",
      "2364/2364 [==============================] - 29s 12ms/step - loss: 0.9029 - accuracy: 0.7260 - val_loss: 1.4265 - val_accuracy: 0.6441\n",
      "Epoch 29/40\n",
      "263/263 [==============================] - 1s 4ms/step\n",
      "2364/2364 [==============================] - 27s 11ms/step - loss: 0.9021 - accuracy: 0.7271 - val_loss: 1.4490 - val_accuracy: 0.6295\n",
      "Epoch 30/40\n",
      "263/263 [==============================] - 1s 4ms/step\n",
      "2364/2364 [==============================] - 28s 12ms/step - loss: 0.8903 - accuracy: 0.7296 - val_loss: 1.4479 - val_accuracy: 0.6394\n",
      "Epoch 31/40\n",
      "263/263 [==============================] - 1s 4ms/step\n",
      "2364/2364 [==============================] - 29s 12ms/step - loss: 0.8809 - accuracy: 0.7309 - val_loss: 1.4564 - val_accuracy: 0.6406\n",
      "Epoch 32/40\n",
      "263/263 [==============================] - 1s 4ms/step\n",
      "2364/2364 [==============================] - 28s 12ms/step - loss: 0.8791 - accuracy: 0.7311 - val_loss: 1.4110 - val_accuracy: 0.6423\n",
      "Epoch 33/40\n",
      "263/263 [==============================] - 1s 4ms/step\n",
      "2364/2364 [==============================] - 27s 11ms/step - loss: 0.8676 - accuracy: 0.7351 - val_loss: 1.4442 - val_accuracy: 0.6458\n",
      "Epoch 34/40\n",
      "263/263 [==============================] - 1s 6ms/step\n",
      "2364/2364 [==============================] - 37s 15ms/step - loss: 0.8664 - accuracy: 0.7358 - val_loss: 1.4385 - val_accuracy: 0.6354\n",
      "Epoch 35/40\n",
      "263/263 [==============================] - 1s 4ms/step\n",
      "2364/2364 [==============================] - 36s 15ms/step - loss: 0.8606 - accuracy: 0.7354 - val_loss: 1.4606 - val_accuracy: 0.6329\n",
      "Epoch 36/40\n",
      "263/263 [==============================] - 2s 6ms/step\n",
      "2364/2364 [==============================] - 36s 15ms/step - loss: 0.8528 - accuracy: 0.7402 - val_loss: 1.5038 - val_accuracy: 0.6361\n",
      "Epoch 37/40\n",
      "263/263 [==============================] - 3s 12ms/step\n",
      "2364/2364 [==============================] - 45s 19ms/step - loss: 0.8480 - accuracy: 0.7398 - val_loss: 1.4962 - val_accuracy: 0.6386\n",
      "Epoch 38/40\n",
      "263/263 [==============================] - 2s 8ms/step\n",
      "2364/2364 [==============================] - 42s 18ms/step - loss: 0.8445 - accuracy: 0.7410 - val_loss: 1.5633 - val_accuracy: 0.6197\n",
      "Epoch 39/40\n",
      "263/263 [==============================] - 2s 7ms/step\n",
      "2364/2364 [==============================] - 36s 15ms/step - loss: 0.8343 - accuracy: 0.7423 - val_loss: 1.5763 - val_accuracy: 0.6341\n",
      "Epoch 40/40\n",
      "263/263 [==============================] - 2s 8ms/step\n",
      "2364/2364 [==============================] - 43s 18ms/step - loss: 0.8349 - accuracy: 0.7434 - val_loss: 1.5401 - val_accuracy: 0.6337\n",
      "485/485 [==============================] - 4s 8ms/step\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         119     0.8750    0.4114    0.5597      1174\n",
      "         120     0.3141    0.7241    0.4382       203\n",
      "         125     0.7137    0.6101    0.6579       572\n",
      "         134     0.4048    0.8947    0.5574        19\n",
      "         189     0.6104    0.7899    0.6886       119\n",
      "         190     0.6535    0.7129    0.6819       209\n",
      "          20     0.3740    0.2218    0.2785       870\n",
      "         200     0.6637    0.4575    0.5416       647\n",
      "         203     0.5000    0.5000    0.5000        36\n",
      "          22     0.7872    0.8111    0.7989       561\n",
      "         254     0.0735    0.1316    0.0943        38\n",
      "         255     0.3131    0.4493    0.3690        69\n",
      "         264     0.5055    0.5959    0.5470       542\n",
      "         269     0.1753    0.4250    0.2482       120\n",
      "         276     0.2136    0.2973    0.2486        74\n",
      "         284     0.2132    0.2248    0.2189       129\n",
      "         287     0.3459    0.5428    0.4225       304\n",
      "         295     0.6615    0.5119    0.5772        84\n",
      "         306     0.2703    0.2128    0.2381        94\n",
      "         310     0.7463    0.7899    0.7675       257\n",
      "         312     0.2453    0.2549    0.2500        51\n",
      "         319     0.6333    0.3455    0.4471        55\n",
      "         326     0.2903    0.2647    0.2769        34\n",
      "         327     0.4706    0.2105    0.2909        38\n",
      "         345     0.0370    0.0769    0.0500        26\n",
      "         347     0.2000    0.6667    0.3077        27\n",
      "         352     0.5877    0.8725    0.7023       549\n",
      "         362     0.6552    0.6835    0.6690       139\n",
      "         399     0.3333    0.5948    0.4272       269\n",
      "         400     0.2347    0.3597    0.2841       139\n",
      "         401     0.3733    0.5000    0.4275        56\n",
      "         415     0.8846    0.5000    0.6389        46\n",
      "         416     0.7041    0.7199    0.7119       357\n",
      "         426     0.6190    0.5909    0.6047        44\n",
      "         427     0.5079    0.6809    0.5818        47\n",
      "         434     0.3526    0.8389    0.4965       211\n",
      "         476     0.7020    0.7650    0.7321       234\n",
      "         502     0.3853    0.7568    0.5106       111\n",
      "         522     0.3750    0.4839    0.4225        93\n",
      "         532     0.3537    0.5800    0.4394        50\n",
      "          59     0.7097    0.5893    0.6439       112\n",
      "         601     0.5130    0.7108    0.5960        83\n",
      "         611     0.7865    0.7071    0.7447        99\n",
      "         617     0.5312    0.3953    0.4533        43\n",
      "         639     0.1011    0.2903    0.1500        31\n",
      "         668     0.1163    0.1000    0.1075        50\n",
      "         732     0.2110    0.2255    0.2180       102\n",
      "          74     0.0567    0.2073    0.0890        82\n",
      "         755     0.1364    0.0811    0.1017        37\n",
      "          77     0.4429    0.4079    0.4247       152\n",
      "         770     0.1278    0.3108    0.1811        74\n",
      "         772     0.5429    0.4872    0.5135        39\n",
      "          78     0.5272    0.6991    0.6011       319\n",
      "         787     0.6657    0.4875    0.5628       960\n",
      "          79     0.9773    0.6238    0.7615      2552\n",
      "         798     0.7857    0.5969    0.6784       129\n",
      "         835     0.3871    0.5333    0.4486        45\n",
      "         843     0.4884    0.5250    0.5060        40\n",
      "         862     0.4011    0.6222    0.4878       225\n",
      "         863     0.1562    0.2439    0.1905       123\n",
      "          89     0.9837    0.6133    0.7556      1081\n",
      "         908     0.2500    0.2857    0.2667        28\n",
      "         918     0.5492    0.6768    0.6063        99\n",
      "          94     0.3371    0.5784    0.4260       306\n",
      "\n",
      "    accuracy                         0.5603     15508\n",
      "   macro avg     0.4554    0.5009    0.4566     15508\n",
      "weighted avg     0.6529    0.5603    0.5789     15508\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['label_encoder_train.joblib']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import joblib\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class F1ScoreCallback(Callback):\n",
    "    def __init__(self, X_val, y_val):\n",
    "        super(F1ScoreCallback, self).__init__()\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.best_f1 = 0.0\n",
    "        self.best_model = None\n",
    "        self.f1_scores = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_val_pred = np.argmax(self.model.predict(self.X_val), axis=1)\n",
    "        f1 = f1_score(self.y_val, y_val_pred, average='weighted')\n",
    "        self.f1_scores.append(f1)\n",
    "        \n",
    "\n",
    "        if f1 > self.best_f1:\n",
    "            self.best_f1 = f1\n",
    "            self.best_model = self.model\n",
    "            print(f\"Epoch {epoch + 1} - F1 Score: {f1:.4f}\")\n",
    "            print(\"Saved best model\")\n",
    "            print(self.f1_scores)\n",
    "\n",
    "with open('train_without_test2.pickle', 'rb') as f1:\n",
    "    balanced = pickle.load(f1)\n",
    "\n",
    "with open('test_phi-2_decisivo_base_0.pickle', 'rb') as f2:\n",
    "    unbalanced = pickle.load(f2)\n",
    "\n",
    "train = np.array([item['cve_description_phi_mean'] for item in balanced if item['cwe'] != 'None'])\n",
    "test = np.array([item['cwe'] for item in balanced if item['cwe'] != 'None'])\n",
    "np.random.seed(42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(train,test,test_size=0.1,random_state=42)\n",
    "\n",
    "X_test = np.array([item['cve_description_phi_mean'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "y_test = np.array([item['cwe'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "\n",
    "label_encoder_train = LabelEncoder()\n",
    "y_train_encoded = label_encoder_train.fit_transform(y_train)\n",
    "label_encoder_test = LabelEncoder()\n",
    "y_test_encoded = label_encoder_test.fit_transform(y_test)\n",
    "\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = len(np.unique(y_train))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=input_dim, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(output_dim, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "f1_callback = F1ScoreCallback(X_val, label_encoder_train.transform(y_val))\n",
    "\n",
    "history = model.fit(X_train, y_train_encoded, epochs=40, batch_size=32, validation_data=(X_val, label_encoder_train.transform(y_val)), verbose=1, callbacks=[f1_callback])\n",
    "\n",
    "best_model = f1_callback.best_model\n",
    "\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(best_model, 'CWE_classes.joblib')\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_probs = best_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "y_pred_original = label_encoder_train.inverse_transform(y_pred)\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_original, digits=4))\n",
    "\n",
    "joblib.dump(label_encoder_train, 'label_encoder_train.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run inference with the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "485/485 [==============================] - 5s 10ms/step\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         119     0.8750    0.4114    0.5597      1174\n",
      "         120     0.3141    0.7241    0.4382       203\n",
      "         125     0.7137    0.6101    0.6579       572\n",
      "         134     0.4048    0.8947    0.5574        19\n",
      "         189     0.6104    0.7899    0.6886       119\n",
      "         190     0.6535    0.7129    0.6819       209\n",
      "          20     0.3740    0.2218    0.2785       870\n",
      "         200     0.6637    0.4575    0.5416       647\n",
      "         203     0.5000    0.5000    0.5000        36\n",
      "          22     0.7872    0.8111    0.7989       561\n",
      "         254     0.0735    0.1316    0.0943        38\n",
      "         255     0.3131    0.4493    0.3690        69\n",
      "         264     0.5055    0.5959    0.5470       542\n",
      "         269     0.1753    0.4250    0.2482       120\n",
      "         276     0.2136    0.2973    0.2486        74\n",
      "         284     0.2132    0.2248    0.2189       129\n",
      "         287     0.3459    0.5428    0.4225       304\n",
      "         295     0.6615    0.5119    0.5772        84\n",
      "         306     0.2703    0.2128    0.2381        94\n",
      "         310     0.7463    0.7899    0.7675       257\n",
      "         312     0.2453    0.2549    0.2500        51\n",
      "         319     0.6333    0.3455    0.4471        55\n",
      "         326     0.2903    0.2647    0.2769        34\n",
      "         327     0.4706    0.2105    0.2909        38\n",
      "         345     0.0370    0.0769    0.0500        26\n",
      "         347     0.2000    0.6667    0.3077        27\n",
      "         352     0.5877    0.8725    0.7023       549\n",
      "         362     0.6552    0.6835    0.6690       139\n",
      "         399     0.3333    0.5948    0.4272       269\n",
      "         400     0.2347    0.3597    0.2841       139\n",
      "         401     0.3733    0.5000    0.4275        56\n",
      "         415     0.8846    0.5000    0.6389        46\n",
      "         416     0.7041    0.7199    0.7119       357\n",
      "         426     0.6190    0.5909    0.6047        44\n",
      "         427     0.5079    0.6809    0.5818        47\n",
      "         434     0.3526    0.8389    0.4965       211\n",
      "         476     0.7020    0.7650    0.7321       234\n",
      "         502     0.3853    0.7568    0.5106       111\n",
      "         522     0.3750    0.4839    0.4225        93\n",
      "         532     0.3537    0.5800    0.4394        50\n",
      "          59     0.7097    0.5893    0.6439       112\n",
      "         601     0.5130    0.7108    0.5960        83\n",
      "         611     0.7865    0.7071    0.7447        99\n",
      "         617     0.5312    0.3953    0.4533        43\n",
      "         639     0.1011    0.2903    0.1500        31\n",
      "         668     0.1163    0.1000    0.1075        50\n",
      "         732     0.2110    0.2255    0.2180       102\n",
      "          74     0.0567    0.2073    0.0890        82\n",
      "         755     0.1364    0.0811    0.1017        37\n",
      "          77     0.4429    0.4079    0.4247       152\n",
      "         770     0.1278    0.3108    0.1811        74\n",
      "         772     0.5429    0.4872    0.5135        39\n",
      "          78     0.5272    0.6991    0.6011       319\n",
      "         787     0.6657    0.4875    0.5628       960\n",
      "          79     0.9773    0.6238    0.7615      2552\n",
      "         798     0.7857    0.5969    0.6784       129\n",
      "         835     0.3871    0.5333    0.4486        45\n",
      "         843     0.4884    0.5250    0.5060        40\n",
      "         862     0.4011    0.6222    0.4878       225\n",
      "         863     0.1562    0.2439    0.1905       123\n",
      "          89     0.9837    0.6133    0.7556      1081\n",
      "         908     0.2500    0.2857    0.2667        28\n",
      "         918     0.5492    0.6768    0.6063        99\n",
      "          94     0.3371    0.5784    0.4260       306\n",
      "\n",
      "    accuracy                         0.5603     15508\n",
      "   macro avg     0.4554    0.5009    0.4566     15508\n",
      "weighted avg     0.6529    0.5603    0.5789     15508\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Save the best model\n",
    "with open('test_phi-2_decisivo_base_0.pickle', 'rb') as f2:\n",
    "    unbalanced = pickle.load(f2)\n",
    "\n",
    "X_test = np.array([item['cve_description_phi_mean'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "y_test = np.array([item['cwe'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "\n",
    "best_model=joblib.load('CWE_classes.joblib')\n",
    "label_encoder_train=joblib.load('label_encoder_train.joblib')\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_probs = best_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "y_pred_original = label_encoder_train.inverse_transform(y_pred)\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_original, digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
