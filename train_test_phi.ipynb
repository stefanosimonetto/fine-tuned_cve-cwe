{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the best F1-score on the first 50 epochs and extract the best model\n",
    "\n",
    "Embedded: \n",
    "\"\"\"\n",
    "### CVE description:\n",
    "CVE description\n",
    "\n",
    "### Predicted CWE:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: 1.9\n",
      "Epoch 1 - F1 Score: 0.5566\n",
      "Saved best model\n",
      "[0.5565874734029986]\n",
      "2516/2516 [==============================] - 31s 11ms/step - loss: 1.9138 - accuracy: 0.4893 - val_loss: 1.5094 - val_accuracy: 0.5774\n",
      "Epoch 2/60\n",
      "280/280 [==============================] - 1s 2ms/step loss\n",
      "Epoch 2 - F1 Score: 0.6042\n",
      "Saved best model\n",
      "[0.5565874734029986, 0.6042255489651361]\n",
      "2516/2516 [==============================] - 20s 8ms/step - loss: 1.3956 - accuracy: 0.6057 - val_loss: 1.3354 - val_accuracy: 0.6215\n",
      "Epoch 3/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: \n",
      "Epoch 3 - F1 Score: 0.6284\n",
      "Saved best model\n",
      "[0.5565874734029986, 0.6042255489651361, 0.6284177738137727]\n",
      "2516/2516 [==============================] - 21s 8ms/step - loss: 1.3015 - accuracy: 0.6318 - val_loss: 1.2742 - val_accuracy: 0.6365\n",
      "Epoch 4/60\n",
      "280/280 [==============================] - 3s 9ms/step\n",
      "Epoch 4 - F1 Score: 0.6329\n",
      "Saved best model\n",
      "[0.5565874734029986, 0.6042255489651361, 0.6284177738137727, 0.6328744680572176]\n",
      "2516/2516 [==============================] - 26s 10ms/step - loss: 1.2409 - accuracy: 0.6470 - val_loss: 1.2515 - val_accuracy: 0.6450\n",
      "Epoch 5/60\n",
      "280/280 [==============================] - 1s 3ms/step los\n",
      "Epoch 5 - F1 Score: 0.6407\n",
      "Saved best model\n",
      "[0.5565874734029986, 0.6042255489651361, 0.6284177738137727, 0.6328744680572176, 0.6406723811472624]\n",
      "2516/2516 [==============================] - 37s 15ms/step - loss: 1.2109 - accuracy: 0.6549 - val_loss: 1.2157 - val_accuracy: 0.6563\n",
      "Epoch 6/60\n",
      "280/280 [==============================] - 1s 4ms/step\n",
      "2516/2516 [==============================] - 40s 16ms/step - loss: 1.1814 - accuracy: 0.6626 - val_loss: 1.2064 - val_accuracy: 0.6557\n",
      "Epoch 7/60\n",
      "280/280 [==============================] - 1s 5ms/step\n",
      "2516/2516 [==============================] - 41s 16ms/step - loss: 1.1585 - accuracy: 0.6655 - val_loss: 1.2870 - val_accuracy: 0.6435\n",
      "Epoch 8/60\n",
      "280/280 [==============================] - 1s 4ms/step\n",
      "Epoch 8 - F1 Score: 0.6529\n",
      "Saved best model\n",
      "[0.5565874734029986, 0.6042255489651361, 0.6284177738137727, 0.6328744680572176, 0.6406723811472624, 0.6395705373232328, 0.6284738981197656, 0.6529203245786126]\n",
      "2516/2516 [==============================] - 33s 13ms/step - loss: 1.1343 - accuracy: 0.6737 - val_loss: 1.1809 - val_accuracy: 0.6612\n",
      "Epoch 9/60\n",
      "280/280 [==============================] - 1s 3ms/step loss\n",
      "2516/2516 [==============================] - 32s 13ms/step - loss: 1.1198 - accuracy: 0.6770 - val_loss: 1.2296 - val_accuracy: 0.6547\n",
      "Epoch 10/60\n",
      "280/280 [==============================] - 2s 6ms/step\n",
      "2516/2516 [==============================] - 29s 12ms/step - loss: 1.1027 - accuracy: 0.6817 - val_loss: 1.2043 - val_accuracy: 0.6543\n",
      "Epoch 11/60\n",
      "280/280 [==============================] - 1s 5ms/step\n",
      "Epoch 11 - F1 Score: 0.6569\n",
      "Saved best model\n",
      "[0.5565874734029986, 0.6042255489651361, 0.6284177738137727, 0.6328744680572176, 0.6406723811472624, 0.6395705373232328, 0.6284738981197656, 0.6529203245786126, 0.6436788360132993, 0.6403900526059164, 0.6569138851431896]\n",
      "2516/2516 [==============================] - 53s 21ms/step - loss: 1.0900 - accuracy: 0.6843 - val_loss: 1.1683 - val_accuracy: 0.6686\n",
      "Epoch 12/60\n",
      "280/280 [==============================] - 1s 4ms/step\n",
      "Epoch 12 - F1 Score: 0.6625\n",
      "Saved best model\n",
      "[0.5565874734029986, 0.6042255489651361, 0.6284177738137727, 0.6328744680572176, 0.6406723811472624, 0.6395705373232328, 0.6284738981197656, 0.6529203245786126, 0.6436788360132993, 0.6403900526059164, 0.6569138851431896, 0.6624682047572487]\n",
      "2516/2516 [==============================] - 44s 18ms/step - loss: 1.0843 - accuracy: 0.6871 - val_loss: 1.1415 - val_accuracy: 0.6764\n",
      "Epoch 13/60\n",
      "280/280 [==============================] - 2s 6ms/step\n",
      "2516/2516 [==============================] - 50s 20ms/step - loss: 1.0708 - accuracy: 0.6897 - val_loss: 1.2296 - val_accuracy: 0.6579\n",
      "Epoch 14/60\n",
      "280/280 [==============================] - 2s 8ms/step\n",
      "2516/2516 [==============================] - 67s 27ms/step - loss: 1.0604 - accuracy: 0.6917 - val_loss: 1.1898 - val_accuracy: 0.6651\n",
      "Epoch 15/60\n",
      "280/280 [==============================] - 2s 8ms/step\n",
      "Epoch 15 - F1 Score: 0.6737\n",
      "Saved best model\n",
      "[0.5565874734029986, 0.6042255489651361, 0.6284177738137727, 0.6328744680572176, 0.6406723811472624, 0.6395705373232328, 0.6284738981197656, 0.6529203245786126, 0.6436788360132993, 0.6403900526059164, 0.6569138851431896, 0.6624682047572487, 0.6504962668404374, 0.6617982349189995, 0.6736803523444839]\n",
      "2516/2516 [==============================] - 65s 26ms/step - loss: 1.0530 - accuracy: 0.6930 - val_loss: 1.1258 - val_accuracy: 0.6809\n",
      "Epoch 16/60\n",
      "280/280 [==============================] - 2s 8ms/step\n",
      "2516/2516 [==============================] - 67s 27ms/step - loss: 1.0444 - accuracy: 0.6936 - val_loss: 1.2181 - val_accuracy: 0.6610\n",
      "Epoch 17/60\n",
      "280/280 [==============================] - 3s 8ms/step\n",
      "2516/2516 [==============================] - 69s 28ms/step - loss: 1.0360 - accuracy: 0.6975 - val_loss: 1.1753 - val_accuracy: 0.6721\n",
      "Epoch 18/60\n",
      "280/280 [==============================] - 3s 9ms/step\n",
      "2516/2516 [==============================] - 63s 25ms/step - loss: 1.0294 - accuracy: 0.6990 - val_loss: 1.1450 - val_accuracy: 0.6752\n",
      "Epoch 19/60\n",
      "280/280 [==============================] - 3s 9ms/step\n",
      "2516/2516 [==============================] - 66s 26ms/step - loss: 1.0208 - accuracy: 0.7020 - val_loss: 1.1768 - val_accuracy: 0.6697\n",
      "Epoch 20/60\n",
      "280/280 [==============================] - 3s 8ms/step\n",
      "2516/2516 [==============================] - 69s 27ms/step - loss: 1.0184 - accuracy: 0.7026 - val_loss: 1.1097 - val_accuracy: 0.6813\n",
      "Epoch 21/60\n",
      "280/280 [==============================] - 3s 8ms/step\n",
      "2516/2516 [==============================] - 68s 27ms/step - loss: 1.0102 - accuracy: 0.7048 - val_loss: 1.1344 - val_accuracy: 0.6780\n",
      "Epoch 22/60\n",
      "280/280 [==============================] - 3s 10ms/step\n",
      "2516/2516 [==============================] - 67s 27ms/step - loss: 1.0043 - accuracy: 0.7061 - val_loss: 1.1289 - val_accuracy: 0.6792\n",
      "Epoch 23/60\n",
      "280/280 [==============================] - 2s 8ms/step\n",
      "2516/2516 [==============================] - 66s 26ms/step - loss: 1.0010 - accuracy: 0.7067 - val_loss: 1.1187 - val_accuracy: 0.6868\n",
      "Epoch 24/60\n",
      "280/280 [==============================] - 2s 8ms/step\n",
      "Epoch 24 - F1 Score: 0.6747\n",
      "Saved best model\n",
      "[0.5565874734029986, 0.6042255489651361, 0.6284177738137727, 0.6328744680572176, 0.6406723811472624, 0.6395705373232328, 0.6284738981197656, 0.6529203245786126, 0.6436788360132993, 0.6403900526059164, 0.6569138851431896, 0.6624682047572487, 0.6504962668404374, 0.6617982349189995, 0.6736803523444839, 0.6571019817074667, 0.6657393015121512, 0.6712299871323921, 0.6559823408921298, 0.6711371439962333, 0.670264349820064, 0.6660487258311731, 0.6710478549739208, 0.674682075903833]\n",
      "2516/2516 [==============================] - 68s 27ms/step - loss: 0.9920 - accuracy: 0.7089 - val_loss: 1.0955 - val_accuracy: 0.6858\n",
      "Epoch 25/60\n",
      "280/280 [==============================] - 3s 8ms/step\n",
      "2516/2516 [==============================] - 66s 26ms/step - loss: 0.9904 - accuracy: 0.7083 - val_loss: 1.1700 - val_accuracy: 0.6697\n",
      "Epoch 26/60\n",
      "280/280 [==============================] - 3s 8ms/step\n",
      "2516/2516 [==============================] - 68s 27ms/step - loss: 0.9861 - accuracy: 0.7110 - val_loss: 1.1341 - val_accuracy: 0.6770\n",
      "Epoch 27/60\n",
      "280/280 [==============================] - 3s 8ms/step\n",
      "2516/2516 [==============================] - 66s 26ms/step - loss: 0.9787 - accuracy: 0.7124 - val_loss: 1.1758 - val_accuracy: 0.6710\n",
      "Epoch 28/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: \n",
      "2516/2516 [==============================] - 46s 18ms/step - loss: 0.9753 - accuracy: 0.7123 - val_loss: 1.1586 - val_accuracy: 0.6808\n",
      "Epoch 29/60\n",
      "280/280 [==============================] - 1s 3ms/step \n",
      "2516/2516 [==============================] - 24s 10ms/step - loss: 0.9745 - accuracy: 0.7130 - val_loss: 1.1471 - val_accuracy: 0.6765\n",
      "Epoch 30/60\n",
      "280/280 [==============================] - 1s 3ms/step lo\n",
      "2516/2516 [==============================] - 24s 10ms/step - loss: 0.9681 - accuracy: 0.7150 - val_loss: 1.2051 - val_accuracy: 0.6604\n",
      "Epoch 31/60\n",
      "280/280 [==============================] - 1s 3ms/step l\n",
      "Epoch 31 - F1 Score: 0.6759\n",
      "Saved best model\n",
      "[0.5565874734029986, 0.6042255489651361, 0.6284177738137727, 0.6328744680572176, 0.6406723811472624, 0.6395705373232328, 0.6284738981197656, 0.6529203245786126, 0.6436788360132993, 0.6403900526059164, 0.6569138851431896, 0.6624682047572487, 0.6504962668404374, 0.6617982349189995, 0.6736803523444839, 0.6571019817074667, 0.6657393015121512, 0.6712299871323921, 0.6559823408921298, 0.6711371439962333, 0.670264349820064, 0.6660487258311731, 0.6710478549739208, 0.674682075903833, 0.6579529843724192, 0.6700499422411219, 0.6603742046225872, 0.6736678353525573, 0.6632038361354994, 0.6592260946104543, 0.6758657508734822]\n",
      "2516/2516 [==============================] - 25s 10ms/step - loss: 0.9636 - accuracy: 0.7148 - val_loss: 1.1396 - val_accuracy: 0.6806\n",
      "Epoch 32/60\n",
      "280/280 [==============================] - 1s 3ms/step lo\n",
      "Epoch 32 - F1 Score: 0.6778\n",
      "Saved best model\n",
      "[0.5565874734029986, 0.6042255489651361, 0.6284177738137727, 0.6328744680572176, 0.6406723811472624, 0.6395705373232328, 0.6284738981197656, 0.6529203245786126, 0.6436788360132993, 0.6403900526059164, 0.6569138851431896, 0.6624682047572487, 0.6504962668404374, 0.6617982349189995, 0.6736803523444839, 0.6571019817074667, 0.6657393015121512, 0.6712299871323921, 0.6559823408921298, 0.6711371439962333, 0.670264349820064, 0.6660487258311731, 0.6710478549739208, 0.674682075903833, 0.6579529843724192, 0.6700499422411219, 0.6603742046225872, 0.6736678353525573, 0.6632038361354994, 0.6592260946104543, 0.6758657508734822, 0.6778010995413053]\n",
      "2516/2516 [==============================] - 24s 10ms/step - loss: 0.9612 - accuracy: 0.7165 - val_loss: 1.1184 - val_accuracy: 0.6818\n",
      "Epoch 33/60\n",
      "280/280 [==============================] - 1s 3ms/step\n",
      "Epoch 33 - F1 Score: 0.6798\n",
      "Saved best model\n",
      "[0.5565874734029986, 0.6042255489651361, 0.6284177738137727, 0.6328744680572176, 0.6406723811472624, 0.6395705373232328, 0.6284738981197656, 0.6529203245786126, 0.6436788360132993, 0.6403900526059164, 0.6569138851431896, 0.6624682047572487, 0.6504962668404374, 0.6617982349189995, 0.6736803523444839, 0.6571019817074667, 0.6657393015121512, 0.6712299871323921, 0.6559823408921298, 0.6711371439962333, 0.670264349820064, 0.6660487258311731, 0.6710478549739208, 0.674682075903833, 0.6579529843724192, 0.6700499422411219, 0.6603742046225872, 0.6736678353525573, 0.6632038361354994, 0.6592260946104543, 0.6758657508734822, 0.6778010995413053, 0.6798477862370249]\n",
      "2516/2516 [==============================] - 24s 10ms/step - loss: 0.9556 - accuracy: 0.7171 - val_loss: 1.1269 - val_accuracy: 0.6876\n",
      "Epoch 34/60\n",
      "280/280 [==============================] - 1s 3ms/step \n",
      "Epoch 34 - F1 Score: 0.6852\n",
      "Saved best model\n",
      "[0.5565874734029986, 0.6042255489651361, 0.6284177738137727, 0.6328744680572176, 0.6406723811472624, 0.6395705373232328, 0.6284738981197656, 0.6529203245786126, 0.6436788360132993, 0.6403900526059164, 0.6569138851431896, 0.6624682047572487, 0.6504962668404374, 0.6617982349189995, 0.6736803523444839, 0.6571019817074667, 0.6657393015121512, 0.6712299871323921, 0.6559823408921298, 0.6711371439962333, 0.670264349820064, 0.6660487258311731, 0.6710478549739208, 0.674682075903833, 0.6579529843724192, 0.6700499422411219, 0.6603742046225872, 0.6736678353525573, 0.6632038361354994, 0.6592260946104543, 0.6758657508734822, 0.6778010995413053, 0.6798477862370249, 0.6851731892789437]\n",
      "2516/2516 [==============================] - 25s 10ms/step - loss: 0.9529 - accuracy: 0.7187 - val_loss: 1.1240 - val_accuracy: 0.6894\n",
      "Epoch 35/60\n",
      "280/280 [==============================] - 1s 3ms/step\n",
      "2516/2516 [==============================] - 25s 10ms/step - loss: 0.9495 - accuracy: 0.7184 - val_loss: 1.1669 - val_accuracy: 0.6766\n",
      "Epoch 36/60\n",
      "280/280 [==============================] - 1s 3ms/step \n",
      "2516/2516 [==============================] - 25s 10ms/step - loss: 0.9477 - accuracy: 0.7212 - val_loss: 1.1232 - val_accuracy: 0.6848\n",
      "Epoch 37/60\n",
      "280/280 [==============================] - 1s 3ms/step lo\n",
      "2516/2516 [==============================] - 24s 10ms/step - loss: 0.9441 - accuracy: 0.7203 - val_loss: 1.1460 - val_accuracy: 0.6789\n",
      "Epoch 38/60\n",
      "280/280 [==============================] - 1s 3ms/step l\n",
      "2516/2516 [==============================] - 24s 10ms/step - loss: 0.9409 - accuracy: 0.7225 - val_loss: 1.2021 - val_accuracy: 0.6669\n",
      "Epoch 39/60\n",
      "280/280 [==============================] - 1s 3ms/step l\n",
      "2516/2516 [==============================] - 24s 10ms/step - loss: 0.9412 - accuracy: 0.7210 - val_loss: 1.0941 - val_accuracy: 0.6927\n",
      "Epoch 40/60\n",
      "280/280 [==============================] - 1s 3ms/step lo\n",
      "2516/2516 [==============================] - 23s 9ms/step - loss: 0.9310 - accuracy: 0.7224 - val_loss: 1.1522 - val_accuracy: 0.6853\n",
      "Epoch 41/60\n",
      "280/280 [==============================] - 1s 3ms/step loss\n",
      "2516/2516 [==============================] - 23s 9ms/step - loss: 0.9348 - accuracy: 0.7228 - val_loss: 1.0990 - val_accuracy: 0.6924\n",
      "Epoch 42/60\n",
      "280/280 [==============================] - 1s 3ms/step loss\n",
      "2516/2516 [==============================] - 23s 9ms/step - loss: 0.9299 - accuracy: 0.7249 - val_loss: 1.1343 - val_accuracy: 0.6861\n",
      "Epoch 43/60\n",
      "280/280 [==============================] - 1s 3ms/step loss\n",
      "2516/2516 [==============================] - 23s 9ms/step - loss: 0.9235 - accuracy: 0.7272 - val_loss: 1.1337 - val_accuracy: 0.6847\n",
      "Epoch 44/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: 0\n",
      "2516/2516 [==============================] - 23s 9ms/step - loss: 0.9229 - accuracy: 0.7267 - val_loss: 1.1103 - val_accuracy: 0.6889\n",
      "Epoch 45/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: 0.\n",
      "2516/2516 [==============================] - 20s 8ms/step - loss: 0.9207 - accuracy: 0.7283 - val_loss: 1.1727 - val_accuracy: 0.6696\n",
      "Epoch 46/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: 0.\n",
      "Epoch 46 - F1 Score: 0.6900\n",
      "Saved best model\n",
      "[0.5565874734029986, 0.6042255489651361, 0.6284177738137727, 0.6328744680572176, 0.6406723811472624, 0.6395705373232328, 0.6284738981197656, 0.6529203245786126, 0.6436788360132993, 0.6403900526059164, 0.6569138851431896, 0.6624682047572487, 0.6504962668404374, 0.6617982349189995, 0.6736803523444839, 0.6571019817074667, 0.6657393015121512, 0.6712299871323921, 0.6559823408921298, 0.6711371439962333, 0.670264349820064, 0.6660487258311731, 0.6710478549739208, 0.674682075903833, 0.6579529843724192, 0.6700499422411219, 0.6603742046225872, 0.6736678353525573, 0.6632038361354994, 0.6592260946104543, 0.6758657508734822, 0.6778010995413053, 0.6798477862370249, 0.6851731892789437, 0.6673675640544641, 0.6788822304585607, 0.66342472320457, 0.6561766006688082, 0.6837020490553186, 0.6728017528996372, 0.6792508374577109, 0.6725180888671795, 0.6740361912601383, 0.6790238484511355, 0.6673100260445132, 0.6899710540550333]\n",
      "2516/2516 [==============================] - 19s 8ms/step - loss: 0.9210 - accuracy: 0.7252 - val_loss: 1.0962 - val_accuracy: 0.6952\n",
      "Epoch 47/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: \n",
      "2516/2516 [==============================] - 20s 8ms/step - loss: 0.9152 - accuracy: 0.7287 - val_loss: 1.1405 - val_accuracy: 0.6820\n",
      "Epoch 48/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: \n",
      "2516/2516 [==============================] - 20s 8ms/step - loss: 0.9155 - accuracy: 0.7280 - val_loss: 1.1706 - val_accuracy: 0.6783\n",
      "Epoch 49/60\n",
      "280/280 [==============================] - 1s 3ms/step lo\n",
      "2516/2516 [==============================] - 22s 9ms/step - loss: 0.9123 - accuracy: 0.7267 - val_loss: 1.1681 - val_accuracy: 0.6761\n",
      "Epoch 50/60\n",
      "280/280 [==============================] - 1s 3ms/step lo\n",
      "2516/2516 [==============================] - 22s 9ms/step - loss: 0.9094 - accuracy: 0.7293 - val_loss: 1.1435 - val_accuracy: 0.6839\n",
      "Epoch 51/60\n",
      "280/280 [==============================] - 1s 3ms/step loss\n",
      "2516/2516 [==============================] - 22s 9ms/step - loss: 0.9124 - accuracy: 0.7282 - val_loss: 1.1471 - val_accuracy: 0.6815\n",
      "Epoch 52/60\n",
      "280/280 [==============================] - 1s 3ms/step loss\n",
      "2516/2516 [==============================] - 23s 9ms/step - loss: 0.9064 - accuracy: 0.7286 - val_loss: 1.1530 - val_accuracy: 0.6840\n",
      "Epoch 53/60\n",
      "280/280 [==============================] - 1s 3ms/step lo\n",
      "2516/2516 [==============================] - 23s 9ms/step - loss: 0.9069 - accuracy: 0.7291 - val_loss: 1.1311 - val_accuracy: 0.6882\n",
      "Epoch 54/60\n",
      "280/280 [==============================] - 1s 3ms/step loss\n",
      "2516/2516 [==============================] - 22s 9ms/step - loss: 0.9031 - accuracy: 0.7308 - val_loss: 1.1559 - val_accuracy: 0.6806\n",
      "Epoch 55/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: \n",
      "2516/2516 [==============================] - 20s 8ms/step - loss: 0.8964 - accuracy: 0.7324 - val_loss: 1.1489 - val_accuracy: 0.6830\n",
      "Epoch 56/60\n",
      "280/280 [==============================] - 1s 2ms/step loss\n",
      "2516/2516 [==============================] - 20s 8ms/step - loss: 0.8993 - accuracy: 0.7318 - val_loss: 1.1543 - val_accuracy: 0.6828\n",
      "Epoch 57/60\n",
      "280/280 [==============================] - 1s 3ms/step lo\n",
      "2516/2516 [==============================] - 22s 9ms/step - loss: 0.8977 - accuracy: 0.7312 - val_loss: 1.1396 - val_accuracy: 0.6880\n",
      "Epoch 58/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: \n",
      "2516/2516 [==============================] - 19s 8ms/step - loss: 0.8905 - accuracy: 0.7329 - val_loss: 1.1413 - val_accuracy: 0.6853\n",
      "Epoch 59/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: \n",
      "2516/2516 [==============================] - 20s 8ms/step - loss: 0.8912 - accuracy: 0.7312 - val_loss: 1.1847 - val_accuracy: 0.6809\n",
      "Epoch 60/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: \n",
      "2516/2516 [==============================] - 20s 8ms/step - loss: 0.8917 - accuracy: 0.7320 - val_loss: 1.1388 - val_accuracy: 0.6825\n",
      "485/485 [==============================] - 1s 2ms/step\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         119     0.8559    0.5009    0.6319      1174\n",
      "         120     0.5194    0.7931    0.6277       203\n",
      "         125     0.7184    0.8654    0.7851       572\n",
      "         134     0.6154    0.8421    0.7111        19\n",
      "         189     0.6600    0.8319    0.7361       119\n",
      "         190     0.8944    0.7703    0.8278       209\n",
      "          20     0.5206    0.2034    0.2926       870\n",
      "         200     0.7917    0.4699    0.5897       647\n",
      "         203     0.4658    0.9444    0.6239        36\n",
      "          22     0.8938    0.9002    0.8970       561\n",
      "         254     0.3333    0.1053    0.1600        38\n",
      "         255     0.5490    0.4058    0.4667        69\n",
      "         264     0.6217    0.6458    0.6335       542\n",
      "         269     0.5063    0.3333    0.4020       120\n",
      "         276     0.2951    0.4865    0.3673        74\n",
      "         284     0.5556    0.1163    0.1923       129\n",
      "         287     0.4475    0.7434    0.5587       304\n",
      "         295     0.8243    0.7262    0.7722        84\n",
      "         306     0.3082    0.4787    0.3750        94\n",
      "         310     0.8044    0.8482    0.8258       257\n",
      "         312     0.5263    0.1961    0.2857        51\n",
      "         319     0.4923    0.5818    0.5333        55\n",
      "         326     0.4286    0.7059    0.5333        34\n",
      "         327     0.4828    0.3684    0.4179        38\n",
      "         345     0.5714    0.3077    0.4000        26\n",
      "         347     0.6000    0.4444    0.5106        27\n",
      "         352     0.4596    0.9545    0.6205       549\n",
      "         362     0.5721    0.8561    0.6859       139\n",
      "         399     0.4930    0.6580    0.5637       269\n",
      "         400     0.3846    0.6115    0.4722       139\n",
      "         401     0.7273    0.5714    0.6400        56\n",
      "         415     0.9744    0.8261    0.8941        46\n",
      "         416     0.7897    0.8627    0.8246       357\n",
      "         426     0.5745    0.6136    0.5934        44\n",
      "         427     0.3929    0.9362    0.5535        47\n",
      "         434     0.8095    0.8057    0.8076       211\n",
      "         476     0.8534    0.8462    0.8498       234\n",
      "         502     0.7029    0.8739    0.7791       111\n",
      "         522     0.4510    0.7419    0.5610        93\n",
      "         532     0.8333    0.5000    0.6250        50\n",
      "          59     0.8485    0.7500    0.7962       112\n",
      "         601     0.7609    0.8434    0.8000        83\n",
      "         611     0.8889    0.8889    0.8889        99\n",
      "         617     0.6545    0.8372    0.7347        43\n",
      "         639     0.8421    0.5161    0.6400        31\n",
      "         668     0.1351    0.2000    0.1613        50\n",
      "         732     0.2016    0.2451    0.2212       102\n",
      "          74     0.2092    0.3902    0.2723        82\n",
      "         755     0.6667    0.3243    0.4364        37\n",
      "          77     0.6000    0.3553    0.4463       152\n",
      "         770     0.5588    0.2568    0.3519        74\n",
      "         772     0.6250    0.6410    0.6329        39\n",
      "          78     0.5227    0.9028    0.6621       319\n",
      "         787     0.6661    0.8021    0.7278       960\n",
      "          79     0.9789    0.7265    0.8340      2552\n",
      "         798     0.7630    0.7984    0.7803       129\n",
      "         835     0.8000    0.6222    0.7000        45\n",
      "         843     0.7179    0.7000    0.7089        40\n",
      "         862     0.4313    0.7956    0.5594       225\n",
      "         863     0.3286    0.3740    0.3498       123\n",
      "          89     0.9987    0.7382    0.8489      1081\n",
      "         908     0.3692    0.8571    0.5161        28\n",
      "         918     0.9157    0.7677    0.8352        99\n",
      "          94     0.4316    0.7320    0.5430       306\n",
      "\n",
      "    accuracy                         0.6748     15508\n",
      "   macro avg     0.6127    0.6303    0.5949     15508\n",
      "weighted avg     0.7279    0.6748    0.6750     15508\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['label_encoder_train.joblib']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import joblib\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class F1ScoreCallback(Callback):\n",
    "    def __init__(self, X_val, y_val):\n",
    "        super(F1ScoreCallback, self).__init__()\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.best_f1 = 0.0\n",
    "        self.best_model = None\n",
    "        self.f1_scores = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_val_pred = np.argmax(self.model.predict(self.X_val), axis=1)\n",
    "        f1 = f1_score(self.y_val, y_val_pred, average='weighted')\n",
    "        self.f1_scores.append(f1)\n",
    "        \n",
    "\n",
    "        if f1 > self.best_f1:\n",
    "            self.best_f1 = f1\n",
    "            self.best_model = self.model\n",
    "            print(f\"Epoch {epoch + 1} - F1 Score: {f1:.4f}\")\n",
    "            print(\"Saved best model\")\n",
    "            print(self.f1_scores)\n",
    "\n",
    "with open('train_phi.pickle', 'rb') as f1:\n",
    "    balanced = pickle.load(f1)\n",
    "\n",
    "with open('Phi_test.pickle', 'rb') as f2:\n",
    "    unbalanced = pickle.load(f2)\n",
    "\n",
    "train = np.array([item['cve_description_phi_mean'] for item in balanced if item['cwe'] != 'None'])\n",
    "test = np.array([item['cwe'] for item in balanced if item['cwe'] != 'None'])\n",
    "np.random.seed(42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(train,test,test_size=0.1,random_state=42)\n",
    "\n",
    "X_test = np.array([item['cve_description_phi_mean'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "y_test = np.array([item['cwe'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "\n",
    "label_encoder_train = LabelEncoder()\n",
    "y_train_encoded = label_encoder_train.fit_transform(y_train)\n",
    "label_encoder_test = LabelEncoder()\n",
    "y_test_encoded = label_encoder_test.fit_transform(y_test)\n",
    "\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = len(np.unique(y_train))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=input_dim, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(output_dim, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "f1_callback = F1ScoreCallback(X_val, label_encoder_train.transform(y_val))\n",
    "\n",
    "history = model.fit(X_train, y_train_encoded, epochs=60, batch_size=32, validation_data=(X_val, label_encoder_train.transform(y_val)), verbose=1, callbacks=[f1_callback])\n",
    "\n",
    "best_model = f1_callback.best_model\n",
    "\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(best_model, 'CWE_classes.joblib')\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_probs = best_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "y_pred_original = label_encoder_train.inverse_transform(y_pred)\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_original, digits=4))\n",
    "\n",
    "joblib.dump(label_encoder_train, 'label_encoder_train.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run inference with the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "485/485 [==============================] - 7s 13ms/step\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         119     0.8559    0.5009    0.6319      1174\n",
      "         120     0.5194    0.7931    0.6277       203\n",
      "         125     0.7184    0.8654    0.7851       572\n",
      "         134     0.6154    0.8421    0.7111        19\n",
      "         189     0.6600    0.8319    0.7361       119\n",
      "         190     0.8944    0.7703    0.8278       209\n",
      "          20     0.5206    0.2034    0.2926       870\n",
      "         200     0.7917    0.4699    0.5897       647\n",
      "         203     0.4658    0.9444    0.6239        36\n",
      "          22     0.8938    0.9002    0.8970       561\n",
      "         254     0.3333    0.1053    0.1600        38\n",
      "         255     0.5490    0.4058    0.4667        69\n",
      "         264     0.6217    0.6458    0.6335       542\n",
      "         269     0.5063    0.3333    0.4020       120\n",
      "         276     0.2951    0.4865    0.3673        74\n",
      "         284     0.5556    0.1163    0.1923       129\n",
      "         287     0.4475    0.7434    0.5587       304\n",
      "         295     0.8243    0.7262    0.7722        84\n",
      "         306     0.3082    0.4787    0.3750        94\n",
      "         310     0.8044    0.8482    0.8258       257\n",
      "         312     0.5263    0.1961    0.2857        51\n",
      "         319     0.4923    0.5818    0.5333        55\n",
      "         326     0.4286    0.7059    0.5333        34\n",
      "         327     0.4828    0.3684    0.4179        38\n",
      "         345     0.5714    0.3077    0.4000        26\n",
      "         347     0.6000    0.4444    0.5106        27\n",
      "         352     0.4596    0.9545    0.6205       549\n",
      "         362     0.5721    0.8561    0.6859       139\n",
      "         399     0.4930    0.6580    0.5637       269\n",
      "         400     0.3846    0.6115    0.4722       139\n",
      "         401     0.7273    0.5714    0.6400        56\n",
      "         415     0.9744    0.8261    0.8941        46\n",
      "         416     0.7897    0.8627    0.8246       357\n",
      "         426     0.5745    0.6136    0.5934        44\n",
      "         427     0.3929    0.9362    0.5535        47\n",
      "         434     0.8095    0.8057    0.8076       211\n",
      "         476     0.8534    0.8462    0.8498       234\n",
      "         502     0.7029    0.8739    0.7791       111\n",
      "         522     0.4510    0.7419    0.5610        93\n",
      "         532     0.8333    0.5000    0.6250        50\n",
      "          59     0.8485    0.7500    0.7962       112\n",
      "         601     0.7609    0.8434    0.8000        83\n",
      "         611     0.8889    0.8889    0.8889        99\n",
      "         617     0.6545    0.8372    0.7347        43\n",
      "         639     0.8421    0.5161    0.6400        31\n",
      "         668     0.1351    0.2000    0.1613        50\n",
      "         732     0.2016    0.2451    0.2212       102\n",
      "          74     0.2092    0.3902    0.2723        82\n",
      "         755     0.6667    0.3243    0.4364        37\n",
      "          77     0.6000    0.3553    0.4463       152\n",
      "         770     0.5588    0.2568    0.3519        74\n",
      "         772     0.6250    0.6410    0.6329        39\n",
      "          78     0.5227    0.9028    0.6621       319\n",
      "         787     0.6661    0.8021    0.7278       960\n",
      "          79     0.9789    0.7265    0.8340      2552\n",
      "         798     0.7630    0.7984    0.7803       129\n",
      "         835     0.8000    0.6222    0.7000        45\n",
      "         843     0.7179    0.7000    0.7089        40\n",
      "         862     0.4313    0.7956    0.5594       225\n",
      "         863     0.3286    0.3740    0.3498       123\n",
      "          89     0.9987    0.7382    0.8489      1081\n",
      "         908     0.3692    0.8571    0.5161        28\n",
      "         918     0.9157    0.7677    0.8352        99\n",
      "          94     0.4316    0.7320    0.5430       306\n",
      "\n",
      "    accuracy                         0.6748     15508\n",
      "   macro avg     0.6127    0.6303    0.5949     15508\n",
      "weighted avg     0.7279    0.6748    0.6750     15508\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Save the best model\n",
    "with open('Phi_test.pickle', 'rb') as f2:\n",
    "    unbalanced = pickle.load(f2)\n",
    "\n",
    "X_test = np.array([item['cve_description_phi_mean'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "y_test = np.array([item['cwe'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "\n",
    "best_model=joblib.load('CWE_classes.joblib')\n",
    "label_encoder_train=joblib.load('label_encoder_train.joblib')\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_probs = best_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "y_pred_original = label_encoder_train.inverse_transform(y_pred)\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_original, digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
