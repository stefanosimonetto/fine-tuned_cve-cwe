{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the best F1-score on the first 50 epochs and extract the best model\n",
    "\n",
    "Embedded: \n",
    "\"\"\"\n",
    "### CVE description:\n",
    "CVE description\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "280/280 [==============================] - 2s 6ms/step\n",
      "Epoch 1 - F1 Score: 0.4886\n",
      "Saved best model\n",
      "[0.4885580809058775]\n",
      "2516/2516 [==============================] - 85s 28ms/step - loss: 2.1013 - accuracy: 0.4396 - val_loss: 1.7001 - val_accuracy: 0.5239\n",
      "Epoch 2/60\n",
      "280/280 [==============================] - 2s 6ms/step\n",
      "Epoch 2 - F1 Score: 0.5562\n",
      "Saved best model\n",
      "[0.4885580809058775, 0.556170455285328]\n",
      "2516/2516 [==============================] - 64s 25ms/step - loss: 1.5536 - accuracy: 0.5624 - val_loss: 1.5021 - val_accuracy: 0.5798\n",
      "Epoch 3/60\n",
      "280/280 [==============================] - 2s 6ms/step\n",
      "2516/2516 [==============================] - 66s 26ms/step - loss: 1.4301 - accuracy: 0.5921 - val_loss: 1.4995 - val_accuracy: 0.5726\n",
      "Epoch 4/60\n",
      "280/280 [==============================] - 2s 6ms/step\n",
      "Epoch 4 - F1 Score: 0.5734\n",
      "Saved best model\n",
      "[0.4885580809058775, 0.556170455285328, 0.5553632299094174, 0.5734318560496712]\n",
      "2516/2516 [==============================] - 67s 27ms/step - loss: 1.3586 - accuracy: 0.6103 - val_loss: 1.4485 - val_accuracy: 0.5914\n",
      "Epoch 5/60\n",
      "280/280 [==============================] - 2s 5ms/step\n",
      "Epoch 5 - F1 Score: 0.5900\n",
      "Saved best model\n",
      "[0.4885580809058775, 0.556170455285328, 0.5553632299094174, 0.5734318560496712, 0.5899519551011747]\n",
      "2516/2516 [==============================] - 61s 24ms/step - loss: 1.3043 - accuracy: 0.6229 - val_loss: 1.4120 - val_accuracy: 0.6073\n",
      "Epoch 6/60\n",
      "280/280 [==============================] - 2s 7ms/step\n",
      "2516/2516 [==============================] - 65s 26ms/step - loss: 1.2654 - accuracy: 0.6349 - val_loss: 1.4309 - val_accuracy: 0.6015\n",
      "Epoch 7/60\n",
      "280/280 [==============================] - 2s 7ms/step\n",
      "Epoch 7 - F1 Score: 0.5966\n",
      "Saved best model\n",
      "[0.4885580809058775, 0.556170455285328, 0.5553632299094174, 0.5734318560496712, 0.5899519551011747, 0.5852793105234771, 0.5965500051618823]\n",
      "2516/2516 [==============================] - 67s 27ms/step - loss: 1.2323 - accuracy: 0.6411 - val_loss: 1.4188 - val_accuracy: 0.6052\n",
      "Epoch 8/60\n",
      "280/280 [==============================] - 2s 7ms/step\n",
      "2516/2516 [==============================] - 66s 26ms/step - loss: 1.2042 - accuracy: 0.6485 - val_loss: 1.4296 - val_accuracy: 0.6054\n",
      "Epoch 9/60\n",
      "280/280 [==============================] - 2s 6ms/step\n",
      "Epoch 9 - F1 Score: 0.6140\n",
      "Saved best model\n",
      "[0.4885580809058775, 0.556170455285328, 0.5553632299094174, 0.5734318560496712, 0.5899519551011747, 0.5852793105234771, 0.5965500051618823, 0.5907296665450869, 0.6139612083935275]\n",
      "2516/2516 [==============================] - 67s 26ms/step - loss: 1.1811 - accuracy: 0.6553 - val_loss: 1.3828 - val_accuracy: 0.6226\n",
      "Epoch 10/60\n",
      "280/280 [==============================] - 2s 7ms/step\n",
      "2516/2516 [==============================] - 68s 27ms/step - loss: 1.1625 - accuracy: 0.6592 - val_loss: 1.4398 - val_accuracy: 0.6037\n",
      "Epoch 11/60\n",
      "280/280 [==============================] - 2s 6ms/step\n",
      "2516/2516 [==============================] - 68s 27ms/step - loss: 1.1438 - accuracy: 0.6625 - val_loss: 1.3936 - val_accuracy: 0.6237\n",
      "Epoch 12/60\n",
      "280/280 [==============================] - 2s 6ms/step\n",
      "2516/2516 [==============================] - 67s 27ms/step - loss: 1.1276 - accuracy: 0.6670 - val_loss: 1.4629 - val_accuracy: 0.6009\n",
      "Epoch 13/60\n",
      "280/280 [==============================] - 2s 7ms/step\n",
      "2516/2516 [==============================] - 67s 27ms/step - loss: 1.1137 - accuracy: 0.6696 - val_loss: 1.4128 - val_accuracy: 0.6119\n",
      "Epoch 14/60\n",
      "280/280 [==============================] - 2s 6ms/step\n",
      "2516/2516 [==============================] - 65s 26ms/step - loss: 1.0999 - accuracy: 0.6746 - val_loss: 1.4930 - val_accuracy: 0.6020\n",
      "Epoch 15/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: \n",
      "Epoch 15 - F1 Score: 0.6219\n",
      "Saved best model\n",
      "[0.4885580809058775, 0.556170455285328, 0.5553632299094174, 0.5734318560496712, 0.5899519551011747, 0.5852793105234771, 0.5965500051618823, 0.5907296665450869, 0.6139612083935275, 0.5983540285597523, 0.608785181158622, 0.5913404198241913, 0.6042153313144709, 0.598387406543427, 0.6219015923647799]\n",
      "2516/2516 [==============================] - 58s 23ms/step - loss: 1.0889 - accuracy: 0.6773 - val_loss: 1.3761 - val_accuracy: 0.6291\n",
      "Epoch 16/60\n",
      "280/280 [==============================] - 1s 3ms/step \n",
      "2516/2516 [==============================] - 24s 10ms/step - loss: 1.0755 - accuracy: 0.6803 - val_loss: 1.3829 - val_accuracy: 0.6261\n",
      "Epoch 17/60\n",
      "280/280 [==============================] - 1s 3ms/step lo\n",
      "2516/2516 [==============================] - 24s 10ms/step - loss: 1.0645 - accuracy: 0.6837 - val_loss: 1.3762 - val_accuracy: 0.6266\n",
      "Epoch 18/60\n",
      "280/280 [==============================] - 1s 3ms/step lo\n",
      "2516/2516 [==============================] - 25s 10ms/step - loss: 1.0587 - accuracy: 0.6850 - val_loss: 1.4725 - val_accuracy: 0.6174\n",
      "Epoch 19/60\n",
      "280/280 [==============================] - 1s 3ms/step lo\n",
      "2516/2516 [==============================] - 25s 10ms/step - loss: 1.0500 - accuracy: 0.6858 - val_loss: 1.3981 - val_accuracy: 0.6292\n",
      "Epoch 20/60\n",
      "280/280 [==============================] - 1s 3ms/step \n",
      "Epoch 20 - F1 Score: 0.6272\n",
      "Saved best model\n",
      "[0.4885580809058775, 0.556170455285328, 0.5553632299094174, 0.5734318560496712, 0.5899519551011747, 0.5852793105234771, 0.5965500051618823, 0.5907296665450869, 0.6139612083935275, 0.5983540285597523, 0.608785181158622, 0.5913404198241913, 0.6042153313144709, 0.598387406543427, 0.6219015923647799, 0.6198615942041825, 0.6111712551707829, 0.6065834921070664, 0.6206743968208857, 0.6272325330789489]\n",
      "2516/2516 [==============================] - 25s 10ms/step - loss: 1.0421 - accuracy: 0.6891 - val_loss: 1.3730 - val_accuracy: 0.6341\n",
      "Epoch 21/60\n",
      "280/280 [==============================] - 1s 2ms/step loss\n",
      "2516/2516 [==============================] - 24s 10ms/step - loss: 1.0326 - accuracy: 0.6913 - val_loss: 1.4103 - val_accuracy: 0.6180\n",
      "Epoch 22/60\n",
      "280/280 [==============================] - 1s 3ms/step loss\n",
      "2516/2516 [==============================] - 25s 10ms/step - loss: 1.0196 - accuracy: 0.6946 - val_loss: 1.4034 - val_accuracy: 0.6254\n",
      "Epoch 23/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: \n",
      "2516/2516 [==============================] - 24s 9ms/step - loss: 1.0106 - accuracy: 0.6969 - val_loss: 1.4279 - val_accuracy: 0.6242\n",
      "Epoch 24/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: \n",
      "2516/2516 [==============================] - 24s 10ms/step - loss: 1.0107 - accuracy: 0.6968 - val_loss: 1.3870 - val_accuracy: 0.6307\n",
      "Epoch 25/60\n",
      "280/280 [==============================] - 1s 2ms/step loss\n",
      "2516/2516 [==============================] - 24s 10ms/step - loss: 0.9995 - accuracy: 0.7007 - val_loss: 1.4834 - val_accuracy: 0.6172\n",
      "Epoch 26/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: \n",
      "2516/2516 [==============================] - 24s 10ms/step - loss: 0.9960 - accuracy: 0.7007 - val_loss: 1.4288 - val_accuracy: 0.6209\n",
      "Epoch 27/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: \n",
      "2516/2516 [==============================] - 25s 10ms/step - loss: 0.9888 - accuracy: 0.7036 - val_loss: 1.4276 - val_accuracy: 0.6265\n",
      "Epoch 28/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: 0.\n",
      "2516/2516 [==============================] - 23s 9ms/step - loss: 0.9814 - accuracy: 0.7029 - val_loss: 1.4561 - val_accuracy: 0.6244\n",
      "Epoch 29/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: 0.\n",
      "Epoch 29 - F1 Score: 0.6273\n",
      "Saved best model\n",
      "[0.4885580809058775, 0.556170455285328, 0.5553632299094174, 0.5734318560496712, 0.5899519551011747, 0.5852793105234771, 0.5965500051618823, 0.5907296665450869, 0.6139612083935275, 0.5983540285597523, 0.608785181158622, 0.5913404198241913, 0.6042153313144709, 0.598387406543427, 0.6219015923647799, 0.6198615942041825, 0.6111712551707829, 0.6065834921070664, 0.6206743968208857, 0.6272325330789489, 0.6079558916842273, 0.6170998924506769, 0.610177757082214, 0.6240077624559125, 0.604444576636432, 0.6186736069461806, 0.6217604176987024, 0.6153649581439886, 0.6273466688288905]\n",
      "2516/2516 [==============================] - 23s 9ms/step - loss: 0.9785 - accuracy: 0.7056 - val_loss: 1.4225 - val_accuracy: 0.6323\n",
      "Epoch 30/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: 0\n",
      "2516/2516 [==============================] - 22s 9ms/step - loss: 0.9724 - accuracy: 0.7060 - val_loss: 1.4418 - val_accuracy: 0.6333\n",
      "Epoch 31/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: 0.\n",
      "2516/2516 [==============================] - 23s 9ms/step - loss: 0.9713 - accuracy: 0.7069 - val_loss: 1.4457 - val_accuracy: 0.6259\n",
      "Epoch 32/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: 0.\n",
      "2516/2516 [==============================] - 21s 8ms/step - loss: 0.9572 - accuracy: 0.7100 - val_loss: 1.4492 - val_accuracy: 0.6295\n",
      "Epoch 33/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: 0.\n",
      "2516/2516 [==============================] - 19s 8ms/step - loss: 0.9565 - accuracy: 0.7103 - val_loss: 1.5066 - val_accuracy: 0.6223\n",
      "Epoch 34/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: 0.\n",
      "2516/2516 [==============================] - 20s 8ms/step - loss: 0.9519 - accuracy: 0.7116 - val_loss: 1.4795 - val_accuracy: 0.6232\n",
      "Epoch 35/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: \n",
      "2516/2516 [==============================] - 20s 8ms/step - loss: 0.9469 - accuracy: 0.7145 - val_loss: 1.5082 - val_accuracy: 0.6233\n",
      "Epoch 36/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: \n",
      "2516/2516 [==============================] - 22s 9ms/step - loss: 0.9421 - accuracy: 0.7152 - val_loss: 1.5282 - val_accuracy: 0.6105\n",
      "Epoch 37/60\n",
      "280/280 [==============================] - 1s 3ms/step loss\n",
      "2516/2516 [==============================] - 23s 9ms/step - loss: 0.9384 - accuracy: 0.7158 - val_loss: 1.4598 - val_accuracy: 0.6282\n",
      "Epoch 38/60\n",
      "280/280 [==============================] - 1s 3ms/step loss\n",
      "2516/2516 [==============================] - 23s 9ms/step - loss: 0.9312 - accuracy: 0.7166 - val_loss: 1.4892 - val_accuracy: 0.6289\n",
      "Epoch 39/60\n",
      "280/280 [==============================] - 1s 2ms/step loss\n",
      "2516/2516 [==============================] - 23s 9ms/step - loss: 0.9288 - accuracy: 0.7163 - val_loss: 1.4842 - val_accuracy: 0.6266\n",
      "Epoch 40/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: \n",
      "2516/2516 [==============================] - 23s 9ms/step - loss: 0.9226 - accuracy: 0.7184 - val_loss: 1.4900 - val_accuracy: 0.6318\n",
      "Epoch 41/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: 0.\n",
      "2516/2516 [==============================] - 22s 9ms/step - loss: 0.9176 - accuracy: 0.7212 - val_loss: 1.4951 - val_accuracy: 0.6311\n",
      "Epoch 42/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: 0.\n",
      "2516/2516 [==============================] - 21s 8ms/step - loss: 0.9180 - accuracy: 0.7219 - val_loss: 1.4978 - val_accuracy: 0.6309\n",
      "Epoch 43/60\n",
      "280/280 [==============================] - 1s 2ms/step loss\n",
      "2516/2516 [==============================] - 20s 8ms/step - loss: 0.9154 - accuracy: 0.7212 - val_loss: 1.5262 - val_accuracy: 0.6273\n",
      "Epoch 44/60\n",
      "280/280 [==============================] - 1s 3ms/step loss\n",
      "2516/2516 [==============================] - 22s 9ms/step - loss: 0.9108 - accuracy: 0.7215 - val_loss: 1.5091 - val_accuracy: 0.6174\n",
      "Epoch 45/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: \n",
      "2516/2516 [==============================] - 20s 8ms/step - loss: 0.9075 - accuracy: 0.7222 - val_loss: 1.5271 - val_accuracy: 0.6273\n",
      "Epoch 46/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: \n",
      "2516/2516 [==============================] - 20s 8ms/step - loss: 0.9058 - accuracy: 0.7232 - val_loss: 1.5512 - val_accuracy: 0.6292\n",
      "Epoch 47/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: \n",
      "2516/2516 [==============================] - 20s 8ms/step - loss: 0.9025 - accuracy: 0.7238 - val_loss: 1.5548 - val_accuracy: 0.6237\n",
      "Epoch 48/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: 0.8969\n",
      "2516/2516 [==============================] - 17s 7ms/step - loss: 0.8968 - accuracy: 0.7256 - val_loss: 1.5501 - val_accuracy: 0.6289\n",
      "Epoch 49/60\n",
      "280/280 [==============================] - 0s 2ms/step loss: 0.8946 \n",
      "2516/2516 [==============================] - 16s 7ms/step - loss: 0.8946 - accuracy: 0.7276 - val_loss: 1.5395 - val_accuracy: 0.6283\n",
      "Epoch 50/60\n",
      "280/280 [==============================] - 0s 2ms/step loss: 0.8937 -\n",
      "2516/2516 [==============================] - 16s 6ms/step - loss: 0.8938 - accuracy: 0.7267 - val_loss: 1.5684 - val_accuracy: 0.6176\n",
      "Epoch 51/60\n",
      "280/280 [==============================] - 0s 2ms/step loss: 0.8847 \n",
      "2516/2516 [==============================] - 15s 6ms/step - loss: 0.8851 - accuracy: 0.7287 - val_loss: 1.6681 - val_accuracy: 0.6086\n",
      "Epoch 52/60\n",
      "280/280 [==============================] - 0s 1ms/step loss: 0.8837 \n",
      "2516/2516 [==============================] - 16s 6ms/step - loss: 0.8837 - accuracy: 0.7299 - val_loss: 1.5660 - val_accuracy: 0.6236\n",
      "Epoch 53/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: 0.8833\n",
      "2516/2516 [==============================] - 15s 6ms/step - loss: 0.8833 - accuracy: 0.7308 - val_loss: 1.5622 - val_accuracy: 0.6248\n",
      "Epoch 54/60\n",
      "280/280 [==============================] - 0s 1ms/step loss: 0.8826 - \n",
      "2516/2516 [==============================] - 15s 6ms/step - loss: 0.8826 - accuracy: 0.7294 - val_loss: 1.5609 - val_accuracy: 0.6321\n",
      "Epoch 55/60\n",
      "280/280 [==============================] - 0s 2ms/step loss: 0.8784 \n",
      "2516/2516 [==============================] - 15s 6ms/step - loss: 0.8788 - accuracy: 0.7306 - val_loss: 1.5972 - val_accuracy: 0.6214\n",
      "Epoch 56/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: 0.8\n",
      "2516/2516 [==============================] - 18s 7ms/step - loss: 0.8793 - accuracy: 0.7311 - val_loss: 1.5511 - val_accuracy: 0.6261\n",
      "Epoch 57/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: 0.87\n",
      "Epoch 57 - F1 Score: 0.6288\n",
      "Saved best model\n",
      "[0.4885580809058775, 0.556170455285328, 0.5553632299094174, 0.5734318560496712, 0.5899519551011747, 0.5852793105234771, 0.5965500051618823, 0.5907296665450869, 0.6139612083935275, 0.5983540285597523, 0.608785181158622, 0.5913404198241913, 0.6042153313144709, 0.598387406543427, 0.6219015923647799, 0.6198615942041825, 0.6111712551707829, 0.6065834921070664, 0.6206743968208857, 0.6272325330789489, 0.6079558916842273, 0.6170998924506769, 0.610177757082214, 0.6240077624559125, 0.604444576636432, 0.6186736069461806, 0.6217604176987024, 0.6153649581439886, 0.6273466688288905, 0.6233052167946387, 0.6208481403650356, 0.6246311025465299, 0.6168925499972802, 0.617115411477267, 0.6195887264697255, 0.6050589825725678, 0.6211966548219058, 0.6236179210896077, 0.6201094518815506, 0.6234997659734383, 0.6220153498420951, 0.6209395063668041, 0.6175603691383058, 0.6116890587470574, 0.6219140649581589, 0.6203556677779661, 0.6183323559947322, 0.6251927674662985, 0.6168999766217563, 0.6123912875535922, 0.5985032934271562, 0.6165981468358493, 0.621011023899858, 0.6225940733866814, 0.618541429905634, 0.6228202382094851, 0.6287734757967154]\n",
      "2516/2516 [==============================] - 18s 7ms/step - loss: 0.8718 - accuracy: 0.7336 - val_loss: 1.5459 - val_accuracy: 0.6308\n",
      "Epoch 58/60\n",
      "280/280 [==============================] - 1s 3ms/step lo\n",
      "2516/2516 [==============================] - 16s 6ms/step - loss: 0.8688 - accuracy: 0.7322 - val_loss: 1.6504 - val_accuracy: 0.6259\n",
      "Epoch 59/60\n",
      "280/280 [==============================] - 0s 1ms/step loss: 0.8681 - \n",
      "2516/2516 [==============================] - 14s 5ms/step - loss: 0.8680 - accuracy: 0.7326 - val_loss: 1.6155 - val_accuracy: 0.6215\n",
      "Epoch 60/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: 0.8\n",
      "2516/2516 [==============================] - 16s 7ms/step - loss: 0.8649 - accuracy: 0.7342 - val_loss: 1.5644 - val_accuracy: 0.6304\n",
      "485/485 [==============================] - 1s 2ms/step\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         119     0.8775    0.4455    0.5910      1174\n",
      "         120     0.4939    0.7980    0.6102       203\n",
      "         125     0.6343    0.7885    0.7030       572\n",
      "         134     0.4000    0.9474    0.5625        19\n",
      "         189     0.6993    0.8403    0.7634       119\n",
      "         190     0.6146    0.8852    0.7255       209\n",
      "          20     0.5534    0.1609    0.2493       870\n",
      "         200     0.6461    0.5332    0.5843       647\n",
      "         203     0.4267    0.8889    0.5766        36\n",
      "          22     0.9346    0.8146    0.8705       561\n",
      "         254     0.2174    0.1316    0.1639        38\n",
      "         255     0.3942    0.5942    0.4740        69\n",
      "         264     0.6872    0.5554    0.6143       542\n",
      "         269     0.3679    0.6500    0.4699       120\n",
      "         276     0.4494    0.5405    0.4908        74\n",
      "         284     0.4182    0.3566    0.3849       129\n",
      "         287     0.4458    0.7039    0.5459       304\n",
      "         295     0.8133    0.7262    0.7673        84\n",
      "         306     0.3333    0.5319    0.4098        94\n",
      "         310     0.8054    0.8054    0.8054       257\n",
      "         312     0.2626    0.5098    0.3467        51\n",
      "         319     0.3417    0.7455    0.4686        55\n",
      "         326     0.5000    0.3824    0.4333        34\n",
      "         327     0.3108    0.6053    0.4107        38\n",
      "         345     0.5000    0.2308    0.3158        26\n",
      "         347     0.4255    0.7407    0.5405        27\n",
      "         352     0.6575    0.8707    0.7492       549\n",
      "         362     0.6812    0.6763    0.6787       139\n",
      "         399     0.4070    0.7807    0.5350       269\n",
      "         400     0.3767    0.5827    0.4576       139\n",
      "         401     0.5882    0.7143    0.6452        56\n",
      "         415     0.7660    0.7826    0.7742        46\n",
      "         416     0.6943    0.8207    0.7522       357\n",
      "         426     0.6250    0.6818    0.6522        44\n",
      "         427     0.4706    0.8511    0.6061        47\n",
      "         434     0.5629    0.8910    0.6899       211\n",
      "         476     0.7575    0.8675    0.8088       234\n",
      "         502     0.6552    0.8559    0.7422       111\n",
      "         522     0.4336    0.5269    0.4757        93\n",
      "         532     0.5000    0.7800    0.6094        50\n",
      "          59     0.8000    0.7857    0.7928       112\n",
      "         601     0.4417    0.8675    0.5854        83\n",
      "         611     0.7073    0.8788    0.7838        99\n",
      "         617     0.6383    0.6977    0.6667        43\n",
      "         639     0.3704    0.6452    0.4706        31\n",
      "         668     0.1610    0.3800    0.2262        50\n",
      "         732     0.3953    0.3333    0.3617       102\n",
      "          74     0.0960    0.4146    0.1560        82\n",
      "         755     0.2039    0.5676    0.3000        37\n",
      "          77     0.3826    0.7500    0.5067       152\n",
      "         770     0.4400    0.4459    0.4430        74\n",
      "         772     0.6571    0.5897    0.6216        39\n",
      "          78     0.6329    0.6865    0.6586       319\n",
      "         787     0.6671    0.5510    0.6035       960\n",
      "          79     0.9867    0.5505    0.7067      2552\n",
      "         798     0.7013    0.8372    0.7633       129\n",
      "         835     0.3738    0.8889    0.5263        45\n",
      "         843     0.4394    0.7250    0.5472        40\n",
      "         862     0.5472    0.7467    0.6316       225\n",
      "         863     0.4091    0.2195    0.2857       123\n",
      "          89     0.9814    0.6346    0.7708      1081\n",
      "         908     0.1642    0.7857    0.2716        28\n",
      "         918     0.3591    0.9394    0.5196        99\n",
      "          94     0.3750    0.7255    0.4944       306\n",
      "\n",
      "    accuracy                         0.6180     15508\n",
      "   macro avg     0.5259    0.6600    0.5586     15508\n",
      "weighted avg     0.7045    0.6180    0.6247     15508\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['label_encoder_train.joblib']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import joblib\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class F1ScoreCallback(Callback):\n",
    "    def __init__(self, X_val, y_val):\n",
    "        super(F1ScoreCallback, self).__init__()\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.best_f1 = 0.0\n",
    "        self.best_model = None\n",
    "        self.f1_scores = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_val_pred = np.argmax(self.model.predict(self.X_val), axis=1)\n",
    "        f1 = f1_score(self.y_val, y_val_pred, average='weighted')\n",
    "        self.f1_scores.append(f1)\n",
    "        \n",
    "\n",
    "        if f1 > self.best_f1:\n",
    "            self.best_f1 = f1\n",
    "            self.best_model = self.model\n",
    "            print(f\"Epoch {epoch + 1} - F1 Score: {f1:.4f}\")\n",
    "            print(\"Saved best model\")\n",
    "            print(self.f1_scores)\n",
    "\n",
    "# Load JSON data with embeddings\n",
    "with open('train_phi.pickle', 'rb') as f1:\n",
    "    balanced = pickle.load(f1)\n",
    "\n",
    "with open('phi_test.pickle', 'rb') as f2:\n",
    "    unbalanced = pickle.load(f2)\n",
    "\n",
    "train = np.array([item['cve_description_phi_mean'] for item in balanced if item['cwe'] != 'None'])\n",
    "test = np.array([item['cwe'] for item in balanced if item['cwe'] != 'None'])\n",
    "np.random.seed(42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(train,test,test_size=0.1,random_state=42)\n",
    "\n",
    "X_test = np.array([item['cve_description_phi_mean'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "y_test = np.array([item['cwe'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "\n",
    "label_encoder_train = LabelEncoder()\n",
    "y_train_encoded = label_encoder_train.fit_transform(y_train)\n",
    "label_encoder_test = LabelEncoder()\n",
    "y_test_encoded = label_encoder_test.fit_transform(y_test)\n",
    "\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = len(np.unique(y_train))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=input_dim, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(output_dim, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "f1_callback = F1ScoreCallback(X_val, label_encoder_train.transform(y_val))\n",
    "\n",
    "history = model.fit(X_train, y_train_encoded, epochs=60, batch_size=32, validation_data=(X_val, label_encoder_train.transform(y_val)), verbose=1, callbacks=[f1_callback])\n",
    "\n",
    "best_model = f1_callback.best_model\n",
    "\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(best_model, 'CWE_classes.joblib')\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_probs = best_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "y_pred_original = label_encoder_train.inverse_transform(y_pred)\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_original, digits=4))\n",
    "joblib.dump(label_encoder_train, 'label_encoder_train.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run inference with the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "485/485 [==============================] - 1s 1ms/step\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         119     0.8574    0.4557    0.5951      1174\n",
      "         120     0.5357    0.7389    0.6211       203\n",
      "         125     0.6319    0.8252    0.7157       572\n",
      "         134     0.5152    0.8947    0.6538        19\n",
      "         189     0.7500    0.8067    0.7773       119\n",
      "         190     0.6805    0.8660    0.7621       209\n",
      "          20     0.4418    0.2966    0.3549       870\n",
      "         200     0.7449    0.5100    0.6055       647\n",
      "         203     0.6744    0.8056    0.7342        36\n",
      "          22     0.8792    0.8431    0.8608       561\n",
      "         254     0.1905    0.1053    0.1356        38\n",
      "         255     0.5323    0.4783    0.5038        69\n",
      "         264     0.7319    0.5037    0.5967       542\n",
      "         269     0.5312    0.5667    0.5484       120\n",
      "         276     0.3661    0.5541    0.4409        74\n",
      "         284     0.5479    0.3101    0.3960       129\n",
      "         287     0.4594    0.7072    0.5570       304\n",
      "         295     0.6737    0.7619    0.7151        84\n",
      "         306     0.4416    0.3617    0.3977        94\n",
      "         310     0.8195    0.8482    0.8337       257\n",
      "         312     0.4127    0.5098    0.4561        51\n",
      "         319     0.4675    0.6545    0.5455        55\n",
      "         326     0.4737    0.2647    0.3396        34\n",
      "         327     0.4419    0.5000    0.4691        38\n",
      "         345     0.2059    0.5385    0.2979        26\n",
      "         347     0.4490    0.8148    0.5789        27\n",
      "         352     0.5070    0.9290    0.6559       549\n",
      "         362     0.8208    0.6259    0.7102       139\n",
      "         399     0.4933    0.5465    0.5185       269\n",
      "         400     0.4277    0.5324    0.4744       139\n",
      "         401     0.7200    0.6429    0.6792        56\n",
      "         415     0.6923    0.7826    0.7347        46\n",
      "         416     0.6235    0.8908    0.7336       357\n",
      "         426     0.8000    0.5455    0.6486        44\n",
      "         427     0.6667    0.7660    0.7129        47\n",
      "         434     0.6811    0.8199    0.7441       211\n",
      "         476     0.7282    0.8932    0.8023       234\n",
      "         502     0.7344    0.8468    0.7866       111\n",
      "         522     0.4855    0.7204    0.5801        93\n",
      "         532     0.3134    0.8400    0.4565        50\n",
      "          59     0.5657    0.8839    0.6899       112\n",
      "         601     0.4128    0.8554    0.5569        83\n",
      "         611     0.8750    0.9192    0.8966        99\n",
      "         617     0.7333    0.7674    0.7500        43\n",
      "         639     0.2673    0.8710    0.4091        31\n",
      "         668     0.2708    0.2600    0.2653        50\n",
      "         732     0.4138    0.3529    0.3810       102\n",
      "          74     0.0612    0.7439    0.1132        82\n",
      "         755     0.4118    0.3784    0.3944        37\n",
      "          77     0.6863    0.4605    0.5512       152\n",
      "         770     0.3923    0.6892    0.5000        74\n",
      "         772     0.8667    0.6667    0.7536        39\n",
      "          78     0.5213    0.8809    0.6550       319\n",
      "         787     0.6421    0.6000    0.6204       960\n",
      "          79     0.9896    0.4471    0.6159      2552\n",
      "         798     0.8596    0.7597    0.8066       129\n",
      "         835     0.6296    0.7556    0.6869        45\n",
      "         843     0.5345    0.7750    0.6327        40\n",
      "         862     0.6590    0.6356    0.6471       225\n",
      "         863     0.3529    0.3902    0.3707       123\n",
      "          89     0.9688    0.6892    0.8054      1081\n",
      "         908     0.5000    0.4643    0.4815        28\n",
      "         918     0.8723    0.8283    0.8497        99\n",
      "          94     0.3852    0.6634    0.4874       306\n",
      "\n",
      "    accuracy                         0.6124     15508\n",
      "   macro avg     0.5785    0.6506    0.5883     15508\n",
      "weighted avg     0.7122    0.6124    0.6258     15508\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Save the best model\n",
    "with open('phi_test.pickle', 'rb') as f2:\n",
    "    unbalanced = pickle.load(f2)\n",
    "\n",
    "X_test = np.array([item['cve_description_phi_mean'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "y_test = np.array([item['cwe'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "\n",
    "best_model=joblib.load('CWE_classes.joblib')\n",
    "label_encoder_train=joblib.load('label_encoder_train.joblib')\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_probs = best_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "y_pred_original = label_encoder_train.inverse_transform(y_pred)\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_original, digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
