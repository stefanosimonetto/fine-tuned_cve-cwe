{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the best F1-score on the first 50 epochs and extract the best model\n",
    "\n",
    "Embedded: \n",
    "\"\"\"\n",
    "### CVE description:\n",
    "CVE description\n",
    "\n",
    "### Predicted CWE:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "280/280 [==============================] - 2s 6ms/step\n",
      "Epoch 1 - F1 Score: 0.6566\n",
      "Saved best model\n",
      "[0.6566109175742626]\n",
      "2516/2516 [==============================] - 60s 22ms/step - loss: 1.4249 - accuracy: 0.6129 - val_loss: 1.1638 - val_accuracy: 0.6704\n",
      "Epoch 2/60\n",
      "280/280 [==============================] - 1s 3ms/step loss\n",
      "Epoch 2 - F1 Score: 0.6623\n",
      "Saved best model\n",
      "[0.6566109175742626, 0.6623067045206272]\n",
      "2516/2516 [==============================] - 45s 18ms/step - loss: 1.1581 - accuracy: 0.6704 - val_loss: 1.1775 - val_accuracy: 0.6695\n",
      "Epoch 3/60\n",
      "280/280 [==============================] - 2s 6ms/step\n",
      "Epoch 3 - F1 Score: 0.6735\n",
      "Saved best model\n",
      "[0.6566109175742626, 0.6623067045206272, 0.673467073855684]\n",
      "2516/2516 [==============================] - 50s 20ms/step - loss: 1.0949 - accuracy: 0.6872 - val_loss: 1.1187 - val_accuracy: 0.6856\n",
      "Epoch 4/60\n",
      "280/280 [==============================] - 2s 6ms/step\n",
      "Epoch 4 - F1 Score: 0.6835\n",
      "Saved best model\n",
      "[0.6566109175742626, 0.6623067045206272, 0.673467073855684, 0.6835090776280909]\n",
      "2516/2516 [==============================] - 66s 26ms/step - loss: 1.0602 - accuracy: 0.6967 - val_loss: 1.1178 - val_accuracy: 0.6931\n",
      "Epoch 5/60\n",
      "280/280 [==============================] - 2s 6ms/step\n",
      "Epoch 5 - F1 Score: 0.6960\n",
      "Saved best model\n",
      "[0.6566109175742626, 0.6623067045206272, 0.673467073855684, 0.6835090776280909, 0.6960332422551533]\n",
      "2516/2516 [==============================] - 64s 25ms/step - loss: 1.0312 - accuracy: 0.7029 - val_loss: 1.0828 - val_accuracy: 0.7016\n",
      "Epoch 6/60\n",
      "280/280 [==============================] - 2s 6ms/step\n",
      "2516/2516 [==============================] - 65s 26ms/step - loss: 1.0089 - accuracy: 0.7113 - val_loss: 1.0802 - val_accuracy: 0.6990\n",
      "Epoch 7/60\n",
      "280/280 [==============================] - 2s 7ms/step\n",
      "2516/2516 [==============================] - 67s 27ms/step - loss: 0.9919 - accuracy: 0.7120 - val_loss: 1.0746 - val_accuracy: 0.7002\n",
      "Epoch 8/60\n",
      "280/280 [==============================] - 1s 5ms/step\n",
      "2516/2516 [==============================] - 61s 24ms/step - loss: 0.9707 - accuracy: 0.7165 - val_loss: 1.0958 - val_accuracy: 0.6908\n",
      "Epoch 9/60\n",
      "280/280 [==============================] - 2s 7ms/step\n",
      "2516/2516 [==============================] - 65s 26ms/step - loss: 0.9600 - accuracy: 0.7193 - val_loss: 1.0976 - val_accuracy: 0.7008\n",
      "Epoch 10/60\n",
      "280/280 [==============================] - 2s 7ms/step\n",
      "Epoch 10 - F1 Score: 0.7005\n",
      "Saved best model\n",
      "[0.6566109175742626, 0.6623067045206272, 0.673467073855684, 0.6835090776280909, 0.6960332422551533, 0.6862365590016538, 0.6939764819045576, 0.6885548052882945, 0.6898323278932528, 0.700463747468592]\n",
      "2516/2516 [==============================] - 68s 27ms/step - loss: 0.9428 - accuracy: 0.7242 - val_loss: 1.0493 - val_accuracy: 0.7094\n",
      "Epoch 11/60\n",
      "280/280 [==============================] - 2s 7ms/step\n",
      "Epoch 11 - F1 Score: 0.7018\n",
      "Saved best model\n",
      "[0.6566109175742626, 0.6623067045206272, 0.673467073855684, 0.6835090776280909, 0.6960332422551533, 0.6862365590016538, 0.6939764819045576, 0.6885548052882945, 0.6898323278932528, 0.700463747468592, 0.7017569140538578]\n",
      "2516/2516 [==============================] - 67s 27ms/step - loss: 0.9357 - accuracy: 0.7254 - val_loss: 1.0602 - val_accuracy: 0.7056\n",
      "Epoch 12/60\n",
      "280/280 [==============================] - 2s 7ms/step\n",
      "Epoch 12 - F1 Score: 0.7023\n",
      "Saved best model\n",
      "[0.6566109175742626, 0.6623067045206272, 0.673467073855684, 0.6835090776280909, 0.6960332422551533, 0.6862365590016538, 0.6939764819045576, 0.6885548052882945, 0.6898323278932528, 0.700463747468592, 0.7017569140538578, 0.7023348940671393]\n",
      "2516/2516 [==============================] - 65s 26ms/step - loss: 0.9234 - accuracy: 0.7300 - val_loss: 1.0751 - val_accuracy: 0.7099\n",
      "Epoch 13/60\n",
      "280/280 [==============================] - 2s 7ms/step\n",
      "Epoch 13 - F1 Score: 0.7045\n",
      "Saved best model\n",
      "[0.6566109175742626, 0.6623067045206272, 0.673467073855684, 0.6835090776280909, 0.6960332422551533, 0.6862365590016538, 0.6939764819045576, 0.6885548052882945, 0.6898323278932528, 0.700463747468592, 0.7017569140538578, 0.7023348940671393, 0.7044798098735754]\n",
      "2516/2516 [==============================] - 66s 26ms/step - loss: 0.9160 - accuracy: 0.7304 - val_loss: 1.0584 - val_accuracy: 0.7106\n",
      "Epoch 14/60\n",
      "280/280 [==============================] - 2s 6ms/step\n",
      "Epoch 14 - F1 Score: 0.7138\n",
      "Saved best model\n",
      "[0.6566109175742626, 0.6623067045206272, 0.673467073855684, 0.6835090776280909, 0.6960332422551533, 0.6862365590016538, 0.6939764819045576, 0.6885548052882945, 0.6898323278932528, 0.700463747468592, 0.7017569140538578, 0.7023348940671393, 0.7044798098735754, 0.7137885365636368]\n",
      "2516/2516 [==============================] - 67s 27ms/step - loss: 0.9047 - accuracy: 0.7337 - val_loss: 1.0528 - val_accuracy: 0.7183\n",
      "Epoch 15/60\n",
      "280/280 [==============================] - 2s 6ms/step\n",
      "2516/2516 [==============================] - 66s 26ms/step - loss: 0.8939 - accuracy: 0.7370 - val_loss: 1.0764 - val_accuracy: 0.7074\n",
      "Epoch 16/60\n",
      "280/280 [==============================] - 3s 8ms/step\n",
      "2516/2516 [==============================] - 67s 27ms/step - loss: 0.8887 - accuracy: 0.7370 - val_loss: 1.0309 - val_accuracy: 0.7156\n",
      "Epoch 17/60\n",
      "280/280 [==============================] - 2s 7ms/step\n",
      "2516/2516 [==============================] - 65s 26ms/step - loss: 0.8826 - accuracy: 0.7388 - val_loss: 1.0427 - val_accuracy: 0.7178\n",
      "Epoch 18/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: 0.\n",
      "2516/2516 [==============================] - 60s 24ms/step - loss: 0.8762 - accuracy: 0.7401 - val_loss: 1.0625 - val_accuracy: 0.7155\n",
      "Epoch 19/60\n",
      "280/280 [==============================] - 1s 3ms/step lo\n",
      "2516/2516 [==============================] - 23s 9ms/step - loss: 0.8713 - accuracy: 0.7419 - val_loss: 1.0543 - val_accuracy: 0.7128\n",
      "Epoch 20/60\n",
      "280/280 [==============================] - 1s 3ms/step lo\n",
      "2516/2516 [==============================] - 25s 10ms/step - loss: 0.8595 - accuracy: 0.7433 - val_loss: 1.0657 - val_accuracy: 0.7147\n",
      "Epoch 21/60\n",
      "280/280 [==============================] - 1s 3ms/step los\n",
      "2516/2516 [==============================] - 24s 10ms/step - loss: 0.8546 - accuracy: 0.7454 - val_loss: 1.0783 - val_accuracy: 0.7090\n",
      "Epoch 22/60\n",
      "280/280 [==============================] - 1s 2ms/step loss\n",
      "2516/2516 [==============================] - 25s 10ms/step - loss: 0.8462 - accuracy: 0.7474 - val_loss: 1.0810 - val_accuracy: 0.7108\n",
      "Epoch 23/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: \n",
      "2516/2516 [==============================] - 25s 10ms/step - loss: 0.8429 - accuracy: 0.7487 - val_loss: 1.0937 - val_accuracy: 0.7094\n",
      "Epoch 24/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: 0.\n",
      "2516/2516 [==============================] - 25s 10ms/step - loss: 0.8384 - accuracy: 0.7498 - val_loss: 1.1000 - val_accuracy: 0.7087\n",
      "Epoch 25/60\n",
      "280/280 [==============================] - 1s 2ms/step loss\n",
      "2516/2516 [==============================] - 25s 10ms/step - loss: 0.8319 - accuracy: 0.7497 - val_loss: 1.0827 - val_accuracy: 0.7111\n",
      "Epoch 26/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: \n",
      "2516/2516 [==============================] - 24s 10ms/step - loss: 0.8233 - accuracy: 0.7526 - val_loss: 1.0772 - val_accuracy: 0.7105\n",
      "Epoch 27/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: \n",
      "2516/2516 [==============================] - 24s 10ms/step - loss: 0.8192 - accuracy: 0.7538 - val_loss: 1.1049 - val_accuracy: 0.7127\n",
      "Epoch 28/60\n",
      "280/280 [==============================] - 1s 3ms/step lo\n",
      "2516/2516 [==============================] - 25s 10ms/step - loss: 0.8195 - accuracy: 0.7538 - val_loss: 1.0658 - val_accuracy: 0.7181\n",
      "Epoch 29/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: \n",
      "2516/2516 [==============================] - 24s 10ms/step - loss: 0.8087 - accuracy: 0.7571 - val_loss: 1.0838 - val_accuracy: 0.7155\n",
      "Epoch 30/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: \n",
      "2516/2516 [==============================] - 25s 10ms/step - loss: 0.8066 - accuracy: 0.7568 - val_loss: 1.0958 - val_accuracy: 0.7153\n",
      "Epoch 31/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: \n",
      "2516/2516 [==============================] - 23s 9ms/step - loss: 0.8015 - accuracy: 0.7581 - val_loss: 1.1016 - val_accuracy: 0.7121\n",
      "Epoch 32/60\n",
      "280/280 [==============================] - 1s 3ms/step loss\n",
      "2516/2516 [==============================] - 23s 9ms/step - loss: 0.7963 - accuracy: 0.7592 - val_loss: 1.1230 - val_accuracy: 0.7063\n",
      "Epoch 33/60\n",
      "280/280 [==============================] - 1s 3ms/step los\n",
      "2516/2516 [==============================] - 23s 9ms/step - loss: 0.7915 - accuracy: 0.7591 - val_loss: 1.1392 - val_accuracy: 0.7088\n",
      "Epoch 34/60\n",
      "280/280 [==============================] - 1s 3ms/step lo\n",
      "2516/2516 [==============================] - 23s 9ms/step - loss: 0.7925 - accuracy: 0.7604 - val_loss: 1.0896 - val_accuracy: 0.7109\n",
      "Epoch 35/60\n",
      "280/280 [==============================] - 1s 2ms/step loss\n",
      "2516/2516 [==============================] - 21s 8ms/step - loss: 0.7846 - accuracy: 0.7617 - val_loss: 1.1456 - val_accuracy: 0.7103\n",
      "Epoch 36/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: \n",
      "2516/2516 [==============================] - 19s 8ms/step - loss: 0.7841 - accuracy: 0.7624 - val_loss: 1.1402 - val_accuracy: 0.7132\n",
      "Epoch 37/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: 0.\n",
      "2516/2516 [==============================] - 20s 8ms/step - loss: 0.7778 - accuracy: 0.7632 - val_loss: 1.1109 - val_accuracy: 0.7127\n",
      "Epoch 38/60\n",
      "280/280 [==============================] - 1s 3ms/step loss\n",
      "2516/2516 [==============================] - 20s 8ms/step - loss: 0.7715 - accuracy: 0.7656 - val_loss: 1.1416 - val_accuracy: 0.7079\n",
      "Epoch 39/60\n",
      "280/280 [==============================] - 1s 3ms/step loss\n",
      "2516/2516 [==============================] - 22s 9ms/step - loss: 0.7701 - accuracy: 0.7647 - val_loss: 1.1565 - val_accuracy: 0.7111\n",
      "Epoch 40/60\n",
      "280/280 [==============================] - 1s 3ms/step loss: \n",
      "2516/2516 [==============================] - 23s 9ms/step - loss: 0.7690 - accuracy: 0.7662 - val_loss: 1.1258 - val_accuracy: 0.7134\n",
      "Epoch 41/60\n",
      "280/280 [==============================] - 1s 3ms/step lo\n",
      "2516/2516 [==============================] - 23s 9ms/step - loss: 0.7642 - accuracy: 0.7668 - val_loss: 1.1414 - val_accuracy: 0.7120\n",
      "Epoch 42/60\n",
      "280/280 [==============================] - 1s 3ms/step \n",
      "Epoch 42 - F1 Score: 0.7171\n",
      "Saved best model\n",
      "[0.6566109175742626, 0.6623067045206272, 0.673467073855684, 0.6835090776280909, 0.6960332422551533, 0.6862365590016538, 0.6939764819045576, 0.6885548052882945, 0.6898323278932528, 0.700463747468592, 0.7017569140538578, 0.7023348940671393, 0.7044798098735754, 0.7137885365636368, 0.6993472431118214, 0.7099653133621664, 0.7112344169156531, 0.7102056356710146, 0.705980788353426, 0.7086994116087885, 0.7053153431925386, 0.7087808587085093, 0.7038895142402938, 0.7055089686280753, 0.7089764908202666, 0.7047734081416448, 0.705851822478334, 0.7136705619587285, 0.7101966489360817, 0.7089986739398321, 0.7118707375986151, 0.7033246630772811, 0.7046904090194608, 0.7069115251108473, 0.7074026587117994, 0.7080007439308896, 0.710676219091098, 0.7054559418933355, 0.7060074461821174, 0.7080229405233545, 0.7086078222361164, 0.7171098132957224]\n",
      "2516/2516 [==============================] - 23s 9ms/step - loss: 0.7588 - accuracy: 0.7676 - val_loss: 1.1138 - val_accuracy: 0.7188\n",
      "Epoch 43/60\n",
      "280/280 [==============================] - 1s 3ms/step loss\n",
      "2516/2516 [==============================] - 23s 9ms/step - loss: 0.7567 - accuracy: 0.7687 - val_loss: 1.1623 - val_accuracy: 0.7126\n",
      "Epoch 44/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: \n",
      "2516/2516 [==============================] - 22s 9ms/step - loss: 0.7569 - accuracy: 0.7697 - val_loss: 1.1463 - val_accuracy: 0.7102\n",
      "Epoch 45/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: \n",
      "2516/2516 [==============================] - 21s 8ms/step - loss: 0.7484 - accuracy: 0.7710 - val_loss: 1.1625 - val_accuracy: 0.7149\n",
      "Epoch 46/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: \n",
      "2516/2516 [==============================] - 20s 8ms/step - loss: 0.7456 - accuracy: 0.7720 - val_loss: 1.2122 - val_accuracy: 0.7028\n",
      "Epoch 47/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: \n",
      "2516/2516 [==============================] - 22s 9ms/step - loss: 0.7459 - accuracy: 0.7710 - val_loss: 1.1450 - val_accuracy: 0.7184\n",
      "Epoch 48/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: \n",
      "2516/2516 [==============================] - 20s 8ms/step - loss: 0.7420 - accuracy: 0.7713 - val_loss: 1.1627 - val_accuracy: 0.7112\n",
      "Epoch 49/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: 0.\n",
      "2516/2516 [==============================] - 20s 8ms/step - loss: 0.7375 - accuracy: 0.7742 - val_loss: 1.1853 - val_accuracy: 0.7021\n",
      "Epoch 50/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: 0.\n",
      "2516/2516 [==============================] - 20s 8ms/step - loss: 0.7378 - accuracy: 0.7730 - val_loss: 1.1537 - val_accuracy: 0.7150\n",
      "Epoch 51/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: 0.730\n",
      "2516/2516 [==============================] - 17s 7ms/step - loss: 0.7302 - accuracy: 0.7748 - val_loss: 1.1893 - val_accuracy: 0.7105\n",
      "Epoch 52/60\n",
      "280/280 [==============================] - 0s 2ms/step loss: 0.7296 \n",
      "2516/2516 [==============================] - 16s 6ms/step - loss: 0.7296 - accuracy: 0.7749 - val_loss: 1.2161 - val_accuracy: 0.7077\n",
      "Epoch 53/60\n",
      "280/280 [==============================] - 0s 2ms/step loss: 0.7286 \n",
      "2516/2516 [==============================] - 16s 6ms/step - loss: 0.7287 - accuracy: 0.7745 - val_loss: 1.1790 - val_accuracy: 0.7123\n",
      "Epoch 54/60\n",
      "280/280 [==============================] - 0s 2ms/step loss: 0.7235 \n",
      "2516/2516 [==============================] - 15s 6ms/step - loss: 0.7234 - accuracy: 0.7769 - val_loss: 1.2494 - val_accuracy: 0.6993\n",
      "Epoch 55/60\n",
      "280/280 [==============================] - 0s 1ms/step loss: 0.7246 -\n",
      "2516/2516 [==============================] - 16s 6ms/step - loss: 0.7247 - accuracy: 0.7761 - val_loss: 1.2002 - val_accuracy: 0.7061\n",
      "Epoch 56/60\n",
      "280/280 [==============================] - 0s 2ms/step loss: 0.7185 \n",
      "2516/2516 [==============================] - 16s 6ms/step - loss: 0.7186 - accuracy: 0.7782 - val_loss: 1.1885 - val_accuracy: 0.7094\n",
      "Epoch 57/60\n",
      "280/280 [==============================] - 0s 1ms/step loss: 0.7157 \n",
      "2516/2516 [==============================] - 15s 6ms/step - loss: 0.7158 - accuracy: 0.7766 - val_loss: 1.2060 - val_accuracy: 0.7042\n",
      "Epoch 58/60\n",
      "280/280 [==============================] - 0s 2ms/step loss: 0.7130 \n",
      "2516/2516 [==============================] - 15s 6ms/step - loss: 0.7132 - accuracy: 0.7783 - val_loss: 1.2098 - val_accuracy: 0.6960\n",
      "Epoch 59/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: 0.70\n",
      "2516/2516 [==============================] - 19s 7ms/step - loss: 0.7073 - accuracy: 0.7801 - val_loss: 1.1932 - val_accuracy: 0.7083\n",
      "Epoch 60/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: 0\n",
      "2516/2516 [==============================] - 17s 7ms/step - loss: 0.7096 - accuracy: 0.7796 - val_loss: 1.2115 - val_accuracy: 0.7078\n",
      "485/485 [==============================] - 1s 2ms/step\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         119     0.9317    0.3952    0.5550      1174\n",
      "         120     0.4712    0.8867    0.6154       203\n",
      "         125     0.8284    0.8864    0.8564       572\n",
      "         134     0.8000    0.8421    0.8205        19\n",
      "         189     0.7615    0.8319    0.7952       119\n",
      "         190     0.8750    0.8373    0.8557       209\n",
      "          20     0.4581    0.2448    0.3191       870\n",
      "         200     0.7329    0.5471    0.6265       647\n",
      "         203     0.7297    0.7500    0.7397        36\n",
      "          22     0.8232    0.9376    0.8767       561\n",
      "         254     0.2381    0.1316    0.1695        38\n",
      "         255     0.5246    0.4638    0.4923        69\n",
      "         264     0.6565    0.6347    0.6454       542\n",
      "         269     0.5372    0.5417    0.5394       120\n",
      "         276     0.5303    0.4730    0.5000        74\n",
      "         284     0.3140    0.4186    0.3588       129\n",
      "         287     0.5061    0.6875    0.5830       304\n",
      "         295     0.8667    0.7738    0.8176        84\n",
      "         306     0.4103    0.6809    0.5120        94\n",
      "         310     0.8664    0.7821    0.8221       257\n",
      "         312     0.4516    0.2745    0.3415        51\n",
      "         319     0.6167    0.6727    0.6435        55\n",
      "         326     0.5152    0.5000    0.5075        34\n",
      "         327     0.6154    0.2105    0.3137        38\n",
      "         345     0.6875    0.4231    0.5238        26\n",
      "         347     0.8000    0.5926    0.6809        27\n",
      "         352     0.6267    0.9417    0.7525       549\n",
      "         362     0.7660    0.7770    0.7714       139\n",
      "         399     0.4707    0.6877    0.5589       269\n",
      "         400     0.4638    0.4604    0.4621       139\n",
      "         401     0.8529    0.5179    0.6444        56\n",
      "         415     0.9714    0.7391    0.8395        46\n",
      "         416     0.9352    0.8487    0.8899       357\n",
      "         426     0.5606    0.8409    0.6727        44\n",
      "         427     0.7632    0.6170    0.6824        47\n",
      "         434     0.4868    0.8768    0.6261       211\n",
      "         476     0.7590    0.9017    0.8242       234\n",
      "         502     0.8750    0.8198    0.8465       111\n",
      "         522     0.5308    0.7419    0.6188        93\n",
      "         532     0.5065    0.7800    0.6142        50\n",
      "          59     0.6992    0.8304    0.7592       112\n",
      "         601     0.4093    0.9518    0.5725        83\n",
      "         611     0.7442    0.9697    0.8421        99\n",
      "         617     0.7059    0.8372    0.7660        43\n",
      "         639     0.4286    0.4839    0.4545        31\n",
      "         668     0.2526    0.4800    0.3310        50\n",
      "         732     0.4219    0.5294    0.4696       102\n",
      "          74     0.1012    0.6098    0.1736        82\n",
      "         755     0.3125    0.6757    0.4274        37\n",
      "          77     0.5629    0.6184    0.5893       152\n",
      "         770     0.4146    0.6892    0.5178        74\n",
      "         772     0.6750    0.6923    0.6835        39\n",
      "          78     0.6184    0.8433    0.7135       319\n",
      "         787     0.7150    0.7812    0.7466       960\n",
      "          79     0.9966    0.5827    0.7354      2552\n",
      "         798     0.8244    0.8372    0.8308       129\n",
      "         835     0.5493    0.8667    0.6724        45\n",
      "         843     0.6939    0.8500    0.7640        40\n",
      "         862     0.3699    0.8400    0.5136       225\n",
      "         863     0.3280    0.3333    0.3306       123\n",
      "          89     0.9947    0.6994    0.8213      1081\n",
      "         908     0.5227    0.8214    0.6389        28\n",
      "         918     0.8812    0.8990    0.8900        99\n",
      "          94     0.3688    0.7582    0.4963       306\n",
      "\n",
      "    accuracy                         0.6642     15508\n",
      "   macro avg     0.6204    0.6789    0.6259     15508\n",
      "weighted avg     0.7423    0.6642    0.6715     15508\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['label_encoder_train.joblib']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import joblib\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class F1ScoreCallback(Callback):\n",
    "    def __init__(self, X_val, y_val):\n",
    "        super(F1ScoreCallback, self).__init__()\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.best_f1 = 0.0\n",
    "        self.best_model = None\n",
    "        self.f1_scores = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_val_pred = np.argmax(self.model.predict(self.X_val), axis=1)\n",
    "        f1 = f1_score(self.y_val, y_val_pred, average='weighted')\n",
    "        self.f1_scores.append(f1)\n",
    "        \n",
    "\n",
    "        if f1 > self.best_f1:\n",
    "            self.best_f1 = f1\n",
    "            self.best_model = self.model\n",
    "            print(f\"Epoch {epoch + 1} - F1 Score: {f1:.4f}\")\n",
    "            print(\"Saved best model\")\n",
    "            print(self.f1_scores)\n",
    "\n",
    "with open('train_fine-tuned_phi.pickle', 'rb') as f1:\n",
    "    balanced = pickle.load(f1)\n",
    "\n",
    "with open('test_fine-tuned_phi.pickle', 'rb') as f2:\n",
    "    unbalanced = pickle.load(f2)\n",
    "\n",
    "train = np.array([item['cve_description_phi_mean'] for item in balanced if item['cwe'] != 'None'])\n",
    "test = np.array([item['cwe'] for item in balanced if item['cwe'] != 'None'])\n",
    "np.random.seed(42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(train,test,test_size=0.1,random_state=42)\n",
    "\n",
    "X_test = np.array([item['cve_description_phi_mean'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "y_test = np.array([item['cwe'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "\n",
    "label_encoder_train = LabelEncoder()\n",
    "y_train_encoded = label_encoder_train.fit_transform(y_train)\n",
    "label_encoder_test = LabelEncoder()\n",
    "y_test_encoded = label_encoder_test.fit_transform(y_test)\n",
    "\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = len(np.unique(y_train))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=input_dim, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(output_dim, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "f1_callback = F1ScoreCallback(X_val, label_encoder_train.transform(y_val))\n",
    "\n",
    "history = model.fit(X_train, y_train_encoded, epochs=60, batch_size=32, validation_data=(X_val, label_encoder_train.transform(y_val)), verbose=1, callbacks=[f1_callback])\n",
    "\n",
    "best_model = f1_callback.best_model\n",
    "\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(best_model, 'CWE_classes.joblib')\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_probs = best_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "y_pred_original = label_encoder_train.inverse_transform(y_pred)\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_original, digits=4))\n",
    "joblib.dump(label_encoder_train, 'label_encoder_train.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run inference with the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "485/485 [==============================] - 1s 2ms/step\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         119     0.8938    0.4514    0.5999      1174\n",
      "         120     0.4450    0.9163    0.5990       203\n",
      "         125     0.7821    0.9038    0.8386       572\n",
      "         134     0.5625    0.9474    0.7059        19\n",
      "         189     0.7410    0.8655    0.7984       119\n",
      "         190     0.8310    0.8469    0.8389       209\n",
      "          20     0.5880    0.2034    0.3023       870\n",
      "         200     0.7805    0.5440    0.6412       647\n",
      "         203     0.7447    0.9722    0.8434        36\n",
      "          22     0.9032    0.8984    0.9008       561\n",
      "         254     0.2941    0.1316    0.1818        38\n",
      "         255     0.5333    0.5797    0.5556        69\n",
      "         264     0.6716    0.6642    0.6679       542\n",
      "         269     0.3803    0.7417    0.5028       120\n",
      "         276     0.4918    0.4054    0.4444        74\n",
      "         284     0.3636    0.4031    0.3824       129\n",
      "         287     0.5515    0.7928    0.6505       304\n",
      "         295     0.8313    0.8214    0.8263        84\n",
      "         306     0.5904    0.5213    0.5537        94\n",
      "         310     0.8014    0.8949    0.8456       257\n",
      "         312     0.5000    0.2549    0.3377        51\n",
      "         319     0.3894    0.8000    0.5238        55\n",
      "         326     0.5625    0.5294    0.5455        34\n",
      "         327     0.7500    0.4737    0.5806        38\n",
      "         345     0.3421    0.5000    0.4063        26\n",
      "         347     0.7333    0.8148    0.7719        27\n",
      "         352     0.8085    0.9381    0.8685       549\n",
      "         362     0.7762    0.7986    0.7872       139\n",
      "         399     0.4785    0.8290    0.6068       269\n",
      "         400     0.4822    0.6835    0.5655       139\n",
      "         401     0.7174    0.5893    0.6471        56\n",
      "         415     0.8824    0.9783    0.9278        46\n",
      "         416     0.8698    0.8796    0.8747       357\n",
      "         426     0.6047    0.5909    0.5977        44\n",
      "         427     0.6230    0.8085    0.7037        47\n",
      "         434     0.5348    0.9100    0.6737       211\n",
      "         476     0.7235    0.9060    0.8046       234\n",
      "         502     0.6667    0.9189    0.7727       111\n",
      "         522     0.5032    0.8387    0.6290        93\n",
      "         532     0.5125    0.8200    0.6308        50\n",
      "          59     0.7132    0.8661    0.7823       112\n",
      "         601     0.5748    0.8795    0.6952        83\n",
      "         611     0.7886    0.9798    0.8739        99\n",
      "         617     0.7358    0.9070    0.8125        43\n",
      "         639     0.5526    0.6774    0.6087        31\n",
      "         668     0.2857    0.4400    0.3465        50\n",
      "         732     0.3275    0.5490    0.4103       102\n",
      "          74     0.1624    0.5366    0.2493        82\n",
      "         755     0.5588    0.5135    0.5352        37\n",
      "          77     0.6629    0.3882    0.4896       152\n",
      "         770     0.5873    0.5000    0.5401        74\n",
      "         772     0.5636    0.7949    0.6596        39\n",
      "          78     0.5962    0.8746    0.7090       319\n",
      "         787     0.7098    0.8052    0.7545       960\n",
      "          79     0.9953    0.7398    0.8487      2552\n",
      "         798     0.8640    0.8372    0.8504       129\n",
      "         835     0.7170    0.8444    0.7755        45\n",
      "         843     0.7500    0.8250    0.7857        40\n",
      "         862     0.6527    0.7600    0.7023       225\n",
      "         863     0.4492    0.4309    0.4398       123\n",
      "          89     0.9949    0.9038    0.9472      1081\n",
      "         908     0.6286    0.7857    0.6984        28\n",
      "         918     0.8614    0.8788    0.8700        99\n",
      "          94     0.5405    0.7418    0.6253       306\n",
      "\n",
      "    accuracy                         0.7201     15508\n",
      "   macro avg     0.6330    0.7160    0.6554     15508\n",
      "weighted avg     0.7628    0.7201    0.7185     15508\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Save the best model\n",
    "with open('test_fine-tuned_phi.pickle', 'rb') as f2:\n",
    "    unbalanced = pickle.load(f2)\n",
    "\n",
    "X_test = np.array([item['cve_description_phi_mean'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "y_test = np.array([item['cwe'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "\n",
    "best_model=joblib.load('CWE_classes.joblib')\n",
    "label_encoder_train=joblib.load('label_encoder_train.joblib')\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_probs = best_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "y_pred_original = label_encoder_train.inverse_transform(y_pred)\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_original, digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
