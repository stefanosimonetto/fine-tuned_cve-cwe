{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the best F1-score on the first 40 epochs and extract the best model\n",
    "\n",
    "Embedded: CVE description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train examples:  84025\n",
      "Number of classes :  64\n",
      "Test examples:  15508\n",
      "Number of classes :  64\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import pickle\n",
    "with open('train_without_test2.pickle', 'rb') as f:\n",
    "    train = pickle.load(f)\n",
    "\n",
    "print(\"Train examples: \", len(train))\n",
    "cve_counter = Counter(entry['cwe']for entry in train)\n",
    "print(\"Number of classes : \", len(cve_counter))\n",
    "\n",
    "with open('test_ada.pickle', 'rb') as f:\n",
    "    test = pickle.load(f)\n",
    "\n",
    "print(\"Test examples: \", len(test))\n",
    "cve_counter = Counter(entry['cwe']for entry in test)\n",
    "print(\"Number of classes : \", len(cve_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "263/263 [==============================] - 1s 2ms/step loss: 1.99\n",
      "Epoch 1 - F1 Score: 0.5790\n",
      "Saved best model\n",
      "[0.5790137671930181]\n",
      "2364/2364 [==============================] - 13s 4ms/step - loss: 1.9921 - accuracy: 0.4714 - val_loss: 1.4192 - val_accuracy: 0.6055\n",
      "Epoch 2/40\n",
      "263/263 [==============================] - 1s 2ms/step loss: 1.297\n",
      "Epoch 2 - F1 Score: 0.6297\n",
      "Saved best model\n",
      "[0.5790137671930181, 0.6297322253014483]\n",
      "2364/2364 [==============================] - 10s 4ms/step - loss: 1.2978 - accuracy: 0.6295 - val_loss: 1.2340 - val_accuracy: 0.6481\n",
      "Epoch 3/40\n",
      "263/263 [==============================] - 0s 2ms/step loss: 1.1664 \n",
      "Epoch 3 - F1 Score: 0.6523\n",
      "Saved best model\n",
      "[0.5790137671930181, 0.6297322253014483, 0.6523256221016372]\n",
      "2364/2364 [==============================] - 9s 4ms/step - loss: 1.1668 - accuracy: 0.6634 - val_loss: 1.1835 - val_accuracy: 0.6624\n",
      "Epoch 4/40\n",
      "263/263 [==============================] - 0s 2ms/step loss: 1.0985 \n",
      "Epoch 4 - F1 Score: 0.6613\n",
      "Saved best model\n",
      "[0.5790137671930181, 0.6297322253014483, 0.6523256221016372, 0.6612526562136407]\n",
      "2364/2364 [==============================] - 10s 4ms/step - loss: 1.0985 - accuracy: 0.6825 - val_loss: 1.1305 - val_accuracy: 0.6743\n",
      "Epoch 5/40\n",
      "263/263 [==============================] - 1s 3ms/step l\n",
      "Epoch 5 - F1 Score: 0.6728\n",
      "Saved best model\n",
      "[0.5790137671930181, 0.6297322253014483, 0.6523256221016372, 0.6612526562136407, 0.6728446079723402]\n",
      "2364/2364 [==============================] - 10s 4ms/step - loss: 1.0502 - accuracy: 0.6954 - val_loss: 1.0924 - val_accuracy: 0.6843\n",
      "Epoch 6/40\n",
      "263/263 [==============================] - 1s 3ms/step los\n",
      "Epoch 6 - F1 Score: 0.6809\n",
      "Saved best model\n",
      "[0.5790137671930181, 0.6297322253014483, 0.6523256221016372, 0.6612526562136407, 0.6728446079723402, 0.6809108495972687]\n",
      "2364/2364 [==============================] - 10s 4ms/step - loss: 1.0135 - accuracy: 0.7043 - val_loss: 1.0900 - val_accuracy: 0.6895\n",
      "Epoch 7/40\n",
      "263/263 [==============================] - 1s 2ms/step loss: 0.9\n",
      "Epoch 7 - F1 Score: 0.6836\n",
      "Saved best model\n",
      "[0.5790137671930181, 0.6297322253014483, 0.6523256221016372, 0.6612526562136407, 0.6728446079723402, 0.6809108495972687, 0.6835900239143852]\n",
      "2364/2364 [==============================] - 9s 4ms/step - loss: 0.9817 - accuracy: 0.7133 - val_loss: 1.0604 - val_accuracy: 0.6953\n",
      "Epoch 8/40\n",
      "263/263 [==============================] - 0s 2ms/step loss: 0.9598 - \n",
      "Epoch 8 - F1 Score: 0.6870\n",
      "Saved best model\n",
      "[0.5790137671930181, 0.6297322253014483, 0.6523256221016372, 0.6612526562136407, 0.6728446079723402, 0.6809108495972687, 0.6835900239143852, 0.6869846165278202]\n",
      "2364/2364 [==============================] - 9s 4ms/step - loss: 0.9598 - accuracy: 0.7176 - val_loss: 1.0627 - val_accuracy: 0.6989\n",
      "Epoch 9/40\n",
      "263/263 [==============================] - 0s 1ms/step loss: 0.9338 - ac\n",
      "Epoch 9 - F1 Score: 0.6887\n",
      "Saved best model\n",
      "[0.5790137671930181, 0.6297322253014483, 0.6523256221016372, 0.6612526562136407, 0.6728446079723402, 0.6809108495972687, 0.6835900239143852, 0.6869846165278202, 0.6886950004626182]\n",
      "2364/2364 [==============================] - 6s 3ms/step - loss: 0.9338 - accuracy: 0.7244 - val_loss: 1.0382 - val_accuracy: 0.6994\n",
      "Epoch 10/40\n",
      "263/263 [==============================] - 1s 2ms/step loss: 0.9096 \n",
      "Epoch 10 - F1 Score: 0.6951\n",
      "Saved best model\n",
      "[0.5790137671930181, 0.6297322253014483, 0.6523256221016372, 0.6612526562136407, 0.6728446079723402, 0.6809108495972687, 0.6835900239143852, 0.6869846165278202, 0.6886950004626182, 0.6951203278086453]\n",
      "2364/2364 [==============================] - 7s 3ms/step - loss: 0.9097 - accuracy: 0.7313 - val_loss: 1.0500 - val_accuracy: 0.6994\n",
      "Epoch 11/40\n",
      "263/263 [==============================] - 1s 2ms/step loss: 0.889\n",
      "2364/2364 [==============================] - 10s 4ms/step - loss: 0.8899 - accuracy: 0.7372 - val_loss: 1.0394 - val_accuracy: 0.6998\n",
      "Epoch 12/40\n",
      "263/263 [==============================] - 1s 2ms/step loss: 0.871\n",
      "Epoch 12 - F1 Score: 0.7008\n",
      "Saved best model\n",
      "[0.5790137671930181, 0.6297322253014483, 0.6523256221016372, 0.6612526562136407, 0.6728446079723402, 0.6809108495972687, 0.6835900239143852, 0.6869846165278202, 0.6886950004626182, 0.6951203278086453, 0.6929557732670677, 0.7007894823384794]\n",
      "2364/2364 [==============================] - 9s 4ms/step - loss: 0.8711 - accuracy: 0.7413 - val_loss: 1.0276 - val_accuracy: 0.7052\n",
      "Epoch 13/40\n",
      "263/263 [==============================] - 0s 2ms/step loss: 0.8517 \n",
      "2364/2364 [==============================] - 9s 4ms/step - loss: 0.8520 - accuracy: 0.7458 - val_loss: 1.0325 - val_accuracy: 0.7058\n",
      "Epoch 14/40\n",
      "263/263 [==============================] - 1s 4ms/step\n",
      "2364/2364 [==============================] - 15s 6ms/step - loss: 0.8340 - accuracy: 0.7514 - val_loss: 1.0282 - val_accuracy: 0.7050\n",
      "Epoch 15/40\n",
      "263/263 [==============================] - 1s 3ms/step l\n",
      "Epoch 15 - F1 Score: 0.7039\n",
      "Saved best model\n",
      "[0.5790137671930181, 0.6297322253014483, 0.6523256221016372, 0.6612526562136407, 0.6728446079723402, 0.6809108495972687, 0.6835900239143852, 0.6869846165278202, 0.6886950004626182, 0.6951203278086453, 0.6929557732670677, 0.7007894823384794, 0.6977060642771632, 0.6972454228050409, 0.7039353004854407]\n",
      "2364/2364 [==============================] - 8s 3ms/step - loss: 0.8170 - accuracy: 0.7550 - val_loss: 1.0246 - val_accuracy: 0.7093\n",
      "Epoch 16/40\n",
      "263/263 [==============================] - 1s 3ms/step lo\n",
      "2364/2364 [==============================] - 11s 5ms/step - loss: 0.8025 - accuracy: 0.7590 - val_loss: 1.0629 - val_accuracy: 0.6961\n",
      "Epoch 17/40\n",
      "263/263 [==============================] - 1s 3ms/step los\n",
      "2364/2364 [==============================] - 11s 5ms/step - loss: 0.7872 - accuracy: 0.7635 - val_loss: 1.0238 - val_accuracy: 0.7088\n",
      "Epoch 18/40\n",
      "263/263 [==============================] - 1s 2ms/step loss: 0.7720 \n",
      "Epoch 18 - F1 Score: 0.7048\n",
      "Saved best model\n",
      "[0.5790137671930181, 0.6297322253014483, 0.6523256221016372, 0.6612526562136407, 0.6728446079723402, 0.6809108495972687, 0.6835900239143852, 0.6869846165278202, 0.6886950004626182, 0.6951203278086453, 0.6929557732670677, 0.7007894823384794, 0.6977060642771632, 0.6972454228050409, 0.7039353004854407, 0.6883564233062218, 0.7025918840840553, 0.7047858784740235]\n",
      "2364/2364 [==============================] - 10s 4ms/step - loss: 0.7721 - accuracy: 0.7676 - val_loss: 1.0296 - val_accuracy: 0.7097\n",
      "Epoch 19/40\n",
      "263/263 [==============================] - 1s 2ms/step loss: 0\n",
      "Epoch 19 - F1 Score: 0.7062\n",
      "Saved best model\n",
      "[0.5790137671930181, 0.6297322253014483, 0.6523256221016372, 0.6612526562136407, 0.6728446079723402, 0.6809108495972687, 0.6835900239143852, 0.6869846165278202, 0.6886950004626182, 0.6951203278086453, 0.6929557732670677, 0.7007894823384794, 0.6977060642771632, 0.6972454228050409, 0.7039353004854407, 0.6883564233062218, 0.7025918840840553, 0.7047858784740235, 0.7061614212085356]\n",
      "2364/2364 [==============================] - 11s 5ms/step - loss: 0.7573 - accuracy: 0.7713 - val_loss: 1.0279 - val_accuracy: 0.7120\n",
      "Epoch 20/40\n",
      "263/263 [==============================] - 1s 3ms/step los\n",
      "2364/2364 [==============================] - 11s 5ms/step - loss: 0.7441 - accuracy: 0.7753 - val_loss: 1.0421 - val_accuracy: 0.7099\n",
      "Epoch 21/40\n",
      "263/263 [==============================] - 1s 2ms/step loss: 0\n",
      "Epoch 21 - F1 Score: 0.7081\n",
      "Saved best model\n",
      "[0.5790137671930181, 0.6297322253014483, 0.6523256221016372, 0.6612526562136407, 0.6728446079723402, 0.6809108495972687, 0.6835900239143852, 0.6869846165278202, 0.6886950004626182, 0.6951203278086453, 0.6929557732670677, 0.7007894823384794, 0.6977060642771632, 0.6972454228050409, 0.7039353004854407, 0.6883564233062218, 0.7025918840840553, 0.7047858784740235, 0.7061614212085356, 0.7059377722196797, 0.7080608360969308]\n",
      "2364/2364 [==============================] - 12s 5ms/step - loss: 0.7312 - accuracy: 0.7784 - val_loss: 1.0540 - val_accuracy: 0.7124\n",
      "Epoch 22/40\n",
      "263/263 [==============================] - 1s 2ms/step loss: 0\n",
      "2364/2364 [==============================] - 13s 6ms/step - loss: 0.7185 - accuracy: 0.7826 - val_loss: 1.0540 - val_accuracy: 0.7083\n",
      "Epoch 23/40\n",
      "263/263 [==============================] - 1s 3ms/step l\n",
      "2364/2364 [==============================] - 10s 4ms/step - loss: 0.7061 - accuracy: 0.7849 - val_loss: 1.0600 - val_accuracy: 0.7057\n",
      "Epoch 24/40\n",
      "263/263 [==============================] - 1s 2ms/step loss: 0\n",
      "2364/2364 [==============================] - 12s 5ms/step - loss: 0.6942 - accuracy: 0.7875 - val_loss: 1.0534 - val_accuracy: 0.7094\n",
      "Epoch 25/40\n",
      "263/263 [==============================] - 1s 3ms/step\n",
      "2364/2364 [==============================] - 12s 5ms/step - loss: 0.6813 - accuracy: 0.7914 - val_loss: 1.0687 - val_accuracy: 0.7025\n",
      "Epoch 26/40\n",
      "263/263 [==============================] - 1s 2ms/step loss: 0.6\n",
      "2364/2364 [==============================] - 10s 4ms/step - loss: 0.6709 - accuracy: 0.7935 - val_loss: 1.0707 - val_accuracy: 0.7099\n",
      "Epoch 27/40\n",
      "263/263 [==============================] - 1s 3ms/step\n",
      "2364/2364 [==============================] - 13s 6ms/step - loss: 0.6587 - accuracy: 0.7974 - val_loss: 1.0993 - val_accuracy: 0.7034\n",
      "Epoch 28/40\n",
      "263/263 [==============================] - 0s 2ms/step loss: 0.6488 \n",
      "2364/2364 [==============================] - 10s 4ms/step - loss: 0.6488 - accuracy: 0.8015 - val_loss: 1.1006 - val_accuracy: 0.7049\n",
      "Epoch 29/40\n",
      "263/263 [==============================] - 0s 2ms/step loss: 0.6372 - \n",
      "2364/2364 [==============================] - 8s 3ms/step - loss: 0.6377 - accuracy: 0.8033 - val_loss: 1.1065 - val_accuracy: 0.7056\n",
      "Epoch 30/40\n",
      "263/263 [==============================] - 0s 2ms/step loss: 0.6261 \n",
      "2364/2364 [==============================] - 9s 4ms/step - loss: 0.6263 - accuracy: 0.8076 - val_loss: 1.1111 - val_accuracy: 0.7056\n",
      "Epoch 31/40\n",
      "263/263 [==============================] - 1s 2ms/step loss: 0.615\n",
      "2364/2364 [==============================] - 8s 3ms/step - loss: 0.6158 - accuracy: 0.8094 - val_loss: 1.1135 - val_accuracy: 0.7033\n",
      "Epoch 32/40\n",
      "263/263 [==============================] - 0s 2ms/step loss: 0.6060 \n",
      "2364/2364 [==============================] - 7s 3ms/step - loss: 0.6060 - accuracy: 0.8123 - val_loss: 1.1415 - val_accuracy: 0.7047\n",
      "Epoch 33/40\n",
      "263/263 [==============================] - 0s 1ms/step loss: 0.5971 - \n",
      "2364/2364 [==============================] - 7s 3ms/step - loss: 0.5970 - accuracy: 0.8149 - val_loss: 1.1326 - val_accuracy: 0.6978\n",
      "Epoch 34/40\n",
      "263/263 [==============================] - 1s 2ms/step loss: 0.5\n",
      "2364/2364 [==============================] - 8s 4ms/step - loss: 0.5858 - accuracy: 0.8173 - val_loss: 1.1487 - val_accuracy: 0.6987\n",
      "Epoch 35/40\n",
      "263/263 [==============================] - 1s 2ms/step loss: 0.575\n",
      "2364/2364 [==============================] - 8s 4ms/step - loss: 0.5767 - accuracy: 0.8197 - val_loss: 1.1703 - val_accuracy: 0.6969\n",
      "Epoch 36/40\n",
      "263/263 [==============================] - 1s 2ms/step loss: 0.5\n",
      "2364/2364 [==============================] - 8s 3ms/step - loss: 0.5675 - accuracy: 0.8237 - val_loss: 1.1803 - val_accuracy: 0.7011\n",
      "Epoch 37/40\n",
      "263/263 [==============================] - 1s 2ms/step loss: 0.559\n",
      "2364/2364 [==============================] - 8s 3ms/step - loss: 0.5602 - accuracy: 0.8238 - val_loss: 1.2096 - val_accuracy: 0.6952\n",
      "Epoch 38/40\n",
      "263/263 [==============================] - 0s 2ms/step loss: 0.5482 \n",
      "2364/2364 [==============================] - 8s 4ms/step - loss: 0.5486 - accuracy: 0.8286 - val_loss: 1.2051 - val_accuracy: 0.6994\n",
      "Epoch 39/40\n",
      "263/263 [==============================] - 0s 2ms/step loss: 0.5403 -\n",
      "2364/2364 [==============================] - 11s 4ms/step - loss: 0.5403 - accuracy: 0.8309 - val_loss: 1.2099 - val_accuracy: 0.6949\n",
      "Epoch 40/40\n",
      "263/263 [==============================] - 1s 2ms/step loss: 0.\n",
      "2364/2364 [==============================] - 10s 4ms/step - loss: 0.5312 - accuracy: 0.8345 - val_loss: 1.2263 - val_accuracy: 0.6945\n",
      "485/485 [==============================] - 1s 1ms/step\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         119     0.8972    0.3492    0.5028      1174\n",
      "         120     0.3952    0.8079    0.5307       203\n",
      "         125     0.7524    0.8077    0.7791       572\n",
      "         134     0.4615    0.9474    0.6207        19\n",
      "         189     0.7168    0.6807    0.6983       119\n",
      "         190     0.6267    0.8756    0.7305       209\n",
      "          20     0.4160    0.2333    0.2990       870\n",
      "         200     0.6492    0.4977    0.5634       647\n",
      "         203     0.4667    0.7778    0.5833        36\n",
      "          22     0.8810    0.8841    0.8826       561\n",
      "         254     0.0741    0.0526    0.0615        38\n",
      "         255     0.4576    0.3913    0.4219        69\n",
      "         264     0.5838    0.5849    0.5843       542\n",
      "         269     0.3255    0.5750    0.4157       120\n",
      "         276     0.2885    0.4054    0.3371        74\n",
      "         284     0.2579    0.3178    0.2847       129\n",
      "         287     0.4929    0.5724    0.5297       304\n",
      "         295     0.6237    0.6905    0.6554        84\n",
      "         306     0.3913    0.3830    0.3871        94\n",
      "         310     0.7626    0.7626    0.7626       257\n",
      "         312     0.4286    0.2353    0.3038        51\n",
      "         319     0.4110    0.5455    0.4688        55\n",
      "         326     0.3158    0.3529    0.3333        34\n",
      "         327     0.3333    0.2895    0.3099        38\n",
      "         345     0.3030    0.3846    0.3390        26\n",
      "         347     0.6071    0.6296    0.6182        27\n",
      "         352     0.3432    0.9672    0.5067       549\n",
      "         362     0.5967    0.7770    0.6750       139\n",
      "         399     0.3877    0.6543    0.4869       269\n",
      "         400     0.3478    0.4604    0.3963       139\n",
      "         401     0.6111    0.5893    0.6000        56\n",
      "         415     0.7407    0.8696    0.8000        46\n",
      "         416     0.6939    0.8571    0.7669       357\n",
      "         426     0.8333    0.5682    0.6757        44\n",
      "         427     0.6207    0.7660    0.6857        47\n",
      "         434     0.5987    0.8483    0.7020       211\n",
      "         476     0.6923    0.8462    0.7615       234\n",
      "         502     0.6597    0.8559    0.7451       111\n",
      "         522     0.5046    0.5914    0.5446        93\n",
      "         532     0.5385    0.7000    0.6087        50\n",
      "          59     0.7179    0.7500    0.7336       112\n",
      "         601     0.7065    0.7831    0.7429        83\n",
      "         611     0.6718    0.8889    0.7652        99\n",
      "         617     0.5500    0.7674    0.6408        43\n",
      "         639     0.2258    0.4516    0.3011        31\n",
      "         668     0.1529    0.2600    0.1926        50\n",
      "         732     0.3675    0.4216    0.3927       102\n",
      "          74     0.1743    0.4634    0.2533        82\n",
      "         755     0.2048    0.4595    0.2833        37\n",
      "          77     0.4056    0.4803    0.4398       152\n",
      "         770     0.3656    0.4595    0.4072        74\n",
      "         772     0.5714    0.5128    0.5405        39\n",
      "          78     0.5413    0.7806    0.6393       319\n",
      "         787     0.6350    0.6958    0.6640       960\n",
      "          79     0.9958    0.4663    0.6352      2552\n",
      "         798     0.7899    0.7287    0.7581       129\n",
      "         835     0.6400    0.7111    0.6737        45\n",
      "         843     0.4921    0.7750    0.6019        40\n",
      "         862     0.6404    0.6489    0.6446       225\n",
      "         863     0.3875    0.2520    0.3054       123\n",
      "          89     0.9874    0.7243    0.8356      1081\n",
      "         908     0.4130    0.6786    0.5135        28\n",
      "         918     0.5321    0.8384    0.6510        99\n",
      "          94     0.4498    0.6144    0.5193       306\n",
      "\n",
      "    accuracy                         0.6014     15508\n",
      "   macro avg     0.5267    0.6093    0.5483     15508\n",
      "weighted avg     0.6864    0.6014    0.6047     15508\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['label_encoder_train.joblib']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import joblib\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class F1ScoreCallback(Callback):\n",
    "    def __init__(self, X_val, y_val):\n",
    "        super(F1ScoreCallback, self).__init__()\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.best_f1 = 0.0\n",
    "        self.best_model = None\n",
    "        self.f1_scores = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_val_pred = np.argmax(self.model.predict(self.X_val), axis=1)\n",
    "        f1 = f1_score(self.y_val, y_val_pred, average='weighted')\n",
    "        self.f1_scores.append(f1)\n",
    "        \n",
    "\n",
    "        if f1 > self.best_f1:\n",
    "            self.best_f1 = f1\n",
    "            self.best_model = self.model\n",
    "            print(f\"Epoch {epoch + 1} - F1 Score: {f1:.4f}\")\n",
    "            print(\"Saved best model\")\n",
    "            print(self.f1_scores)\n",
    "\n",
    "with open('train_without_test2.pickle', 'rb') as f1:\n",
    "    balanced = pickle.load(f1)\n",
    "\n",
    "with open('test_ada.pickle', 'rb') as f2:\n",
    "    unbalanced = pickle.load(f2)\n",
    "\n",
    "train = np.array([item['cve_description_ada_embedding'] for item in balanced if item['cwe'] != 'None'])\n",
    "test = np.array([item['cwe'] for item in balanced if item['cwe'] != 'None'])\n",
    "np.random.seed(42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(train,test,test_size=0.1,random_state=42)\n",
    "\n",
    "X_test = np.array([item['cve_description_ada_embedding'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "y_test = np.array([item['cwe'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "\n",
    "label_encoder_train = LabelEncoder()\n",
    "y_train_encoded = label_encoder_train.fit_transform(y_train)\n",
    "label_encoder_test = LabelEncoder()\n",
    "y_test_encoded = label_encoder_test.fit_transform(y_test)\n",
    "\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = len(np.unique(y_train))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=input_dim, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(output_dim, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "f1_callback = F1ScoreCallback(X_val, label_encoder_train.transform(y_val))\n",
    "\n",
    "history = model.fit(X_train, y_train_encoded, epochs=40, batch_size=32, validation_data=(X_val, label_encoder_train.transform(y_val)), verbose=1, callbacks=[f1_callback])\n",
    "\n",
    "best_model = f1_callback.best_model\n",
    "\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(best_model, 'CWE_classes.joblib')\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_probs = best_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "y_pred_original = label_encoder_train.inverse_transform(y_pred)\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_original, digits=4))\n",
    "\n",
    "joblib.dump(label_encoder_train, 'label_encoder_train.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run inference with the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "485/485 [==============================] - 1s 2ms/step\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         119     0.8972    0.3492    0.5028      1174\n",
      "         120     0.3952    0.8079    0.5307       203\n",
      "         125     0.7524    0.8077    0.7791       572\n",
      "         134     0.4615    0.9474    0.6207        19\n",
      "         189     0.7168    0.6807    0.6983       119\n",
      "         190     0.6267    0.8756    0.7305       209\n",
      "          20     0.4160    0.2333    0.2990       870\n",
      "         200     0.6492    0.4977    0.5634       647\n",
      "         203     0.4667    0.7778    0.5833        36\n",
      "          22     0.8810    0.8841    0.8826       561\n",
      "         254     0.0741    0.0526    0.0615        38\n",
      "         255     0.4576    0.3913    0.4219        69\n",
      "         264     0.5838    0.5849    0.5843       542\n",
      "         269     0.3255    0.5750    0.4157       120\n",
      "         276     0.2885    0.4054    0.3371        74\n",
      "         284     0.2579    0.3178    0.2847       129\n",
      "         287     0.4929    0.5724    0.5297       304\n",
      "         295     0.6237    0.6905    0.6554        84\n",
      "         306     0.3913    0.3830    0.3871        94\n",
      "         310     0.7626    0.7626    0.7626       257\n",
      "         312     0.4286    0.2353    0.3038        51\n",
      "         319     0.4110    0.5455    0.4688        55\n",
      "         326     0.3158    0.3529    0.3333        34\n",
      "         327     0.3333    0.2895    0.3099        38\n",
      "         345     0.3030    0.3846    0.3390        26\n",
      "         347     0.6071    0.6296    0.6182        27\n",
      "         352     0.3432    0.9672    0.5067       549\n",
      "         362     0.5967    0.7770    0.6750       139\n",
      "         399     0.3877    0.6543    0.4869       269\n",
      "         400     0.3478    0.4604    0.3963       139\n",
      "         401     0.6111    0.5893    0.6000        56\n",
      "         415     0.7407    0.8696    0.8000        46\n",
      "         416     0.6939    0.8571    0.7669       357\n",
      "         426     0.8333    0.5682    0.6757        44\n",
      "         427     0.6207    0.7660    0.6857        47\n",
      "         434     0.5987    0.8483    0.7020       211\n",
      "         476     0.6923    0.8462    0.7615       234\n",
      "         502     0.6597    0.8559    0.7451       111\n",
      "         522     0.5046    0.5914    0.5446        93\n",
      "         532     0.5385    0.7000    0.6087        50\n",
      "          59     0.7179    0.7500    0.7336       112\n",
      "         601     0.7065    0.7831    0.7429        83\n",
      "         611     0.6718    0.8889    0.7652        99\n",
      "         617     0.5500    0.7674    0.6408        43\n",
      "         639     0.2258    0.4516    0.3011        31\n",
      "         668     0.1529    0.2600    0.1926        50\n",
      "         732     0.3675    0.4216    0.3927       102\n",
      "          74     0.1743    0.4634    0.2533        82\n",
      "         755     0.2048    0.4595    0.2833        37\n",
      "          77     0.4056    0.4803    0.4398       152\n",
      "         770     0.3656    0.4595    0.4072        74\n",
      "         772     0.5714    0.5128    0.5405        39\n",
      "          78     0.5413    0.7806    0.6393       319\n",
      "         787     0.6350    0.6958    0.6640       960\n",
      "          79     0.9958    0.4663    0.6352      2552\n",
      "         798     0.7899    0.7287    0.7581       129\n",
      "         835     0.6400    0.7111    0.6737        45\n",
      "         843     0.4921    0.7750    0.6019        40\n",
      "         862     0.6404    0.6489    0.6446       225\n",
      "         863     0.3875    0.2520    0.3054       123\n",
      "          89     0.9874    0.7243    0.8356      1081\n",
      "         908     0.4130    0.6786    0.5135        28\n",
      "         918     0.5321    0.8384    0.6510        99\n",
      "          94     0.4498    0.6144    0.5193       306\n",
      "\n",
      "    accuracy                         0.6014     15508\n",
      "   macro avg     0.5267    0.6093    0.5483     15508\n",
      "weighted avg     0.6864    0.6014    0.6047     15508\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Save the best model\n",
    "with open('test_ada.pickle', 'rb') as f2:\n",
    "    unbalanced = pickle.load(f2)\n",
    "\n",
    "X_test = np.array([item['cve_description_ada_embedding'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "y_test = np.array([item['cwe'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "\n",
    "best_model=joblib.load('CWE_classes.joblib')\n",
    "label_encoder_train=joblib.load('label_encoder_train.joblib')\n",
    "# Make predictions on the test set\n",
    "y_pred_probs = best_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "y_pred_original = label_encoder_train.inverse_transform(y_pred)\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_original, digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
