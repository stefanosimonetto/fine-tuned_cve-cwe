{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the best F1-score on the first 50 epochs and extract the best model\n",
    "\n",
    "Embedded: CVE description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "280/280 [==============================] - 0s 1ms/step loss: 1.8976 - acc\n",
      "Epoch 1 - F1 Score: 0.5813\n",
      "Saved best model\n",
      "[0.5813152292734153]\n",
      "2516/2516 [==============================] - 8s 3ms/step - loss: 1.8966 - accuracy: 0.4949 - val_loss: 1.3880 - val_accuracy: 0.6054\n",
      "Epoch 2/60\n",
      "280/280 [==============================] - 0s 2ms/step loss: 1.2791 \n",
      "Epoch 2 - F1 Score: 0.6269\n",
      "Saved best model\n",
      "[0.5813152292734153, 0.6268914640708877]\n",
      "2516/2516 [==============================] - 9s 3ms/step - loss: 1.2789 - accuracy: 0.6344 - val_loss: 1.2485 - val_accuracy: 0.6477\n",
      "Epoch 3/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: 1.1\n",
      "Epoch 3 - F1 Score: 0.6509\n",
      "Saved best model\n",
      "[0.5813152292734153, 0.6268914640708877, 0.6508506141934202]\n",
      "2516/2516 [==============================] - 8s 3ms/step - loss: 1.1634 - accuracy: 0.6665 - val_loss: 1.1878 - val_accuracy: 0.6647\n",
      "Epoch 4/60\n",
      "280/280 [==============================] - 0s 1ms/step loss: 1.0969 - ac\n",
      "Epoch 4 - F1 Score: 0.6704\n",
      "Saved best model\n",
      "[0.5813152292734153, 0.6268914640708877, 0.6508506141934202, 0.6703696904000157]\n",
      "2516/2516 [==============================] - 8s 3ms/step - loss: 1.0967 - accuracy: 0.6823 - val_loss: 1.1573 - val_accuracy: 0.6784\n",
      "Epoch 5/60\n",
      "280/280 [==============================] - 0s 2ms/step loss: 1.0484 \n",
      "Epoch 5 - F1 Score: 0.6771\n",
      "Saved best model\n",
      "[0.5813152292734153, 0.6268914640708877, 0.6508506141934202, 0.6703696904000157, 0.677102999599956]\n",
      "2516/2516 [==============================] - 8s 3ms/step - loss: 1.0482 - accuracy: 0.6973 - val_loss: 1.1043 - val_accuracy: 0.6868\n",
      "Epoch 6/60\n",
      "280/280 [==============================] - 0s 1ms/step loss: 1.0071 - ac\n",
      "2516/2516 [==============================] - 8s 3ms/step - loss: 1.0074 - accuracy: 0.7063 - val_loss: 1.1053 - val_accuracy: 0.6846\n",
      "Epoch 7/60\n",
      "280/280 [==============================] - 0s 1ms/step loss: 0.9751 - ac\n",
      "Epoch 7 - F1 Score: 0.6771\n",
      "Saved best model\n",
      "[0.5813152292734153, 0.6268914640708877, 0.6508506141934202, 0.6703696904000157, 0.677102999599956, 0.6744708252141806, 0.6771262887391366]\n",
      "2516/2516 [==============================] - 13s 5ms/step - loss: 0.9754 - accuracy: 0.7144 - val_loss: 1.0754 - val_accuracy: 0.6900\n",
      "Epoch 8/60\n",
      "280/280 [==============================] - 0s 1ms/step loss: 0.9462 - \n",
      "Epoch 8 - F1 Score: 0.6854\n",
      "Saved best model\n",
      "[0.5813152292734153, 0.6268914640708877, 0.6508506141934202, 0.6703696904000157, 0.677102999599956, 0.6744708252141806, 0.6771262887391366, 0.6854377507894507]\n",
      "2516/2516 [==============================] - 8s 3ms/step - loss: 0.9462 - accuracy: 0.7228 - val_loss: 1.0732 - val_accuracy: 0.6955\n",
      "Epoch 9/60\n",
      "280/280 [==============================] - 0s 1ms/step loss: 0.9214 - accu\n",
      "Epoch 9 - F1 Score: 0.6880\n",
      "Saved best model\n",
      "[0.5813152292734153, 0.6268914640708877, 0.6508506141934202, 0.6703696904000157, 0.677102999599956, 0.6744708252141806, 0.6771262887391366, 0.6854377507894507, 0.6879621406292005]\n",
      "2516/2516 [==============================] - 7s 3ms/step - loss: 0.9210 - accuracy: 0.7287 - val_loss: 1.0816 - val_accuracy: 0.6932\n",
      "Epoch 10/60\n",
      "280/280 [==============================] - 0s 2ms/step loss: 0.8972 \n",
      "Epoch 10 - F1 Score: 0.6967\n",
      "Saved best model\n",
      "[0.5813152292734153, 0.6268914640708877, 0.6508506141934202, 0.6703696904000157, 0.677102999599956, 0.6744708252141806, 0.6771262887391366, 0.6854377507894507, 0.6879621406292005, 0.6966533556518306]\n",
      "2516/2516 [==============================] - 8s 3ms/step - loss: 0.8972 - accuracy: 0.7341 - val_loss: 1.0483 - val_accuracy: 0.7018\n",
      "Epoch 11/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: 0.8\n",
      "2516/2516 [==============================] - 8s 3ms/step - loss: 0.8766 - accuracy: 0.7398 - val_loss: 1.0562 - val_accuracy: 0.6955\n",
      "Epoch 12/60\n",
      "280/280 [==============================] - 0s 1ms/step loss: 0.8562 - ac\n",
      "Epoch 12 - F1 Score: 0.6980\n",
      "Saved best model\n",
      "[0.5813152292734153, 0.6268914640708877, 0.6508506141934202, 0.6703696904000157, 0.677102999599956, 0.6744708252141806, 0.6771262887391366, 0.6854377507894507, 0.6879621406292005, 0.6966533556518306, 0.6915202598651365, 0.6980079822879145]\n",
      "2516/2516 [==============================] - 8s 3ms/step - loss: 0.8562 - accuracy: 0.7443 - val_loss: 1.0551 - val_accuracy: 0.7026\n",
      "Epoch 13/60\n",
      "280/280 [==============================] - 0s 1ms/step loss: 0.8366 - accu\n",
      "2516/2516 [==============================] - 8s 3ms/step - loss: 0.8369 - accuracy: 0.7510 - val_loss: 1.0469 - val_accuracy: 0.6999\n",
      "Epoch 14/60\n",
      "280/280 [==============================] - 0s 2ms/step loss: 0.8190 \n",
      "2516/2516 [==============================] - 7s 3ms/step - loss: 0.8189 - accuracy: 0.7544 - val_loss: 1.0505 - val_accuracy: 0.7004\n",
      "Epoch 15/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: 0.803\n",
      "2516/2516 [==============================] - 8s 3ms/step - loss: 0.8034 - accuracy: 0.7587 - val_loss: 1.0442 - val_accuracy: 0.7042\n",
      "Epoch 16/60\n",
      "280/280 [==============================] - 0s 2ms/step loss: 0.7878 \n",
      "2516/2516 [==============================] - 8s 3ms/step - loss: 0.7878 - accuracy: 0.7632 - val_loss: 1.0439 - val_accuracy: 0.7056\n",
      "Epoch 17/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: 0.7\n",
      "Epoch 17 - F1 Score: 0.6992\n",
      "Saved best model\n",
      "[0.5813152292734153, 0.6268914640708877, 0.6508506141934202, 0.6703696904000157, 0.677102999599956, 0.6744708252141806, 0.6771262887391366, 0.6854377507894507, 0.6879621406292005, 0.6966533556518306, 0.6915202598651365, 0.6980079822879145, 0.6929778532042317, 0.69433956850901, 0.6961872863421907, 0.6977832913014547, 0.699223127868125]\n",
      "2516/2516 [==============================] - 8s 3ms/step - loss: 0.7695 - accuracy: 0.7683 - val_loss: 1.0479 - val_accuracy: 0.7076\n",
      "Epoch 18/60\n",
      "280/280 [==============================] - 0s 956us/steposs: 0.7564 - accu\n",
      "2516/2516 [==============================] - 8s 3ms/step - loss: 0.7564 - accuracy: 0.7712 - val_loss: 1.0596 - val_accuracy: 0.7020\n",
      "Epoch 19/60\n",
      "280/280 [==============================] - 0s 2ms/step loss: 0.741\n",
      "Epoch 19 - F1 Score: 0.7040\n",
      "Saved best model\n",
      "[0.5813152292734153, 0.6268914640708877, 0.6508506141934202, 0.6703696904000157, 0.677102999599956, 0.6744708252141806, 0.6771262887391366, 0.6854377507894507, 0.6879621406292005, 0.6966533556518306, 0.6915202598651365, 0.6980079822879145, 0.6929778532042317, 0.69433956850901, 0.6961872863421907, 0.6977832913014547, 0.699223127868125, 0.6981922791330499, 0.7040203625584874]\n",
      "2516/2516 [==============================] - 8s 3ms/step - loss: 0.7413 - accuracy: 0.7754 - val_loss: 1.0662 - val_accuracy: 0.7068\n",
      "Epoch 20/60\n",
      "280/280 [==============================] - 0s 1ms/step loss: 0.7274 - accu\n",
      "2516/2516 [==============================] - 8s 3ms/step - loss: 0.7274 - accuracy: 0.7790 - val_loss: 1.0715 - val_accuracy: 0.7027\n",
      "Epoch 21/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: 0.7\n",
      "2516/2516 [==============================] - 8s 3ms/step - loss: 0.7121 - accuracy: 0.7838 - val_loss: 1.0764 - val_accuracy: 0.7058\n",
      "Epoch 22/60\n",
      "280/280 [==============================] - 0s 1ms/step loss: 0.6994 \n",
      "2516/2516 [==============================] - 8s 3ms/step - loss: 0.6995 - accuracy: 0.7874 - val_loss: 1.0728 - val_accuracy: 0.7061\n",
      "Epoch 23/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: 0.685\n",
      "2516/2516 [==============================] - 6s 3ms/step - loss: 0.6855 - accuracy: 0.7904 - val_loss: 1.1010 - val_accuracy: 0.6976\n",
      "Epoch 24/60\n",
      "280/280 [==============================] - 0s 1ms/step loss: 0.6719 - accu\n",
      "2516/2516 [==============================] - 8s 3ms/step - loss: 0.6722 - accuracy: 0.7944 - val_loss: 1.1012 - val_accuracy: 0.7014\n",
      "Epoch 25/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: 0.6\n",
      "2516/2516 [==============================] - 8s 3ms/step - loss: 0.6602 - accuracy: 0.7962 - val_loss: 1.0896 - val_accuracy: 0.7063\n",
      "Epoch 26/60\n",
      "280/280 [==============================] - 0s 1ms/step loss: 0.6472 - accu\n",
      "2516/2516 [==============================] - 7s 3ms/step - loss: 0.6471 - accuracy: 0.7999 - val_loss: 1.1202 - val_accuracy: 0.7036\n",
      "Epoch 27/60\n",
      "280/280 [==============================] - 0s 1ms/step loss: 0.6370 - \n",
      "2516/2516 [==============================] - 8s 3ms/step - loss: 0.6370 - accuracy: 0.8043 - val_loss: 1.1446 - val_accuracy: 0.6962\n",
      "Epoch 28/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: 0\n",
      "2516/2516 [==============================] - 8s 3ms/step - loss: 0.6236 - accuracy: 0.8087 - val_loss: 1.1341 - val_accuracy: 0.7031\n",
      "Epoch 29/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: 0.6\n",
      "2516/2516 [==============================] - 8s 3ms/step - loss: 0.6112 - accuracy: 0.8102 - val_loss: 1.1305 - val_accuracy: 0.6990\n",
      "Epoch 30/60\n",
      "280/280 [==============================] - 0s 1ms/step loss: 0.6000 - \n",
      "2516/2516 [==============================] - 8s 3ms/step - loss: 0.6000 - accuracy: 0.8141 - val_loss: 1.1637 - val_accuracy: 0.6972\n",
      "Epoch 31/60\n",
      "280/280 [==============================] - 0s 1ms/step loss: 0.5897 - accu\n",
      "2516/2516 [==============================] - 7s 3ms/step - loss: 0.5906 - accuracy: 0.8158 - val_loss: 1.1468 - val_accuracy: 0.6993\n",
      "Epoch 32/60\n",
      "280/280 [==============================] - 0s 2ms/step loss: 0.5770 \n",
      "2516/2516 [==============================] - 7s 3ms/step - loss: 0.5769 - accuracy: 0.8205 - val_loss: 1.1849 - val_accuracy: 0.7009\n",
      "Epoch 33/60\n",
      "280/280 [==============================] - 0s 2ms/step loss: 0.5676 \n",
      "2516/2516 [==============================] - 8s 3ms/step - loss: 0.5677 - accuracy: 0.8224 - val_loss: 1.1829 - val_accuracy: 0.6956\n",
      "Epoch 34/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: 0.5\n",
      "2516/2516 [==============================] - 8s 3ms/step - loss: 0.5575 - accuracy: 0.8267 - val_loss: 1.2261 - val_accuracy: 0.6884\n",
      "Epoch 35/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: 0.547\n",
      "2516/2516 [==============================] - 8s 3ms/step - loss: 0.5470 - accuracy: 0.8290 - val_loss: 1.2361 - val_accuracy: 0.6924\n",
      "Epoch 36/60\n",
      "280/280 [==============================] - 0s 1ms/step loss: 0.5373 - ac\n",
      "2516/2516 [==============================] - 8s 3ms/step - loss: 0.5373 - accuracy: 0.8316 - val_loss: 1.2355 - val_accuracy: 0.6906\n",
      "Epoch 37/60\n",
      "280/280 [==============================] - 0s 1ms/step loss: 0.5261 - ac\n",
      "2516/2516 [==============================] - 8s 3ms/step - loss: 0.5262 - accuracy: 0.8341 - val_loss: 1.2456 - val_accuracy: 0.6967\n",
      "Epoch 38/60\n",
      "280/280 [==============================] - 1s 2ms/step loss: 0.5\n",
      "2516/2516 [==============================] - 8s 3ms/step - loss: 0.5188 - accuracy: 0.8365 - val_loss: 1.2535 - val_accuracy: 0.6919\n",
      "Epoch 39/60\n",
      "280/280 [==============================] - 1s 4ms/step\n",
      "2516/2516 [==============================] - 17s 7ms/step - loss: 0.5085 - accuracy: 0.8393 - val_loss: 1.3068 - val_accuracy: 0.6899\n",
      "Epoch 40/60\n",
      "280/280 [==============================] - 1s 3ms/step\n",
      "2516/2516 [==============================] - 17s 7ms/step - loss: 0.4988 - accuracy: 0.8411 - val_loss: 1.2827 - val_accuracy: 0.6907\n",
      "Epoch 41/60\n",
      "280/280 [==============================] - 1s 3ms/step lo\n",
      "2516/2516 [==============================] - 18s 7ms/step - loss: 0.4910 - accuracy: 0.8444 - val_loss: 1.3002 - val_accuracy: 0.6885\n",
      "Epoch 42/60\n",
      "280/280 [==============================] - 1s 4ms/step\n",
      "2516/2516 [==============================] - 18s 7ms/step - loss: 0.4800 - accuracy: 0.8471 - val_loss: 1.3186 - val_accuracy: 0.6889\n",
      "Epoch 43/60\n",
      "280/280 [==============================] - 1s 3ms/step lo\n",
      "2516/2516 [==============================] - 18s 7ms/step - loss: 0.4704 - accuracy: 0.8510 - val_loss: 1.3446 - val_accuracy: 0.6899\n",
      "Epoch 44/60\n",
      "280/280 [==============================] - 1s 3ms/step lo\n",
      "2516/2516 [==============================] - 17s 7ms/step - loss: 0.4618 - accuracy: 0.8522 - val_loss: 1.3754 - val_accuracy: 0.6872\n",
      "Epoch 45/60\n",
      "280/280 [==============================] - 1s 3ms/step \n",
      "2516/2516 [==============================] - 17s 7ms/step - loss: 0.4553 - accuracy: 0.8554 - val_loss: 1.3607 - val_accuracy: 0.6882\n",
      "Epoch 46/60\n",
      "280/280 [==============================] - 1s 3ms/step loss\n",
      "2516/2516 [==============================] - 17s 7ms/step - loss: 0.4468 - accuracy: 0.8570 - val_loss: 1.4015 - val_accuracy: 0.6795\n",
      "Epoch 47/60\n",
      "280/280 [==============================] - 1s 3ms/step loss\n",
      "2516/2516 [==============================] - 17s 7ms/step - loss: 0.4368 - accuracy: 0.8600 - val_loss: 1.4299 - val_accuracy: 0.6875\n",
      "Epoch 48/60\n",
      "280/280 [==============================] - 1s 3ms/step lo\n",
      "2516/2516 [==============================] - 18s 7ms/step - loss: 0.4284 - accuracy: 0.8629 - val_loss: 1.4272 - val_accuracy: 0.6875\n",
      "Epoch 49/60\n",
      "280/280 [==============================] - 1s 4ms/step\n",
      "2516/2516 [==============================] - 18s 7ms/step - loss: 0.4236 - accuracy: 0.8638 - val_loss: 1.4297 - val_accuracy: 0.6827\n",
      "Epoch 50/60\n",
      "280/280 [==============================] - 1s 3ms/step loss:\n",
      "2516/2516 [==============================] - 18s 7ms/step - loss: 0.4141 - accuracy: 0.8672 - val_loss: 1.4443 - val_accuracy: 0.6844\n",
      "Epoch 51/60\n",
      "280/280 [==============================] - 1s 3ms/step l\n",
      "2516/2516 [==============================] - 17s 7ms/step - loss: 0.4093 - accuracy: 0.8672 - val_loss: 1.4751 - val_accuracy: 0.6853\n",
      "Epoch 52/60\n",
      "280/280 [==============================] - 1s 3ms/step l\n",
      "2516/2516 [==============================] - 17s 7ms/step - loss: 0.4019 - accuracy: 0.8693 - val_loss: 1.5159 - val_accuracy: 0.6773\n",
      "Epoch 53/60\n",
      "280/280 [==============================] - 1s 3ms/step loss: \n",
      "2516/2516 [==============================] - 17s 7ms/step - loss: 0.3916 - accuracy: 0.8729 - val_loss: 1.5372 - val_accuracy: 0.6795\n",
      "Epoch 54/60\n",
      "280/280 [==============================] - 1s 3ms/step l\n",
      "2516/2516 [==============================] - 17s 7ms/step - loss: 0.3843 - accuracy: 0.8756 - val_loss: 1.5332 - val_accuracy: 0.6810\n",
      "Epoch 55/60\n",
      "280/280 [==============================] - 1s 3ms/step los\n",
      "2516/2516 [==============================] - 17s 7ms/step - loss: 0.3780 - accuracy: 0.8768 - val_loss: 1.5487 - val_accuracy: 0.6806\n",
      "Epoch 56/60\n",
      "280/280 [==============================] - 1s 3ms/step loss\n",
      "2516/2516 [==============================] - 17s 7ms/step - loss: 0.3719 - accuracy: 0.8787 - val_loss: 1.5998 - val_accuracy: 0.6761\n",
      "Epoch 57/60\n",
      "280/280 [==============================] - 1s 3ms/step los\n",
      "2516/2516 [==============================] - 17s 7ms/step - loss: 0.3646 - accuracy: 0.8797 - val_loss: 1.5951 - val_accuracy: 0.6753\n",
      "Epoch 58/60\n",
      "280/280 [==============================] - 1s 3ms/step lo\n",
      "2516/2516 [==============================] - 17s 7ms/step - loss: 0.3582 - accuracy: 0.8830 - val_loss: 1.6204 - val_accuracy: 0.6782\n",
      "Epoch 59/60\n",
      "280/280 [==============================] - 1s 3ms/step lo\n",
      "2516/2516 [==============================] - 17s 7ms/step - loss: 0.3526 - accuracy: 0.8849 - val_loss: 1.6521 - val_accuracy: 0.6783\n",
      "Epoch 60/60\n",
      "280/280 [==============================] - 1s 3ms/step loss: \n",
      "2516/2516 [==============================] - 16s 7ms/step - loss: 0.3453 - accuracy: 0.8875 - val_loss: 1.6722 - val_accuracy: 0.6757\n",
      "485/485 [==============================] - 1s 3ms/step\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         119     0.9471    0.4114    0.5736      1174\n",
      "         120     0.4372    0.9606    0.6009       203\n",
      "         125     0.6905    0.9126    0.7861       572\n",
      "         134     0.8182    0.9474    0.8780        19\n",
      "         189     0.7622    0.9160    0.8321       119\n",
      "         190     0.6667    0.9378    0.7793       209\n",
      "          20     0.5511    0.3471    0.4260       870\n",
      "         200     0.7294    0.6291    0.6755       647\n",
      "         203     0.7447    0.9722    0.8434        36\n",
      "          22     0.9150    0.9020    0.9084       561\n",
      "         254     0.4419    0.5000    0.4691        38\n",
      "         255     0.6081    0.6522    0.6294        69\n",
      "         264     0.8300    0.5314    0.6479       542\n",
      "         269     0.5078    0.8167    0.6262       120\n",
      "         276     0.6966    0.8378    0.7607        74\n",
      "         284     0.5099    0.5969    0.5500       129\n",
      "         287     0.7530    0.8125    0.7816       304\n",
      "         295     0.8384    0.9881    0.9071        84\n",
      "         306     0.7875    0.6702    0.7241        94\n",
      "         310     0.9271    0.8911    0.9087       257\n",
      "         312     0.6724    0.7647    0.7156        51\n",
      "         319     0.6757    0.9091    0.7752        55\n",
      "         326     0.6757    0.7353    0.7042        34\n",
      "         327     0.6905    0.7632    0.7250        38\n",
      "         345     0.4474    0.6538    0.5312        26\n",
      "         347     0.7419    0.8519    0.7931        27\n",
      "         352     0.5078    0.9454    0.6607       549\n",
      "         362     0.7452    0.8417    0.7905       139\n",
      "         399     0.5676    0.8587    0.6834       269\n",
      "         400     0.4836    0.8489    0.6162       139\n",
      "         401     0.6349    0.7143    0.6723        56\n",
      "         415     0.7895    0.9783    0.8738        46\n",
      "         416     0.8079    0.9188    0.8598       357\n",
      "         426     0.8039    0.9318    0.8632        44\n",
      "         427     0.7544    0.9149    0.8269        47\n",
      "         434     0.5795    0.9668    0.7247       211\n",
      "         476     0.7579    0.9231    0.8324       234\n",
      "         502     0.6923    0.9730    0.8090       111\n",
      "         522     0.7190    0.9355    0.8131        93\n",
      "         532     0.6957    0.9600    0.8067        50\n",
      "          59     0.8938    0.9018    0.8978       112\n",
      "         601     0.4689    1.0000    0.6385        83\n",
      "         611     0.5543    0.9798    0.7080        99\n",
      "         617     0.7321    0.9535    0.8283        43\n",
      "         639     0.6304    0.9355    0.7532        31\n",
      "         668     0.3902    0.6400    0.4848        50\n",
      "         732     0.5760    0.7059    0.6344       102\n",
      "          74     0.1409    0.8780    0.2428        82\n",
      "         755     0.5000    0.7568    0.6022        37\n",
      "          77     0.4564    0.8947    0.6044       152\n",
      "         770     0.3758    0.7568    0.5022        74\n",
      "         772     0.5132    1.0000    0.6783        39\n",
      "          78     0.7596    0.8715    0.8117       319\n",
      "         787     0.7626    0.5823    0.6604       960\n",
      "          79     0.9892    0.5372    0.6963      2552\n",
      "         798     0.9360    0.9070    0.9213       129\n",
      "         835     0.6716    1.0000    0.8036        45\n",
      "         843     0.6316    0.9000    0.7423        40\n",
      "         862     0.7519    0.8622    0.8033       225\n",
      "         863     0.5244    0.6992    0.5993       123\n",
      "          89     1.0000    0.5125    0.6777      1081\n",
      "         908     0.5854    0.8571    0.6957        28\n",
      "         918     0.4344    0.9697    0.6000        99\n",
      "          94     0.5396    0.7353    0.6224       306\n",
      "\n",
      "    accuracy                         0.6889     15508\n",
      "   macro avg     0.6629    0.8197    0.7124     15508\n",
      "weighted avg     0.7725    0.6889    0.6903     15508\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import joblib\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class F1ScoreCallback(Callback):\n",
    "    def __init__(self, X_val, y_val):\n",
    "        super(F1ScoreCallback, self).__init__()\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.best_f1 = 0.0\n",
    "        self.best_model = None\n",
    "        self.f1_scores = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_val_pred = np.argmax(self.model.predict(self.X_val), axis=1)\n",
    "        f1 = f1_score(self.y_val, y_val_pred, average='weighted')\n",
    "        self.f1_scores.append(f1)\n",
    "        \n",
    "\n",
    "        if f1 > self.best_f1:\n",
    "            self.best_f1 = f1\n",
    "            self.best_model = self.model\n",
    "            print(f\"Epoch {epoch + 1} - F1 Score: {f1:.4f}\")\n",
    "            print(\"Saved best model\")\n",
    "            print(self.f1_scores)\n",
    "\n",
    "with open('balanced_ada.pickle', 'rb') as f1:\n",
    "    balanced = pickle.load(f1)\n",
    "\n",
    "with open('test_ada.pickle', 'rb') as f2:\n",
    "    unbalanced = pickle.load(f2)\n",
    "\n",
    "train = np.array([item['cve_description_ada_embedding'] for item in balanced if item['cwe'] != 'None'])\n",
    "test = np.array([item['cwe'] for item in balanced if item['cwe'] != 'None'])\n",
    "np.random.seed(42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(train,test,test_size=0.1,random_state=42)\n",
    "\n",
    "X_test = np.array([item['cve_description_ada_embedding'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "y_test = np.array([item['cwe'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "\n",
    "label_encoder_train = LabelEncoder()\n",
    "y_train_encoded = label_encoder_train.fit_transform(y_train)\n",
    "label_encoder_test = LabelEncoder()\n",
    "y_test_encoded = label_encoder_test.fit_transform(y_test)\n",
    "\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = len(np.unique(y_train))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=input_dim, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(output_dim, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "f1_callback = F1ScoreCallback(X_val, label_encoder_train.transform(y_val))\n",
    "\n",
    "history = model.fit(X_train, y_train_encoded, epochs=60, batch_size=32, validation_data=(X_val, label_encoder_train.transform(y_val)), verbose=1, callbacks=[f1_callback])\n",
    "\n",
    "best_model = f1_callback.best_model\n",
    "\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(best_model, 'CWE_classes.joblib')\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_probs = best_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "y_pred_original = label_encoder_train.inverse_transform(y_pred)\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_original, digits=4))\n",
    "\n",
    "joblib.dump(label_encoder_train, 'label_encoder_train.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run inference with the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "485/485 [==============================] - 1s 1ms/step\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         119     0.9471    0.4114    0.5736      1174\n",
      "         120     0.4372    0.9606    0.6009       203\n",
      "         125     0.6905    0.9126    0.7861       572\n",
      "         134     0.8182    0.9474    0.8780        19\n",
      "         189     0.7622    0.9160    0.8321       119\n",
      "         190     0.6667    0.9378    0.7793       209\n",
      "          20     0.5511    0.3471    0.4260       870\n",
      "         200     0.7294    0.6291    0.6755       647\n",
      "         203     0.7447    0.9722    0.8434        36\n",
      "          22     0.9150    0.9020    0.9084       561\n",
      "         254     0.4419    0.5000    0.4691        38\n",
      "         255     0.6081    0.6522    0.6294        69\n",
      "         264     0.8300    0.5314    0.6479       542\n",
      "         269     0.5078    0.8167    0.6262       120\n",
      "         276     0.6966    0.8378    0.7607        74\n",
      "         284     0.5099    0.5969    0.5500       129\n",
      "         287     0.7530    0.8125    0.7816       304\n",
      "         295     0.8384    0.9881    0.9071        84\n",
      "         306     0.7875    0.6702    0.7241        94\n",
      "         310     0.9271    0.8911    0.9087       257\n",
      "         312     0.6724    0.7647    0.7156        51\n",
      "         319     0.6757    0.9091    0.7752        55\n",
      "         326     0.6757    0.7353    0.7042        34\n",
      "         327     0.6905    0.7632    0.7250        38\n",
      "         345     0.4474    0.6538    0.5312        26\n",
      "         347     0.7419    0.8519    0.7931        27\n",
      "         352     0.5078    0.9454    0.6607       549\n",
      "         362     0.7452    0.8417    0.7905       139\n",
      "         399     0.5676    0.8587    0.6834       269\n",
      "         400     0.4836    0.8489    0.6162       139\n",
      "         401     0.6349    0.7143    0.6723        56\n",
      "         415     0.7895    0.9783    0.8738        46\n",
      "         416     0.8079    0.9188    0.8598       357\n",
      "         426     0.8039    0.9318    0.8632        44\n",
      "         427     0.7544    0.9149    0.8269        47\n",
      "         434     0.5795    0.9668    0.7247       211\n",
      "         476     0.7579    0.9231    0.8324       234\n",
      "         502     0.6923    0.9730    0.8090       111\n",
      "         522     0.7190    0.9355    0.8131        93\n",
      "         532     0.6957    0.9600    0.8067        50\n",
      "          59     0.8938    0.9018    0.8978       112\n",
      "         601     0.4689    1.0000    0.6385        83\n",
      "         611     0.5543    0.9798    0.7080        99\n",
      "         617     0.7321    0.9535    0.8283        43\n",
      "         639     0.6304    0.9355    0.7532        31\n",
      "         668     0.3902    0.6400    0.4848        50\n",
      "         732     0.5760    0.7059    0.6344       102\n",
      "          74     0.1409    0.8780    0.2428        82\n",
      "         755     0.5000    0.7568    0.6022        37\n",
      "          77     0.4564    0.8947    0.6044       152\n",
      "         770     0.3758    0.7568    0.5022        74\n",
      "         772     0.5132    1.0000    0.6783        39\n",
      "          78     0.7596    0.8715    0.8117       319\n",
      "         787     0.7626    0.5823    0.6604       960\n",
      "          79     0.9892    0.5372    0.6963      2552\n",
      "         798     0.9360    0.9070    0.9213       129\n",
      "         835     0.6716    1.0000    0.8036        45\n",
      "         843     0.6316    0.9000    0.7423        40\n",
      "         862     0.7519    0.8622    0.8033       225\n",
      "         863     0.5244    0.6992    0.5993       123\n",
      "          89     1.0000    0.5125    0.6777      1081\n",
      "         908     0.5854    0.8571    0.6957        28\n",
      "         918     0.4344    0.9697    0.6000        99\n",
      "          94     0.5396    0.7353    0.6224       306\n",
      "\n",
      "    accuracy                         0.6889     15508\n",
      "   macro avg     0.6629    0.8197    0.7124     15508\n",
      "weighted avg     0.7725    0.6889    0.6903     15508\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Save the best model\n",
    "with open('test_ada.pickle', 'rb') as f2:\n",
    "    unbalanced = pickle.load(f2)\n",
    "\n",
    "X_test = np.array([item['cve_description_ada_embedding'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "y_test = np.array([item['cwe'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "\n",
    "best_model=joblib.load('CWE_classes.joblib')\n",
    "label_encoder_train=joblib.load('label_encoder_train.joblib')\n",
    "# Make predictions on the test set\n",
    "y_pred_probs = best_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "y_pred_original = label_encoder_train.inverse_transform(y_pred)\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_original, digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
