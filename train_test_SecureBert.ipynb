{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the best F1-score on the first 40 epochs and extract the best model\n",
    "\n",
    "Embedded: CVE description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train examples:  84025\n",
      "Number of classes :  64\n",
      "Test examples:  15508\n",
      "Number of classes :  64\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import pickle\n",
    "with open('train_definitivo2.pickle', 'rb') as f:\n",
    "    train = pickle.load(f)\n",
    "\n",
    "print(\"Train examples: \", len(train))\n",
    "cve_counter = Counter(entry['cwe']for entry in train)\n",
    "print(\"Number of classes : \", len(cve_counter))\n",
    "\n",
    "with open('test_SecureBert.pickle', 'rb') as f:\n",
    "    test = pickle.load(f)\n",
    "\n",
    "print(\"Test examples: \", len(test))\n",
    "cve_counter = Counter(entry['cwe']for entry in test)\n",
    "print(\"Number of classes : \", len(cve_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "263/263 [==============================] - 1s 2ms/step loss: 2.59\n",
      "Epoch 1 - F1 Score: 0.3783\n",
      "Saved best model\n",
      "[0.3783293227186078]\n",
      "2364/2364 [==============================] - 13s 5ms/step - loss: 2.5968 - accuracy: 0.3095 - val_loss: 2.0354 - val_accuracy: 0.4199\n",
      "Epoch 2/40\n",
      "263/263 [==============================] - 0s 2ms/step loss: 1.891\n",
      "Epoch 2 - F1 Score: 0.4319\n",
      "Saved best model\n",
      "[0.3783293227186078, 0.43187826365071724]\n",
      "2364/2364 [==============================] - 8s 3ms/step - loss: 1.8893 - accuracy: 0.4651 - val_loss: 1.7985 - val_accuracy: 0.4780\n",
      "Epoch 3/40\n",
      "263/263 [==============================] - 0s 2ms/step loss: 1.722\n",
      "Epoch 3 - F1 Score: 0.4842\n",
      "Saved best model\n",
      "[0.3783293227186078, 0.43187826365071724, 0.48421917084072025]\n",
      "2364/2364 [==============================] - 10s 4ms/step - loss: 1.7227 - accuracy: 0.5070 - val_loss: 1.6817 - val_accuracy: 0.5141\n",
      "Epoch 4/40\n",
      "263/263 [==============================] - 0s 2ms/step loss: 1.6277 \n",
      "Epoch 4 - F1 Score: 0.5057\n",
      "Saved best model\n",
      "[0.3783293227186078, 0.43187826365071724, 0.48421917084072025, 0.5057333508901423]\n",
      "2364/2364 [==============================] - 9s 4ms/step - loss: 1.6277 - accuracy: 0.5316 - val_loss: 1.6141 - val_accuracy: 0.5341\n",
      "Epoch 5/40\n",
      "263/263 [==============================] - 0s 2ms/step loss: 1.562\n",
      "Epoch 5 - F1 Score: 0.5332\n",
      "Saved best model\n",
      "[0.3783293227186078, 0.43187826365071724, 0.48421917084072025, 0.5057333508901423, 0.5331835203901123]\n",
      "2364/2364 [==============================] - 9s 4ms/step - loss: 1.5615 - accuracy: 0.5500 - val_loss: 1.5482 - val_accuracy: 0.5549\n",
      "Epoch 6/40\n",
      "263/263 [==============================] - 0s 2ms/step loss: 1.508\n",
      "Epoch 6 - F1 Score: 0.5383\n",
      "Saved best model\n",
      "[0.3783293227186078, 0.43187826365071724, 0.48421917084072025, 0.5057333508901423, 0.5331835203901123, 0.5382849699362301]\n",
      "2364/2364 [==============================] - 7s 3ms/step - loss: 1.5077 - accuracy: 0.5640 - val_loss: 1.5108 - val_accuracy: 0.5635\n",
      "Epoch 7/40\n",
      "263/263 [==============================] - 0s 1ms/step loss: 1.4631 - ac\n",
      "Epoch 7 - F1 Score: 0.5533\n",
      "Saved best model\n",
      "[0.3783293227186078, 0.43187826365071724, 0.48421917084072025, 0.5057333508901423, 0.5331835203901123, 0.5382849699362301, 0.5533418469064253]\n",
      "2364/2364 [==============================] - 8s 3ms/step - loss: 1.4628 - accuracy: 0.5774 - val_loss: 1.4885 - val_accuracy: 0.5698\n",
      "Epoch 8/40\n",
      "263/263 [==============================] - 0s 2ms/step loss: 1.4250 \n",
      "Epoch 8 - F1 Score: 0.5693\n",
      "Saved best model\n",
      "[0.3783293227186078, 0.43187826365071724, 0.48421917084072025, 0.5057333508901423, 0.5331835203901123, 0.5382849699362301, 0.5533418469064253, 0.5692798196087642]\n",
      "2364/2364 [==============================] - 9s 4ms/step - loss: 1.4252 - accuracy: 0.5862 - val_loss: 1.4371 - val_accuracy: 0.5857\n",
      "Epoch 9/40\n",
      "263/263 [==============================] - 0s 2ms/step loss: 1.3914 \n",
      "2364/2364 [==============================] - 7s 3ms/step - loss: 1.3913 - accuracy: 0.5951 - val_loss: 1.4212 - val_accuracy: 0.5832\n",
      "Epoch 10/40\n",
      "263/263 [==============================] - 0s 1ms/step loss: 1.3648 - \n",
      "Epoch 10 - F1 Score: 0.5739\n",
      "Saved best model\n",
      "[0.3783293227186078, 0.43187826365071724, 0.48421917084072025, 0.5057333508901423, 0.5331835203901123, 0.5382849699362301, 0.5533418469064253, 0.5692798196087642, 0.5581190322809754, 0.5739444019521512]\n",
      "2364/2364 [==============================] - 7s 3ms/step - loss: 1.3640 - accuracy: 0.6020 - val_loss: 1.3888 - val_accuracy: 0.5966\n",
      "Epoch 11/40\n",
      "263/263 [==============================] - 0s 1ms/step loss: 1.3397 - \n",
      "Epoch 11 - F1 Score: 0.5818\n",
      "Saved best model\n",
      "[0.3783293227186078, 0.43187826365071724, 0.48421917084072025, 0.5057333508901423, 0.5331835203901123, 0.5382849699362301, 0.5533418469064253, 0.5692798196087642, 0.5581190322809754, 0.5739444019521512, 0.5818425429290377]\n",
      "2364/2364 [==============================] - 6s 3ms/step - loss: 1.3396 - accuracy: 0.6090 - val_loss: 1.3861 - val_accuracy: 0.5975\n",
      "Epoch 12/40\n",
      "263/263 [==============================] - 1s 2ms/step loss: 1.3\n",
      "2364/2364 [==============================] - 6s 2ms/step - loss: 1.3182 - accuracy: 0.6157 - val_loss: 1.3696 - val_accuracy: 0.5980\n",
      "Epoch 13/40\n",
      "263/263 [==============================] - 0s 1ms/step loss: 1.2979 - ac\n",
      "Epoch 13 - F1 Score: 0.5860\n",
      "Saved best model\n",
      "[0.3783293227186078, 0.43187826365071724, 0.48421917084072025, 0.5057333508901423, 0.5331835203901123, 0.5382849699362301, 0.5533418469064253, 0.5692798196087642, 0.5581190322809754, 0.5739444019521512, 0.5818425429290377, 0.5773133367125108, 0.5860413904624929]\n",
      "2364/2364 [==============================] - 6s 2ms/step - loss: 1.2979 - accuracy: 0.6218 - val_loss: 1.3465 - val_accuracy: 0.6061\n",
      "Epoch 14/40\n",
      "263/263 [==============================] - 1s 2ms/step loss: 1.278\n",
      "Epoch 14 - F1 Score: 0.5920\n",
      "Saved best model\n",
      "[0.3783293227186078, 0.43187826365071724, 0.48421917084072025, 0.5057333508901423, 0.5331835203901123, 0.5382849699362301, 0.5533418469064253, 0.5692798196087642, 0.5581190322809754, 0.5739444019521512, 0.5818425429290377, 0.5773133367125108, 0.5860413904624929, 0.5920444546989441]\n",
      "2364/2364 [==============================] - 6s 3ms/step - loss: 1.2787 - accuracy: 0.6254 - val_loss: 1.3394 - val_accuracy: 0.6098\n",
      "Epoch 15/40\n",
      "263/263 [==============================] - 1s 2ms/step loss: 1.262\n",
      "Epoch 15 - F1 Score: 0.5974\n",
      "Saved best model\n",
      "[0.3783293227186078, 0.43187826365071724, 0.48421917084072025, 0.5057333508901423, 0.5331835203901123, 0.5382849699362301, 0.5533418469064253, 0.5692798196087642, 0.5581190322809754, 0.5739444019521512, 0.5818425429290377, 0.5773133367125108, 0.5860413904624929, 0.5920444546989441, 0.5973710143858737]\n",
      "2364/2364 [==============================] - 6s 2ms/step - loss: 1.2617 - accuracy: 0.6303 - val_loss: 1.3348 - val_accuracy: 0.6136\n",
      "Epoch 16/40\n",
      "263/263 [==============================] - 1s 2ms/step loss: 1\n",
      "2364/2364 [==============================] - 7s 3ms/step - loss: 1.2471 - accuracy: 0.6348 - val_loss: 1.3085 - val_accuracy: 0.6144\n",
      "Epoch 17/40\n",
      "263/263 [==============================] - 1s 2ms/step loss: 1.2\n",
      "2364/2364 [==============================] - 6s 3ms/step - loss: 1.2350 - accuracy: 0.6371 - val_loss: 1.3177 - val_accuracy: 0.6164\n",
      "Epoch 18/40\n",
      "263/263 [==============================] - 0s 2ms/step loss: 1.2231 \n",
      "Epoch 18 - F1 Score: 0.6039\n",
      "Saved best model\n",
      "[0.3783293227186078, 0.43187826365071724, 0.48421917084072025, 0.5057333508901423, 0.5331835203901123, 0.5382849699362301, 0.5533418469064253, 0.5692798196087642, 0.5581190322809754, 0.5739444019521512, 0.5818425429290377, 0.5773133367125108, 0.5860413904624929, 0.5920444546989441, 0.5973710143858737, 0.5958090401046371, 0.5969812324211308, 0.6038676240076568]\n",
      "2364/2364 [==============================] - 6s 3ms/step - loss: 1.2228 - accuracy: 0.6422 - val_loss: 1.3096 - val_accuracy: 0.6189\n",
      "Epoch 19/40\n",
      "263/263 [==============================] - 1s 2ms/step loss: 1.2\n",
      "Epoch 19 - F1 Score: 0.6045\n",
      "Saved best model\n",
      "[0.3783293227186078, 0.43187826365071724, 0.48421917084072025, 0.5057333508901423, 0.5331835203901123, 0.5382849699362301, 0.5533418469064253, 0.5692798196087642, 0.5581190322809754, 0.5739444019521512, 0.5818425429290377, 0.5773133367125108, 0.5860413904624929, 0.5920444546989441, 0.5973710143858737, 0.5958090401046371, 0.5969812324211308, 0.6038676240076568, 0.6044932021259526]\n",
      "2364/2364 [==============================] - 8s 3ms/step - loss: 1.2106 - accuracy: 0.6446 - val_loss: 1.2956 - val_accuracy: 0.6205\n",
      "Epoch 20/40\n",
      "263/263 [==============================] - 0s 2ms/step loss: 1.1978 \n",
      "2364/2364 [==============================] - 7s 3ms/step - loss: 1.1979 - accuracy: 0.6480 - val_loss: 1.2957 - val_accuracy: 0.6188\n",
      "Epoch 21/40\n",
      "263/263 [==============================] - 1s 2ms/step loss: 1.1\n",
      "Epoch 21 - F1 Score: 0.6156\n",
      "Saved best model\n",
      "[0.3783293227186078, 0.43187826365071724, 0.48421917084072025, 0.5057333508901423, 0.5331835203901123, 0.5382849699362301, 0.5533418469064253, 0.5692798196087642, 0.5581190322809754, 0.5739444019521512, 0.5818425429290377, 0.5773133367125108, 0.5860413904624929, 0.5920444546989441, 0.5973710143858737, 0.5958090401046371, 0.5969812324211308, 0.6038676240076568, 0.6044932021259526, 0.6017481144844171, 0.6155563454761418]\n",
      "2364/2364 [==============================] - 6s 2ms/step - loss: 1.1900 - accuracy: 0.6492 - val_loss: 1.2797 - val_accuracy: 0.6300\n",
      "Epoch 22/40\n",
      "263/263 [==============================] - 1s 2ms/step loss: 1.1\n",
      "Epoch 22 - F1 Score: 0.6161\n",
      "Saved best model\n",
      "[0.3783293227186078, 0.43187826365071724, 0.48421917084072025, 0.5057333508901423, 0.5331835203901123, 0.5382849699362301, 0.5533418469064253, 0.5692798196087642, 0.5581190322809754, 0.5739444019521512, 0.5818425429290377, 0.5773133367125108, 0.5860413904624929, 0.5920444546989441, 0.5973710143858737, 0.5958090401046371, 0.5969812324211308, 0.6038676240076568, 0.6044932021259526, 0.6017481144844171, 0.6155563454761418, 0.6161105844308832]\n",
      "2364/2364 [==============================] - 7s 3ms/step - loss: 1.1784 - accuracy: 0.6525 - val_loss: 1.2769 - val_accuracy: 0.6280\n",
      "Epoch 23/40\n",
      "263/263 [==============================] - 1s 2ms/step loss: 1.171\n",
      "2364/2364 [==============================] - 6s 3ms/step - loss: 1.1712 - accuracy: 0.6549 - val_loss: 1.2825 - val_accuracy: 0.6254\n",
      "Epoch 24/40\n",
      "263/263 [==============================] - 1s 2ms/step loss: 1\n",
      "2364/2364 [==============================] - 7s 3ms/step - loss: 1.1616 - accuracy: 0.6576 - val_loss: 1.3212 - val_accuracy: 0.6159\n",
      "Epoch 25/40\n",
      "263/263 [==============================] - 1s 2ms/step loss: 1.1\n",
      "Epoch 25 - F1 Score: 0.6175\n",
      "Saved best model\n",
      "[0.3783293227186078, 0.43187826365071724, 0.48421917084072025, 0.5057333508901423, 0.5331835203901123, 0.5382849699362301, 0.5533418469064253, 0.5692798196087642, 0.5581190322809754, 0.5739444019521512, 0.5818425429290377, 0.5773133367125108, 0.5860413904624929, 0.5920444546989441, 0.5973710143858737, 0.5958090401046371, 0.5969812324211308, 0.6038676240076568, 0.6044932021259526, 0.6017481144844171, 0.6155563454761418, 0.6161105844308832, 0.6138162142763575, 0.6065088951179322, 0.6175274587510632]\n",
      "2364/2364 [==============================] - 6s 3ms/step - loss: 1.1521 - accuracy: 0.6603 - val_loss: 1.2756 - val_accuracy: 0.6291\n",
      "Epoch 26/40\n",
      "263/263 [==============================] - 0s 1ms/step loss: 1.1456 - ac\n",
      "Epoch 26 - F1 Score: 0.6230\n",
      "Saved best model\n",
      "[0.3783293227186078, 0.43187826365071724, 0.48421917084072025, 0.5057333508901423, 0.5331835203901123, 0.5382849699362301, 0.5533418469064253, 0.5692798196087642, 0.5581190322809754, 0.5739444019521512, 0.5818425429290377, 0.5773133367125108, 0.5860413904624929, 0.5920444546989441, 0.5973710143858737, 0.5958090401046371, 0.5969812324211308, 0.6038676240076568, 0.6044932021259526, 0.6017481144844171, 0.6155563454761418, 0.6161105844308832, 0.6138162142763575, 0.6065088951179322, 0.6175274587510632, 0.6229706006856506]\n",
      "2364/2364 [==============================] - 6s 2ms/step - loss: 1.1459 - accuracy: 0.6624 - val_loss: 1.2500 - val_accuracy: 0.6375\n",
      "Epoch 27/40\n",
      "263/263 [==============================] - 1s 2ms/step loss: 1\n",
      "Epoch 27 - F1 Score: 0.6271\n",
      "Saved best model\n",
      "[0.3783293227186078, 0.43187826365071724, 0.48421917084072025, 0.5057333508901423, 0.5331835203901123, 0.5382849699362301, 0.5533418469064253, 0.5692798196087642, 0.5581190322809754, 0.5739444019521512, 0.5818425429290377, 0.5773133367125108, 0.5860413904624929, 0.5920444546989441, 0.5973710143858737, 0.5958090401046371, 0.5969812324211308, 0.6038676240076568, 0.6044932021259526, 0.6017481144844171, 0.6155563454761418, 0.6161105844308832, 0.6138162142763575, 0.6065088951179322, 0.6175274587510632, 0.6229706006856506, 0.6270745913238925]\n",
      "2364/2364 [==============================] - 6s 3ms/step - loss: 1.1375 - accuracy: 0.6655 - val_loss: 1.2559 - val_accuracy: 0.6387\n",
      "Epoch 28/40\n",
      "263/263 [==============================] - 1s 2ms/step loss: 1.130\n",
      "2364/2364 [==============================] - 6s 3ms/step - loss: 1.1302 - accuracy: 0.6668 - val_loss: 1.2702 - val_accuracy: 0.6339\n",
      "Epoch 29/40\n",
      "263/263 [==============================] - 1s 2ms/step loss: 1.124\n",
      "2364/2364 [==============================] - 7s 3ms/step - loss: 1.1243 - accuracy: 0.6664 - val_loss: 1.2787 - val_accuracy: 0.6272\n",
      "Epoch 30/40\n",
      "263/263 [==============================] - 1s 2ms/step loss: 1.1\n",
      "2364/2364 [==============================] - 6s 3ms/step - loss: 1.1170 - accuracy: 0.6700 - val_loss: 1.2441 - val_accuracy: 0.6374\n",
      "Epoch 31/40\n",
      "263/263 [==============================] - 0s 1ms/step loss: 1.1099 - accu\n",
      "2364/2364 [==============================] - 6s 2ms/step - loss: 1.1100 - accuracy: 0.6727 - val_loss: 1.2635 - val_accuracy: 0.6360\n",
      "Epoch 32/40\n",
      "263/263 [==============================] - 1s 2ms/step loss: 1.105\n",
      "Epoch 32 - F1 Score: 0.6304\n",
      "Saved best model\n",
      "[0.3783293227186078, 0.43187826365071724, 0.48421917084072025, 0.5057333508901423, 0.5331835203901123, 0.5382849699362301, 0.5533418469064253, 0.5692798196087642, 0.5581190322809754, 0.5739444019521512, 0.5818425429290377, 0.5773133367125108, 0.5860413904624929, 0.5920444546989441, 0.5973710143858737, 0.5958090401046371, 0.5969812324211308, 0.6038676240076568, 0.6044932021259526, 0.6017481144844171, 0.6155563454761418, 0.6161105844308832, 0.6138162142763575, 0.6065088951179322, 0.6175274587510632, 0.6229706006856506, 0.6270745913238925, 0.6215979884304174, 0.6124537102632293, 0.6232870565451334, 0.6268870925268366, 0.6304109061491657]\n",
      "2364/2364 [==============================] - 7s 3ms/step - loss: 1.1053 - accuracy: 0.6722 - val_loss: 1.2467 - val_accuracy: 0.6383\n",
      "Epoch 33/40\n",
      "263/263 [==============================] - 1s 2ms/step loss: 1.098\n",
      "2364/2364 [==============================] - 6s 3ms/step - loss: 1.0991 - accuracy: 0.6737 - val_loss: 1.2881 - val_accuracy: 0.6272\n",
      "Epoch 34/40\n",
      "263/263 [==============================] - 0s 1ms/step loss: 1.0945 - ac\n",
      "Epoch 34 - F1 Score: 0.6364\n",
      "Saved best model\n",
      "[0.3783293227186078, 0.43187826365071724, 0.48421917084072025, 0.5057333508901423, 0.5331835203901123, 0.5382849699362301, 0.5533418469064253, 0.5692798196087642, 0.5581190322809754, 0.5739444019521512, 0.5818425429290377, 0.5773133367125108, 0.5860413904624929, 0.5920444546989441, 0.5973710143858737, 0.5958090401046371, 0.5969812324211308, 0.6038676240076568, 0.6044932021259526, 0.6017481144844171, 0.6155563454761418, 0.6161105844308832, 0.6138162142763575, 0.6065088951179322, 0.6175274587510632, 0.6229706006856506, 0.6270745913238925, 0.6215979884304174, 0.6124537102632293, 0.6232870565451334, 0.6268870925268366, 0.6304109061491657, 0.6157934810040583, 0.6363561808302083]\n",
      "2364/2364 [==============================] - 6s 3ms/step - loss: 1.0943 - accuracy: 0.6759 - val_loss: 1.2345 - val_accuracy: 0.6457\n",
      "Epoch 35/40\n",
      "263/263 [==============================] - 0s 2ms/step loss: 1.088\n",
      "Epoch 35 - F1 Score: 0.6364\n",
      "Saved best model\n",
      "[0.3783293227186078, 0.43187826365071724, 0.48421917084072025, 0.5057333508901423, 0.5331835203901123, 0.5382849699362301, 0.5533418469064253, 0.5692798196087642, 0.5581190322809754, 0.5739444019521512, 0.5818425429290377, 0.5773133367125108, 0.5860413904624929, 0.5920444546989441, 0.5973710143858737, 0.5958090401046371, 0.5969812324211308, 0.6038676240076568, 0.6044932021259526, 0.6017481144844171, 0.6155563454761418, 0.6161105844308832, 0.6138162142763575, 0.6065088951179322, 0.6175274587510632, 0.6229706006856506, 0.6270745913238925, 0.6215979884304174, 0.6124537102632293, 0.6232870565451334, 0.6268870925268366, 0.6304109061491657, 0.6157934810040583, 0.6363561808302083, 0.6363821308529511]\n",
      "2364/2364 [==============================] - 8s 3ms/step - loss: 1.0882 - accuracy: 0.6775 - val_loss: 1.2321 - val_accuracy: 0.6462\n",
      "Epoch 36/40\n",
      "263/263 [==============================] - 1s 2ms/step loss: 1.0\n",
      "2364/2364 [==============================] - 8s 3ms/step - loss: 1.0815 - accuracy: 0.6797 - val_loss: 1.2278 - val_accuracy: 0.6448\n",
      "Epoch 37/40\n",
      "263/263 [==============================] - 0s 2ms/step loss: 1.0781 - a\n",
      "Epoch 37 - F1 Score: 0.6367\n",
      "Saved best model\n",
      "[0.3783293227186078, 0.43187826365071724, 0.48421917084072025, 0.5057333508901423, 0.5331835203901123, 0.5382849699362301, 0.5533418469064253, 0.5692798196087642, 0.5581190322809754, 0.5739444019521512, 0.5818425429290377, 0.5773133367125108, 0.5860413904624929, 0.5920444546989441, 0.5973710143858737, 0.5958090401046371, 0.5969812324211308, 0.6038676240076568, 0.6044932021259526, 0.6017481144844171, 0.6155563454761418, 0.6161105844308832, 0.6138162142763575, 0.6065088951179322, 0.6175274587510632, 0.6229706006856506, 0.6270745913238925, 0.6215979884304174, 0.6124537102632293, 0.6232870565451334, 0.6268870925268366, 0.6304109061491657, 0.6157934810040583, 0.6363561808302083, 0.6363821308529511, 0.63469244295449, 0.6366893513860254]\n",
      "2364/2364 [==============================] - 8s 4ms/step - loss: 1.0781 - accuracy: 0.6799 - val_loss: 1.2273 - val_accuracy: 0.6471\n",
      "Epoch 38/40\n",
      "263/263 [==============================] - 0s 1ms/step loss: 1.0737 - \n",
      "Epoch 38 - F1 Score: 0.6400\n",
      "Saved best model\n",
      "[0.3783293227186078, 0.43187826365071724, 0.48421917084072025, 0.5057333508901423, 0.5331835203901123, 0.5382849699362301, 0.5533418469064253, 0.5692798196087642, 0.5581190322809754, 0.5739444019521512, 0.5818425429290377, 0.5773133367125108, 0.5860413904624929, 0.5920444546989441, 0.5973710143858737, 0.5958090401046371, 0.5969812324211308, 0.6038676240076568, 0.6044932021259526, 0.6017481144844171, 0.6155563454761418, 0.6161105844308832, 0.6138162142763575, 0.6065088951179322, 0.6175274587510632, 0.6229706006856506, 0.6270745913238925, 0.6215979884304174, 0.6124537102632293, 0.6232870565451334, 0.6268870925268366, 0.6304109061491657, 0.6157934810040583, 0.6363561808302083, 0.6363821308529511, 0.63469244295449, 0.6366893513860254, 0.6400310997589393]\n",
      "2364/2364 [==============================] - 9s 4ms/step - loss: 1.0728 - accuracy: 0.6811 - val_loss: 1.2208 - val_accuracy: 0.6513\n",
      "Epoch 39/40\n",
      "263/263 [==============================] - 1s 3ms/step l\n",
      "2364/2364 [==============================] - 9s 4ms/step - loss: 1.0665 - accuracy: 0.6823 - val_loss: 1.2270 - val_accuracy: 0.6432\n",
      "Epoch 40/40\n",
      "263/263 [==============================] - 1s 3ms/step\n",
      "2364/2364 [==============================] - 11s 5ms/step - loss: 1.0641 - accuracy: 0.6844 - val_loss: 1.2484 - val_accuracy: 0.6404\n",
      "485/485 [==============================] - 2s 3ms/step\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         119     0.7798    0.4344    0.5580      1174\n",
      "         120     0.4338    0.2906    0.3481       203\n",
      "         125     0.4768    0.8252    0.6044       572\n",
      "         134     0.3333    0.5263    0.4082        19\n",
      "         189     0.3686    0.7899    0.5027       119\n",
      "         190     0.3054    0.6459    0.4147       209\n",
      "          20     0.4771    0.0839    0.1427       870\n",
      "         200     0.5884    0.5502    0.5687       647\n",
      "         203     0.3538    0.6389    0.4554        36\n",
      "          22     0.8672    0.7683    0.8147       561\n",
      "         254     0.0000    0.0000    0.0000        38\n",
      "         255     0.1908    0.4203    0.2624        69\n",
      "         264     0.5782    0.4502    0.5062       542\n",
      "         269     0.3778    0.2833    0.3238       120\n",
      "         276     0.2273    0.1351    0.1695        74\n",
      "         284     0.2500    0.0620    0.0994       129\n",
      "         287     0.5323    0.3257    0.4041       304\n",
      "         295     0.5641    0.5238    0.5432        84\n",
      "         306     0.4211    0.0851    0.1416        94\n",
      "         310     0.5057    0.8560    0.6358       257\n",
      "         312     0.1193    0.2549    0.1625        51\n",
      "         319     0.2857    0.2545    0.2692        55\n",
      "         326     0.1515    0.1471    0.1493        34\n",
      "         327     0.3673    0.4737    0.4138        38\n",
      "         345     0.0000    0.0000    0.0000        26\n",
      "         347     0.2941    0.5556    0.3846        27\n",
      "         352     0.5746    0.8488    0.6853       549\n",
      "         362     0.2897    0.6691    0.4043       139\n",
      "         399     0.3563    0.5390    0.4290       269\n",
      "         400     0.3359    0.3094    0.3221       139\n",
      "         401     0.2500    0.3036    0.2742        56\n",
      "         415     0.6190    0.2826    0.3881        46\n",
      "         416     0.7227    0.7227    0.7227       357\n",
      "         426     0.4400    0.5000    0.4681        44\n",
      "         427     0.3919    0.6170    0.4793        47\n",
      "         434     0.6347    0.6588    0.6465       211\n",
      "         476     0.2586    0.8333    0.3947       234\n",
      "         502     0.4845    0.7027    0.5735       111\n",
      "         522     0.2564    0.1075    0.1515        93\n",
      "         532     0.2794    0.7600    0.4086        50\n",
      "          59     0.6609    0.6786    0.6696       112\n",
      "         601     0.5169    0.5542    0.5349        83\n",
      "         611     0.7353    0.7576    0.7463        99\n",
      "         617     0.4062    0.3023    0.3467        43\n",
      "         639     0.3415    0.4516    0.3889        31\n",
      "         668     0.1429    0.0400    0.0625        50\n",
      "         732     0.2178    0.2157    0.2167       102\n",
      "          74     0.0541    0.2561    0.0894        82\n",
      "         755     0.1111    0.3243    0.1655        37\n",
      "          77     0.5833    0.2303    0.3302       152\n",
      "         770     0.3919    0.3919    0.3919        74\n",
      "         772     0.4167    0.2564    0.3175        39\n",
      "          78     0.5332    0.6803    0.5978       319\n",
      "         787     0.6113    0.4177    0.4963       960\n",
      "          79     0.9618    0.6015    0.7401      2552\n",
      "         798     0.7887    0.4341    0.5600       129\n",
      "         835     0.2308    0.7333    0.3511        45\n",
      "         843     0.4138    0.3000    0.3478        40\n",
      "         862     0.5914    0.4889    0.5353       225\n",
      "         863     0.2237    0.2764    0.2473       123\n",
      "          89     0.9958    0.6633    0.7962      1081\n",
      "         908     0.1552    0.3214    0.2093        28\n",
      "         918     0.4389    0.7980    0.5663        99\n",
      "          94     0.2176    0.7255    0.3348       306\n",
      "\n",
      "    accuracy                         0.5320     15508\n",
      "   macro avg     0.4138    0.4552    0.4011     15508\n",
      "weighted avg     0.6279    0.5320    0.5407     15508\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['label_encoder_train.joblib']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import joblib\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class F1ScoreCallback(Callback):\n",
    "    def __init__(self, X_val, y_val):\n",
    "        super(F1ScoreCallback, self).__init__()\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.best_f1 = 0.0\n",
    "        self.best_model = None\n",
    "        self.f1_scores = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_val_pred = np.argmax(self.model.predict(self.X_val), axis=1)\n",
    "        f1 = f1_score(self.y_val, y_val_pred, average='weighted')\n",
    "        self.f1_scores.append(f1)\n",
    "        \n",
    "\n",
    "        if f1 > self.best_f1:\n",
    "            self.best_f1 = f1\n",
    "            self.best_model = self.model\n",
    "            print(f\"Epoch {epoch + 1} - F1 Score: {f1:.4f}\")\n",
    "            print(\"Saved best model\")\n",
    "            print(self.f1_scores)\n",
    "\n",
    "# Load JSON data with embeddings\n",
    "with open('train_definitivo2.pickle', 'rb') as f1:\n",
    "    balanced = pickle.load(f1)\n",
    "\n",
    "with open('test_SecureBert.pickle', 'rb') as f2:\n",
    "    unbalanced = pickle.load(f2)\n",
    "\n",
    "train = np.array([item['cve_description_secureBERT'] for item in balanced if item['cwe'] != 'None'])\n",
    "test = np.array([item['cwe'] for item in balanced if item['cwe'] != 'None'])\n",
    "np.random.seed(42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(train,test,test_size=0.1,random_state=42)\n",
    "\n",
    "X_test = np.array([item['cve_description_secureBERT'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "y_test = np.array([item['cwe'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "\n",
    "label_encoder_train = LabelEncoder()\n",
    "y_train_encoded = label_encoder_train.fit_transform(y_train)\n",
    "label_encoder_test = LabelEncoder()\n",
    "y_test_encoded = label_encoder_test.fit_transform(y_test)\n",
    "\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = len(np.unique(y_train))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=input_dim, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(output_dim, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "f1_callback = F1ScoreCallback(X_val, label_encoder_train.transform(y_val))\n",
    "\n",
    "history = model.fit(X_train, y_train_encoded, epochs=40, batch_size=32, validation_data=(X_val, label_encoder_train.transform(y_val)), verbose=1, callbacks=[f1_callback])\n",
    "\n",
    "best_model = f1_callback.best_model\n",
    "\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(best_model, 'CWE_classes.joblib')\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_probs = best_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "y_pred_original = label_encoder_train.inverse_transform(y_pred)\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_original, digits=4))\n",
    "joblib.dump(label_encoder_train, 'label_encoder_train.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run inference with the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "485/485 [==============================] - 1s 1ms/step\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         119     0.7798    0.4344    0.5580      1174\n",
      "         120     0.4338    0.2906    0.3481       203\n",
      "         125     0.4768    0.8252    0.6044       572\n",
      "         134     0.3333    0.5263    0.4082        19\n",
      "         189     0.3686    0.7899    0.5027       119\n",
      "         190     0.3054    0.6459    0.4147       209\n",
      "          20     0.4771    0.0839    0.1427       870\n",
      "         200     0.5884    0.5502    0.5687       647\n",
      "         203     0.3538    0.6389    0.4554        36\n",
      "          22     0.8672    0.7683    0.8147       561\n",
      "         254     0.0000    0.0000    0.0000        38\n",
      "         255     0.1908    0.4203    0.2624        69\n",
      "         264     0.5782    0.4502    0.5062       542\n",
      "         269     0.3778    0.2833    0.3238       120\n",
      "         276     0.2273    0.1351    0.1695        74\n",
      "         284     0.2500    0.0620    0.0994       129\n",
      "         287     0.5323    0.3257    0.4041       304\n",
      "         295     0.5641    0.5238    0.5432        84\n",
      "         306     0.4211    0.0851    0.1416        94\n",
      "         310     0.5057    0.8560    0.6358       257\n",
      "         312     0.1193    0.2549    0.1625        51\n",
      "         319     0.2857    0.2545    0.2692        55\n",
      "         326     0.1515    0.1471    0.1493        34\n",
      "         327     0.3673    0.4737    0.4138        38\n",
      "         345     0.0000    0.0000    0.0000        26\n",
      "         347     0.2941    0.5556    0.3846        27\n",
      "         352     0.5746    0.8488    0.6853       549\n",
      "         362     0.2897    0.6691    0.4043       139\n",
      "         399     0.3563    0.5390    0.4290       269\n",
      "         400     0.3359    0.3094    0.3221       139\n",
      "         401     0.2500    0.3036    0.2742        56\n",
      "         415     0.6190    0.2826    0.3881        46\n",
      "         416     0.7227    0.7227    0.7227       357\n",
      "         426     0.4400    0.5000    0.4681        44\n",
      "         427     0.3919    0.6170    0.4793        47\n",
      "         434     0.6347    0.6588    0.6465       211\n",
      "         476     0.2586    0.8333    0.3947       234\n",
      "         502     0.4845    0.7027    0.5735       111\n",
      "         522     0.2564    0.1075    0.1515        93\n",
      "         532     0.2794    0.7600    0.4086        50\n",
      "          59     0.6609    0.6786    0.6696       112\n",
      "         601     0.5169    0.5542    0.5349        83\n",
      "         611     0.7353    0.7576    0.7463        99\n",
      "         617     0.4062    0.3023    0.3467        43\n",
      "         639     0.3415    0.4516    0.3889        31\n",
      "         668     0.1429    0.0400    0.0625        50\n",
      "         732     0.2178    0.2157    0.2167       102\n",
      "          74     0.0541    0.2561    0.0894        82\n",
      "         755     0.1111    0.3243    0.1655        37\n",
      "          77     0.5833    0.2303    0.3302       152\n",
      "         770     0.3919    0.3919    0.3919        74\n",
      "         772     0.4167    0.2564    0.3175        39\n",
      "          78     0.5332    0.6803    0.5978       319\n",
      "         787     0.6113    0.4177    0.4963       960\n",
      "          79     0.9618    0.6015    0.7401      2552\n",
      "         798     0.7887    0.4341    0.5600       129\n",
      "         835     0.2308    0.7333    0.3511        45\n",
      "         843     0.4138    0.3000    0.3478        40\n",
      "         862     0.5914    0.4889    0.5353       225\n",
      "         863     0.2237    0.2764    0.2473       123\n",
      "          89     0.9958    0.6633    0.7962      1081\n",
      "         908     0.1552    0.3214    0.2093        28\n",
      "         918     0.4389    0.7980    0.5663        99\n",
      "          94     0.2176    0.7255    0.3348       306\n",
      "\n",
      "    accuracy                         0.5320     15508\n",
      "   macro avg     0.4138    0.4552    0.4011     15508\n",
      "weighted avg     0.6279    0.5320    0.5407     15508\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Save the best model\n",
    "with open('test_SecureBert.pickle', 'rb') as f2:\n",
    "    unbalanced = pickle.load(f2)\n",
    "\n",
    "X_test = np.array([item['cve_description_secureBERT'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "y_test = np.array([item['cwe'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "\n",
    "best_model=joblib.load('CWE_classes.joblib')\n",
    "label_encoder_train=joblib.load('label_encoder_train.joblib')\n",
    "# Make predictions on the test set\n",
    "y_pred_probs = best_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "y_pred_original = label_encoder_train.inverse_transform(y_pred)\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_original, digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
